{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computational Methods in Economics\n",
    "\n",
    "## Lecture 4 - Root Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last update: 2017-11-20 13:47:15.415443\n"
     ]
    }
   ],
   "source": [
    "# Author: Alex Schmitt (schmitt@ifo.de)\n",
    "\n",
    "import datetime\n",
    "print('Last update: ' + str(datetime.datetime.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "\n",
    "import scipy.optimize\n",
    "\n",
    "# import sys\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Lecture\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Bisection](#bisection)\n",
    "- [Function Iteration](#funiter)\n",
    "- [Newton's Method](#newton)\n",
    "- [Numerical Differentiation](#numdiff)\n",
    "- [Quasi-Newton Methods](#quasi)\n",
    "- [Convergence](#convergence)\n",
    "- [The Scipy Package](#scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = \"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "A function $f(x)$ has a *root* (also called a *zero*) at $x^*$ if $f(x^*) = 0$. Two cases are relevant:\n",
    "\n",
    "- $f$ can be a univariate scalar/real-valued function $f: \\mathbb{R}\\ \\rightarrow \\mathbb{R}$, i.e. both input and output are scalars, or both its range and its domain have a dimension of 1\n",
    "- $f$ can be a vector-valued function $\\mathbf{f}: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}^n$, i.e. both its range and its domain have a dimension greater than 1. In this case, finding the roots of a vector-valued function is equivalent to *solving a system of nonlinear equations*. \n",
    "\n",
    "The intermediate case of a multivariate scalar function $f: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}$ - its inputs are vectors, hence its domain has a dimension greater than 1 - is going to be important in the next lecture. \n",
    "\n",
    "Finding the root(s) of a function is one of the most common computational problems in economics, often applied when looking for an equilibrium. In other words, an equilibrium is usually defined by a set of equations, as illustrated by the following example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Neoclassical Growth Model\n",
    "\n",
    "We have seen the standard NGM already in the first lecture. Recall:\n",
    "\n",
    "- Utility function:\n",
    "\n",
    "\\begin{equation}\n",
    "    u(c, h) = \\frac{c^{1-\\nu}}{1-\\nu} - B \\frac{h^{1+\\eta}}{1+\\eta}\n",
    "\\end{equation}\n",
    "\n",
    "with $c$ denoting consumption and $h$ labor supply.\n",
    "\n",
    "- Production function:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(k, h) = A k^\\alpha h^{1-\\alpha}\n",
    "\\end{equation}\n",
    "with $k$ denoting the capital stock, and $A$ the productivity level.\n",
    "\n",
    "- Resource Constraint:\n",
    "\n",
    "\\begin{equation}\n",
    "    k_{t+1} + c_t = f(k_t, h_t) + (1 - \\delta) k_t = A k_t^\\alpha h_t^{1-\\alpha} + (1 - \\delta) k_t\n",
    "\\end{equation}\n",
    "\n",
    "- Planner's Problem:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\max_{\\left\\{c_t, k_{t+1}, h_t\\right\\}} \\sum^\\infty_{t = 0} \\beta^t u(c_t, h_t) \n",
    "\\end{equation}\n",
    "s.t. the resource constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### First-order conditions\n",
    "\n",
    "(1) Euler equation\n",
    "\n",
    "\\begin{equation}\n",
    "    c^{-\\nu} = \\beta \\left[ (c')^{-\\nu} (f_k(k', h') + 1 - \\delta) \\right]    \n",
    "\\end{equation}\n",
    "\n",
    "(2) intratemporal optimality condition\n",
    "\n",
    "\\begin{equation}\n",
    "    B h^{\\eta} = c^{-\\nu} f_h(k, h)  \n",
    "\\end{equation}\n",
    "\n",
    "where I have used the notation $c = c_t$ and $c' = c_{t+1}$ (and analogous for $k$ and $h$) for brevity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Steady State\n",
    "\n",
    "In an equilibrium, the two first-order conditions, combined with the resource constraint, must hold in every period. We will get to how to solve for the full dynamic allocation later in this course. For now, let's consider the *steady state*, where all variables are constant over time, i.e. $c_t = c_{t+1} = c_s$ and so on. The Euler equation then can be simplied to:\n",
    "\n",
    "\\begin{equation}\n",
    "    1 = \\beta \\left[f_k(k_s, h_s) + 1 - \\delta \\right]    \n",
    "\\end{equation}\n",
    "\n",
    "For the intratemporal optimality condition, use the resource constraint to substitute for consumption:\n",
    "\n",
    "\\begin{equation}\n",
    "    B h_s^{\\eta} = \\left[ f(k_s, h_s) - \\delta k_s \\right]^{-\\nu} f_h(k_s, h_s)  \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is a nonlinear system of two equations, with two unknown variables, $k_s$ and $h_s$, which can be solved using the methods introduced below. We can define a vector-valued function $\\mathbf{S}$ with\n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathbf{S}(k, h) = \n",
    "    \\left[\n",
    "    \\begin{array}{c}\n",
    "        \\beta \\left[f_k(k, h) + 1 - \\delta \\right]  - 1 \\\\\n",
    "        \\left[ f(k, h) - \\delta k \\right]^{-\\nu} f_h(k, h) - B h^{\\eta}\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "\\end{equation}\n",
    "\n",
    "Finding the steady state of the model then requires finding a root of function $\\mathbf{S}$, i.e. a vector $(k_s, h_s)$ such that \n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathbf{S}(k_s, h_s) = \n",
    "    \\left[\n",
    "    \\begin{array}{c}\n",
    "        0 \\\\\n",
    "        0\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some Definitions\n",
    "\n",
    "\n",
    "- For a multivariate real-valued function $f: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}$, the vector consisting of the first derivatives is called the *gradient* (vector):\n",
    "\n",
    "\\begin{equation}\n",
    " \\nabla f(\\mathbf{x}) = \\left[\n",
    "\\begin{matrix}\n",
    " \\partial f/ \\partial x_1 \\\\\n",
    " \\vdots \\\\\n",
    "  \\partial f/ \\partial x_n \n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{x}$ is an n-by-1 vector.\n",
    "\n",
    "- For a vector-valued function $\\mathbf{f}: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}^n$, the *Jacobian* (i.e., the matrix of the first derivatives), is defined as \n",
    "\n",
    "\\begin{equation}\n",
    " J(\\mathbf{x}) = \\left[\n",
    "\\begin{matrix}\n",
    " \\partial f_1/ \\partial x_1 & ... & \\partial f_1/ \\partial x_n \\\\\n",
    " \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\partial f_n/ \\partial x_1 & ... & \\partial f_n/ \\partial x_n \n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}\n",
    "\n",
    "Using the gradient notation, we can also write this as\n",
    "\n",
    "\\begin{equation}\n",
    " J(\\mathbf{x}) = \\left[\n",
    "\\begin{matrix}\n",
    "  \\nabla f_1(\\mathbf{x})^T \\\\\n",
    " \\vdots  \\\\\n",
    "  \\nabla f_n(\\mathbf{x})^T\n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Taylor Series and Taylor's Formula\n",
    "\n",
    "For a univariate function $f$ that is $n$ times continuously differentiable, a *Taylor series* or *Taylor approximation* around $x_0$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) \\approx f(x_0) + f'(x_0) (x - x_0) + \\frac{1}{2} f''(x_0) (x - x_0)^2 + ... + \\frac{1}{n!} f^{(n)}(x_0) (x - x_0)^n \n",
    "\\end{equation}\n",
    "\n",
    "Closely related to this is *Taylor's Theorem*: if $f$ is, for example, twice continuously differentiable in an interval that contains $x$ and $x_0$, then \n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = f(x_0) + f'(x_0) (x - x_0) + \\frac{1}{2} f''(c) (x - x_0)^2 \n",
    "\\end{equation}\n",
    "\n",
    "for some number $c$ between $x$ and $x_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a multivariate function $f: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}$, we can state Taylor's Theorem in the following way: if $f$ is twice continuously differentiable and $p \\in \\mathbb{R}^n$, we have that\n",
    "\n",
    "\\begin{equation}\n",
    "    f(\\mathbf{x}_0 + \\mathbf{p}) = f(\\mathbf{x}_0) + \\nabla f(\\mathbf{x}_0)^{T} \\mathbf{p} + \\frac{1}{2} \\mathbf{p}^T \\nabla^2 f(\\mathbf{x}_0 + t\\mathbf{p}) \\mathbf{p} \n",
    "\\end{equation}\n",
    "\n",
    "for some $t \\in (0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'bisection'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bisection\n",
    "\n",
    "The simplest way to compute the root of a continuous univariate real-valued function is the *bisection method*. While simple, bisection captures two important features of most root-finding and optimization methods: it is a *local* method and it is based on an *iterative procedure*.\n",
    "\n",
    "The key idea behind the bisection method is based on the *Intermediate Value Theorem*: if $f$ is continuous and defined on the interval $[a,b]$, and if $f(a)$ and $f(b)$ are distinct values, then $f$ must assume all values in between. Since we are interested in where $f$ assumes the value 0, we need $f(a)$ and $f(b)$ to have *different signs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## cp. figure\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The bisection method implements the following \"pseudo-code\":\n",
    "\n",
    "(i) Start with two distinct values $a$ and $b$, $a < b$, such that $f(a)$ and $f(b)$ are defined and have different signs, i.e. $f(a) \\cdot f(b) < 0$. Moreover, specify a \"tolerance level\" $tol$ which should be a very small number, e.g. 1e-8.\n",
    "\n",
    "(ii) Compute the midpoint between $a$ and $b$, $x = \\frac{a + b}{2}$. \n",
    "\n",
    "(iii) If $f(x)$ has the same sign as $f(a)$, replace the left endpoint of the interval with $x$, i.e. $a = x$.\n",
    "\n",
    "(iv) If $f(x)$ has the same sign as $f(b)$, replace the right endpoint of the interval with $x$, i.e. $b = x$.\n",
    "\n",
    "(v) Check the *stopping rule*: if the absolute value of $f(x)$ is less than the tolerance level, $|f(x)| < tol$, stop and report the solution at $x$. If not, go back to (ii) and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note the following:\n",
    "\n",
    "- Bisection is an *iterative procedure*: at the beginning of each iteration step, the interval $[a,b]$ contains a root of $f$. The interval is then divided (\"bisected\") into two subintervals of equal length. One of the two subintervals must contain the root, and hence have endpoints of different signs. This subinterval is taken as the interval $[a,b]$ used for the next iteration. This process continues until the function value of the midpoint $x$ of the current interval is sufficiently close to 0.  \n",
    "\n",
    "- Moreover, bisection is a *local* method: it will not give you all the roots of a function, but only one of the roots (in case there are multiple roots) between $a$ and $b$. A corollary of this is that the outcome of bisection (and of local methods in general) is sensitive to the starting point chosen by the user, here the values for $a$ and $b$.\n",
    "\n",
    "- The bisection method is robust in the sense that it will find a root in a known number of iterations, assuming the initial choices for $a$ and $b$ lead to different signs for $f(a)$ and $f(b)$. The obvious downside of bisection is that it only works for univariate functions. Moreover, it is usually slower than the other methods discussed below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this week's problem set, you will be asked to code up the bisection method. Of course, most programming languages already have in-built implementations (e.g. in SciPy: **scipy.optimize.bisect**, as discussed below), so writing your own function may seem a bit redundant, but will help you to get used to the inner workings of many of the algorithms used in scientific computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'funiter'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Function Iteration\n",
    "\n",
    "We have started to talk about iterative methods at the end of last lecture. To recap, the basic idea of iterative methods is to generate a sequence of approximations to the object of interest, e.g. the solution to linear or nonlinear system of equations, following an iteration rule: \n",
    "\n",
    "\\begin{equation}\n",
    "    x^{(k+1)} = g( x^{(k)} ),\n",
    "\\end{equation}\n",
    "\n",
    "where $k$ is an indicator counting the number of iterations. Hence, in words, the value for $x$ in the $k+1$-iteration is obtained by applying function $g$ on the value for $x$ in the $k$-iteration. Ideally, these approximations become more and more precise with an increasing number of iterations. Recall that iterative methods, in contrast to direct methods, do not yield an exact solution.\n",
    "\n",
    "\n",
    "When finding the root of a function $f$ or solving for a system of nonlinear equations, the functional form of $g$ is simply\n",
    "\n",
    "\\begin{equation}\n",
    "    g( x ) = x - f(x).\n",
    "\\end{equation}\n",
    "\n",
    "This is intuitive: at the root $ x = x^* $, we have $f(x^*) = 0$ and hence $g (x^*) = x^*$. In other words, $x^*$ is a *fixed point*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following piece of code implements function iteration. As a simple workhorse example, consider the function\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = 4 \\ln(x) - 4,\n",
    "\\end{equation}\n",
    "\n",
    "which has a root at $x = e^1 = 2.718282$. For illustration, we print the current guess for $x^{(k)}$ for each iteration. As we can see,  $x^{(k)}$ converges to $x^*$ as the number of iterations increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    return 4*np.log(x) - 4\n",
    "\n",
    "def g(x):\n",
    "    return x - fun(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.45482255552\n",
      "2.86260463623\n",
      "2.65567694682\n",
      "2.74887857583\n",
      "2.70410642422\n",
      "2.72502036232\n",
      "2.71511676033\n",
      "2.7197769279\n",
      "2.71757746733\n",
      "2.71861408156\n",
      "2.7181251951\n",
      "2.71835569051\n",
      "2.71824700267\n",
      "2.71829824977\n",
      "2.71827408559\n",
      "2.71828547937\n",
      "2.71828010699\n",
      "2.71828264016\n",
      "2.71828144573\n",
      "2.71828200892\n",
      "2.71828174337\n",
      "2.71828186858\n",
      "2.71828180954\n",
      "2.71828183738\n",
      "2.71828182425\n",
      "Number of iterations = 25\n"
     ]
    }
   ],
   "source": [
    "tol = 1e-8\n",
    "x = 4\n",
    "it = 0\n",
    "lst = []\n",
    "while abs((x - g(x))) > tol:\n",
    "    it += 1\n",
    "    x = g(x)\n",
    "    lst.append(x)\n",
    "    print(x)\n",
    "    \n",
    "\n",
    "print(\"Number of iterations = {}\".format(it) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can Python's matplotlib package to illustrate how function iteration works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x3993db7470>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAFXCAYAAABdtRywAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4AAAWOgPbydxsdt/BpsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x399397c3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0.9, 3, 100)\n",
    "fig, ax = plt.subplots(2,1, sharex = True)\n",
    "# ax[0].ylabel($$f(x)$$)\n",
    "ax[0].plot(x, fun(x))\n",
    "ax[0].hlines(0, 0.9, 3)\n",
    "ax[1].plot(x, g(x))\n",
    "ax[1].plot(x, x, '--')\n",
    "ax[1].set_xlabel('$x$')\n",
    "ax[0].set_ylabel('$f(x)$')\n",
    "ax[1].set_ylabel('$g(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'newton'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Newton's Method\n",
    "\n",
    "Most algorithms used in practice to find the roots of a nonlinear system of equations are based on Newton's method. As function iteration, it is an iterative method. However, it uses additional information, namely about the derivative(s) of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Univariate Function\n",
    "\n",
    "Let us start with the case of a univariate function $f$. Recall that we want to find a root $x^*$ of the function $f$, i.e. where $f(x^*) = 0$. Start with an initial guess for $x^*$, denoted by $x_0$. We can approximate $f$ with a first-order Taylor approximation around $x_0$:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) \\approx f(x_0) + (x - x_0) f'(x_0)\n",
    "\\end{equation}\n",
    "\n",
    "Setting $f(x) = 0$ - our target value - and solving this expression for $x$ gives us the \"best guess\" for $x^*$ given the initial guess and the properties of the function (i.e. its value and derivative) at $x_0$:\n",
    "\\begin{equation}\n",
    " x_1 \\approx x_0 - \\frac{f(x_0)}{f'(x_0)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Iterating on this step, we can again generate a sequence $x^{(1)}, x^{(2)}, ..., x^{(n)}$; hence the iteration rule is given by:\n",
    "\n",
    "\\begin{equation}\n",
    " x^{(k+1)} = x^{(k)} - \\frac{f(x^{(k)})}{f'(x^{(k)})}\n",
    "\\end{equation}\n",
    "\n",
    "In other words, the functional form of $g$ is now\n",
    "\\begin{equation}\n",
    "    g( x ) = x - \\frac{f(x)}{f'(x)}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The key idea of Newton's method is *successive linerarization*: a nonlinear problem is replaced with a sequence of linear problems whose solutions converge to the solution of a nonlinear problem. This is illustrated by the following figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## cp. figure in class\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparing to simple function iteration above, we have one additional term, the derivative of $f$ at $x^{(k)}$. Hence, we use more information on the properties of the function than above. More precisely, we put a weight on the distance between the old guess $x^{(k)}$ and the new guess $x^{(k+1)}$. With function iteration, the distance was given by $f(x^{(k)})$, while in Newton's method, it is $f(x^{(k)})/f'(x^{(k)})$. It is intuitive why this is an improvement:\n",
    "- if the absolute value of $f'(x^{(k)})$ is small, this means the function is relatively flat at $x^{(k)}$; in this case, it is likely that the current guess $x^{(k)}$ is still far from the root, and hence the jump to the next guess should be large\n",
    "- if the absolute value of $f'(x^{(k)})$ is large, the function is relatively steep, making it more likely that we are close to  the root; hence the jump to the next guess should be small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Newton's method implements the following pseudo-code:\n",
    "\n",
    "(i) Specify tolerance levels $tol1$ and $tol2$ and choose a starting guess $x^{(0)}$.\n",
    "\n",
    "(ii) Compute the next iterate as \n",
    "\n",
    "\\begin{equation}\n",
    " x^{(k+1)} = x^{(k)} - \\frac{f(x^{(k)})}{f'(x^{(k)})}\n",
    "\\end{equation}\n",
    "\n",
    "(iii) Check the stopping rule: if $|x^{(k+1)}- x^{(k)}| < tol1$, stop. If not, go back to (ii).\n",
    "\n",
    "(iv) If $|f(x^{(k+1)})| < tol2$, report $x^{(k+1)}$ as the solution. Otherwise, report failure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following code implements Newton's method. As before, the current guess for $x$ is printed in every iteration. Unsurprisingly given the intuition above, Newton's method needs considerably fewer iterations than simple function iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def fd(x):\n",
    "    return 4/x\n",
    "\n",
    "def g_newton(fun, fun_d, x):\n",
    "    \"\"\"\n",
    "    Implements the iteration rule for Newton's method. \n",
    "    \"\"\"\n",
    "    f, fd = fun(x), fun_d(x)\n",
    "    return x - f * fd**(-1)\n",
    "\n",
    "def my_newton(fun, fun_d, x, tol1 = 1e-8, tol2 = 1e-8):\n",
    "    \"\"\"\n",
    "    Implements Newton's method. \n",
    "    \"\"\"\n",
    "    eps = 1\n",
    "    it = 0\n",
    "    \n",
    "    while eps > tol1:\n",
    "        it += 1\n",
    "        x_new = g_newton(fun, fun_d, x)\n",
    "        eps = abs(x - x_new)\n",
    "        x = x_new\n",
    "        print(x_new)\n",
    "    \n",
    "    print(\"Number of iterations = {}\".format(it) )\n",
    "    \n",
    "    if abs(fun(x)) < tol2: \n",
    "        return x\n",
    "    else:\n",
    "        print(\"No solution found!\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2.61370563888\n",
      "2.71624392636\n",
      "2.71828106436\n",
      "2.71828182846\n",
      "2.71828182846\n",
      "Number of iterations = 6\n"
     ]
    }
   ],
   "source": [
    "x_root = my_newton(fun, fd, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The stopping criteria should not be set too loosely. For some functions, a higher $tol1$ does not impact the solution from Newton's method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2.61370563888\n",
      "2.71624392636\n",
      "2.71828106436\n",
      "2.71828182846\n",
      "Number of iterations = 5\n"
     ]
    }
   ],
   "source": [
    "x_root = my_newton(fun, fd, 1, tol1 = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For other functions, however, setting $tol1$ too high can result in finding a \"root\" quite far away from zero. This is true for functions that are quite flat around its root. For these functions, we also see very slow convergence. Note that in the example below, $f(x) = x^6$, a small $tol2$ does not compensate for a rather high $tol1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n",
      "0.6944444444444444\n",
      "0.5787037037037037\n",
      "0.48225308641975306\n",
      "0.4018775720164609\n",
      "0.3348979766803841\n",
      "0.2790816472336535\n",
      "0.2325680393613779\n",
      "0.19380669946781492\n",
      "0.16150558288984576\n",
      "0.13458798574153813\n",
      "0.11215665478461512\n",
      "0.09346387898717926\n",
      "0.07788656582264938\n",
      "0.06490547151887449\n",
      "0.05408789293239541\n",
      "0.04507324411032951\n",
      "0.03756103675860792\n",
      "0.031300863965506596\n",
      "0.02608405330458883\n",
      "0.021736711087157357\n",
      "0.018113925905964463\n",
      "0.015094938254970386\n",
      "0.01257911521247532\n",
      "0.010482596010396101\n",
      "0.008735496675330084\n",
      "0.00727958056277507\n",
      "0.006066317135645892\n",
      "0.00505526427970491\n",
      "0.004212720233087425\n",
      "0.0035106001942395207\n",
      "0.002925500161866267\n",
      "0.0024379168015552224\n",
      "0.002031597334629352\n",
      "0.0016929977788577933\n",
      "0.0014108314823814943\n",
      "0.0011756929019845785\n",
      "0.0009797440849871487\n",
      "0.0008164534041559572\n",
      "0.0006803778367966309\n",
      "0.0005669815306638591\n",
      "0.00047248460888654926\n",
      "Number of iterations = 42\n",
      "1.1125665436700811e-20\n"
     ]
    }
   ],
   "source": [
    "def fun2(x):\n",
    "    return x**6\n",
    "def fd2(x):\n",
    "    return 6 * x**(5)\n",
    "\n",
    "x_root = my_newton(fun2, fd2, 1, tol1 = 1e-4, tol2 = 1e-10)\n",
    "print(fun2(x_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Multivariate Case\n",
    "\n",
    "The logic from the univariate case translates to a vector-valued function $\\mathbf{f}: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}^n$.  Recall that its Jacobian is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    " J(\\mathbf{x}) = \\left[\n",
    "\\begin{matrix}\n",
    " \\partial f_1/ \\partial x_1 & ... & \\partial f_1/ \\partial x_n \\\\\n",
    " \\vdots & \\ddots & \\vdots \\\\\n",
    "  \\partial f_n/ \\partial x_1 & ... & \\partial f_n/ \\partial x_n \n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}\n",
    "\n",
    "Start with a first-order Taylor approximation around $\\mathbf{x}_0$:\n",
    "\n",
    "\\begin{equation}\n",
    " 0 = \\mathbf{f}(\\mathbf{x}) \\approx \\mathbf{f}(\\mathbf{x}_0) + J(\\mathbf{x}) (\\mathbf{x} - \\mathbf{x}_0)\n",
    "\\end{equation}\n",
    "\n",
    "As a side note, to see that a Taylor approximation works also in the case of vector-valued functions, note that we can apply it element-wise, i.e. \n",
    "\n",
    "\\begin{equation}\n",
    " f_j(\\mathbf{x}) \\approx f_j(\\mathbf{x}_0) + f_j(\\mathbf{x})^T (\\mathbf{x} - \\mathbf{x}_0)\n",
    "\\end{equation}\n",
    "\n",
    "and recall from above that\n",
    "\n",
    "\\begin{equation}\n",
    " J(\\mathbf{x}) = \\left[\n",
    "\\begin{matrix}\n",
    "  \\nabla f_1(\\mathbf{x})^T \\\\\n",
    " \\vdots  \\\\\n",
    "  \\nabla f_n(\\mathbf{x})^T\n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hence,\n",
    "\\begin{equation}\n",
    " \\mathbf{x} \\approx \\mathbf{x}_0 - J^{-1}(\\mathbf{x}_0) f(\\mathbf{x}_0)\n",
    "\\end{equation}\n",
    "\n",
    "The key idea is to use this relation iteratively, i.e. generate a sequence $\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, ..., \\mathbf{x}^{(m)}$ where\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathbf{x}_{k+1} \\approx \\mathbf{x}_{k} - J^{-1}(\\mathbf{x}_{k}) f(\\mathbf{x}_{k})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As an example, consider the function\n",
    "\n",
    "\\begin{equation}\n",
    " f(\\mathbf{x}) = \\left[\n",
    "\\begin{matrix}\n",
    " x_2^2 - 1 \\\\\n",
    "  \\sin{x_1} - x_2\n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}\n",
    "\n",
    "To apply Newton's method, start by coding up the function and its Jacobian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def foc(x):\n",
    "    \"\"\"\n",
    "    Implements a system of equation in two unknowns, here f(x) = [x2**2 - 1; sin(x1) - x2]\n",
    "    \"\"\"\n",
    "    return np.array( (x[1]**2 - 1 , np.sin(x[0]) - x[1] ) )\n",
    "\n",
    "\n",
    "def foc_J(x):\n",
    "    \"\"\"\n",
    "    Implements the Jacobian system of equation in two unknowns above\n",
    "    \"\"\"\n",
    "    f_00 = 0\n",
    "    f_01 = 2 * x[1]\n",
    "    f_10 = np.cos(x[0])\n",
    "    f_11 = -1\n",
    "    \n",
    "    return np.array([[f_00, f_01], [f_10, f_11]])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations = 24\n",
      "[ 1.57079633  1.        ]\n"
     ]
    }
   ],
   "source": [
    "def my_newton_mult(fun, fun_d, x,  tol = 1e-8):\n",
    "    \"\"\"\n",
    "    Implements Newton's method for a vector-valued function\n",
    "    \"\"\"    \n",
    "    eps = 1\n",
    "    it = 0\n",
    "    while eps > tol:\n",
    "        it += 1\n",
    "        f, J = fun(x), fun_d(x)\n",
    "        x_new = x - np.linalg.inv(J) @ f\n",
    "        eps = np.linalg.norm(x - x_new)\n",
    "        x = x_new\n",
    "    \n",
    "    print(\"Number of iterations = {}\".format(it) )\n",
    "    \n",
    "    return x\n",
    "        \n",
    "x_init = [1.5,0.9]\n",
    "x = my_newton_mult(foc, foc_J, x_init)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations = 27\n",
      "[ 4.71238897 -1.        ]\n"
     ]
    }
   ],
   "source": [
    "x_init = [3,-0.8]\n",
    "x = my_newton_mult(foc, foc_J, x_init)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As bisection, Newton's method is a local method, i.e. does not necessarily find the global optimum. In the case of multiple solutions, like in the example above, it converges to one solution - which one depends on the starting point.\n",
    "\n",
    "Moreover, it is not guaranteed that the iterates in Newton's method converge, in particular for \"erratic\" functions with high derivatives that change sign frequently. In such cases, Newton's method converges only for initial starting values that are sufficiently close to a root $\\mathbf{x}^*$ (always assuming that the Jacobian $J$ is invertible and well-conditioned at  $\\mathbf{x}^*$).\n",
    "\n",
    "We can modify Newton's algorithm as outlined above to increase the likelihood of convergence, by incorporating *backstepping*. Intuitively, if the sequence of iterates $\\mathbf{x}^{(k)}$ \"goes in the wrong direction\" - that is, if the distance to the root gets larger instead of smaller as $k$ increases - we decrease the size of the \"step\" to the next iterate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## cp. classroom notes\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If $J$ is ill-conditioned at  $\\mathbf{x}^*$, it can lead to inaccurately computed updates $\\mathbf{x}^{(k)}$, which may prevent Newton's method from converging. As outlined in the last lecture, ill-conditioning can stem from units of measurement that vary vastly in their order of magnitude. Rescaling variables so that their values have comparable orders of magnitudes may be a remedy here (and is a good idea in general)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = \"numdiff\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numerical Differentiation\n",
    "\n",
    "Before moving on, it is useful to look at *numerical differentiation*: instead of working with precise derivatives of a function, we can use numerical approximations for these derivatives. This is very useful in particular when the function is complicated, and hence its precise derivatives are hard to obtain.  \n",
    "\n",
    "Numerical derivatives are based on *finite differences*. In the one-dimensional case, we have \n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial f}{\\partial x} \\approx \\frac{f(x + \\epsilon) - f(x)}{\\epsilon}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\epsilon$ is small. In other words, the first derivative at $x$ is approximated by the response to a small perturbation of $x$. Note that the approximation above is called the *forward-difference* or *one-sided-difference*. Similarly, the *central-difference* is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial f}{\\partial x} \\approx \\frac{f(x + \\epsilon)- f(x - \\epsilon)}{2 \\epsilon}\n",
    "\\end{equation}\n",
    "\n",
    "Both expression are intuitive to derive graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## cp. figure in class\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Formally, finite differencing is based on Taylor's Theorem, as shown below for a multivariate function $f: \\mathbb{R}^n\\ \\rightarrow \\mathbb{R}$. In this case, we can write the forward-difference formula as:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x + \\epsilon e_i) - f(x)}{\\epsilon},\n",
    "\\end{equation}\n",
    "where $e_i$ is the corresponding canonical vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## cp. classroom notes: finite difference and Taylor's Theorem\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What value should be chosen for $\\epsilon$? A good rule of thumb is the square root of machine epsilon:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\epsilon = \\sqrt{\\epsilon_{DP}} \\approx 10^{-8}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a vector-valued function, a similar derivation gives:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "     J(\\mathbf{x}) \\mathbf{p} \\approx \\frac{\\mathbf{f}(\\mathbf{x}_0 + \\epsilon \\mathbf{p}) - \\mathbf{f}(\\mathbf{x}_0)}{\\epsilon}\n",
    "\\end{equation}\n",
    "\n",
    "Note that this expression does not give us an approximation for the full Jacobian, but rather for the product $J(\\mathbf{x}) \\mathbf{p}$. It requires two function evaluations; it can be shown that a complete approximation of the Jacobian would require $n + 1$ function evaluations for a function of dimension $n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### In Python\n",
    "\n",
    "Numerical differentation is helpful when using Newton's method, as outlined above. One common problem with Newton's method are programming errors when coding the Jacobian, in particular for complicated functions. An easy check is to compare the analytic Jacobian with its finite-difference counterpart. Python has a package **statsmodels** that includes a routine for numerical differentation of a vector-valued function, as illustrated by the following example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,  -2.00000000e+00],\n",
       "       [ -1.08160913e-08,  -1.00000000e+00]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foc_J(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,  -1.99999999e+00],\n",
       "       [  2.37159347e-08,  -1.00000000e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "sm.tools.numdiff.approx_fprime(x, foc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As a side note, the Scipy package (discussed in more detail below) has a function of the same name, **scipy.optimize.approx_fprime**. In contrast to the **statsmodels** version, this works only for univariate and multivariate scalar functions; in the multivariate case, this computes the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1428571428571428"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## precise derivative of the example function below\n",
    "fd(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1428571])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## numerical derivative\n",
    "scipy.optimize.approx_fprime([3.5], fun, [1e-8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'quasi'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quasi-Newton Methods\n",
    "\n",
    "There is an obvious cost of using Newton's method as outlined above: we need to provide the analytical derivative of a univariate scalar function or the Jacobian of a vector-valued function, respectively. While this may be not a big deal for simple functions as in the examples above, for more complicated problems, this step may involve a large cost in terms of time for computing the derivatives and for coding them up. Moreover, as mentioned above, coding up complicated derivatives increases the risk of programming errors.\n",
    "\n",
    "Hence, in practice we often rely on \"derivative-free\" or *Quasi-Newton* methods. In a nutshell, their basic idea is the same as in Newton method's - successive linearization - but instead of using the precise derivatives of a function, we approximate them numerically. A drawback is that we have to provide an initial guess for the function's derivative or Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Univariate Functions: Secant Method\n",
    "\n",
    "In the one-dimensional case, an obvious idea would be to obtain the derivative in Newton's method by numerical differentation:\n",
    "\n",
    "\\begin{equation}\n",
    "    f'(x^{(k)}) \\approx \\frac{f(x^{(k)} + \\epsilon) - f(x^{(k)})}{\\epsilon}\n",
    "\\end{equation}\n",
    "\n",
    "While this would work, in practice we use the *secant method*, where the finite-difference approximation of $f'(x)$ is constructed from the function values at the current and the previous iterate:\n",
    "\n",
    "\\begin{equation}\n",
    "    f'(x^{(k)}) \\approx \\frac{f(x^{(k)}) - f(x^{(k - 1)})}{x^{(k)} - x^{(k-1)}} \\equiv (f')^{(k)}\n",
    "\\end{equation}\n",
    "\n",
    "The main advantage is that this reduces the number of function evaluations: since $f(x^{(k - 1)})$ was evaluated anyway when updating the guess for $x$, it is easy to store and use for the update of $f'(x)$. In the first expression, we would have to *additionally* evaluate $f(x^{(k)} + \\epsilon)$ in every iteration. Note that the computational cost may not be large for simple function, but can be substantial for more complicated functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## cp. classroom notes\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the secant method generates not only a sequence $x^{(k)}$, but also a sequence $(f')^{(k)}$ of approxiations of the derivative. In general, while $x^{(k)}$ converge to the root of the the function, $(f')^{(k)}$ does *not* converge to the derivative of $f$ at the root. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this week's problem set, you are asked to to implement this function numerically. Intuitively, the secant method requires more iterations and hence more function evaluations than Newton's method due to the approximation of the derivative. For more complex functions, however, this doesn't necessarily translate into a longer running time, as the secant method does not require to evaluate the function's derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Multivariate Functions: Broyden's Method\n",
    "\n",
    "The cost of computing exact analytical derivatives increases quadratically in the number of dimensions. Hence, in practice, we usually rely on the multidimensional equivalent to the secant method, *Broyden's Method*. \n",
    "\n",
    "Recall from above that we can approximate the Jacobian of a vector-valued function with\n",
    "\n",
    "\\begin{equation}\n",
    "     J(\\mathbf{x}) \\mathbf{p} \\approx \\frac{\\mathbf{f}(\\mathbf{x}_0 + \\epsilon \\mathbf{p}) - \\mathbf{f}(\\mathbf{x}_0)}{\\epsilon}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Analogous to the one-dimensional case, we will work with an approximation $A$ where $\\epsilon = 1$. Setting $\\mathbf{x}_1 = \\mathbf{x}_0 + \\mathbf{p}$, define $A$ such that\n",
    "\n",
    "\\begin{equation}\n",
    " A \\mathbf{p} = \\mathbf{f}(\\mathbf{x}_1) - \\mathbf{f}(\\mathbf{x}_0).\n",
    "\\end{equation}\n",
    "\n",
    "In other words, a numerical approximation $A$ of the Jacobian should satisfy the *secant condition*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## cp. classroom notes\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'convergence'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convergence\n",
    "\n",
    "All of the methods above were iterative, i.e. generating sequences $x^{(k)}$ (hopefully) converging to the root of the function. Importanly, a sequence of iterates $x^{(k)}$ converges to $x^*$ at a rate of order $p$ if there is a constant $C$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "    || x^{(k+1)} - x^* || \\le C || x^{(k)} - x^* ||^p\n",
    "\\end{equation}\n",
    "\n",
    "for sufficiently large $k$. We can determine $C$ and $p$ for the different methods above:\n",
    "\n",
    "- Bisection converges with $C = 0.5$ and $p = 1$, and hence at a *linear rate*\n",
    "\n",
    "- Function iteration converges also at a linear rate, with $C$ equal to $f'(x^*)$\n",
    "\n",
    "- The secant/Broyden's method converges at a *superlinear rate*, with $1 < p \\approx 1.62 < 2$\n",
    "\n",
    "- Newton's method converges at a *quadratic rate* of $p = 2$\n",
    "\n",
    "Compare the example in M&F, section 3.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary: What method to pick?\n",
    "\n",
    "- Newton's method has the fastest rate of convergence, but can require a lot of \"developmental\" effort; it should be used for nonlinear equations of small dimensions, when the derivatives are not too hard to find and to code and when a equation is to be solved many times\n",
    "\n",
    "- The secant/Broyden's method has a smaller rate of convergence (but still more than linear) and requires less programming time; it should be used when the derivatives are expensive to compute and code, and the equation is not solved too often\n",
    "\n",
    "- In the univariate case, the bisection method is the most robust; for highly irregular functions, it can be used in combination with Newton's method, to find a close initial guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'scipy'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Scipy Package\n",
    "\n",
    "Modern programming languages have built-in implementations of the algorithms outlined above. In Python, we mainly rely on the **Scipy** package, which comes with the Anaconda distribution. For rootfinding and numerical optimization (in the next lecture), we use Scipy's subpackage **scipy.optimize**, which we first need to import: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One dimension\n",
    "\n",
    "For the univariate case, consider again the function \n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = 4 \\ln(x) - 4\n",
    "\\end{equation}\n",
    "\n",
    "We define the function and use the **bisect()** function, an implementation of the bisection method outlined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459567\n"
     ]
    }
   ],
   "source": [
    "def fun(x):\n",
    "    return 4*np.log(x) - 4\n",
    "\n",
    "print(scipy.optimize.bisect(fun,1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**bisect(fun,a,b)** takes three arguments: the function **fun** (which can be built-in or user-written), and an upper and lower initial guess for the root. In other words, you tell the algorithm to look for a root in the interval $[a,b]$. The important thing to remember here is that $f(a)$ and $f(b)$ must have different signs - if they do not, you will get an error message (in this case, change $a$ or $b$ and try again).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the example above, solving for the root using Python is not really necessary. The real advantage of numerical root finding is in situations where finding a solution to $f(x) = 0$ analytically is not feasible. Consider, for example,\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = \\sin(4 (x - 1/4)) + x + x^{20} - 1\n",
    "\\end{equation}\n",
    "\n",
    "We can Matplotlib to plot the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x39956612b0>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFJCAYAAAC2OXUDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3995661da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fun(x):\n",
    "    return np.sin(4 * (x - 0.25)) + x + x**20 - 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(0.3, 1, 50)\n",
    "ax.plot(x, fun(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finding a root via the bisection method is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4082935042806639\n"
     ]
    }
   ],
   "source": [
    "print(scipy.optimize.bisect(fun,0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Scipy function **newton(fun, x, fprime = None)** implements both Newton's method (if the derivate of the function is given as **fprime**)  and the secant method for univariate rootfinding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.408293504279367"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fun_d(x):\n",
    "    return np.cos(4 * (x - 0.25)) * 4 + 1 + 20 * x**19\n",
    "\n",
    "scipy.optimize.newton(fun,0.6, fun_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40829350427936667"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.newton(fun,0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use Jupyter's **%timeit** magic to compare the running time of each of the three methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 308 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit -n1 scipy.optimize.bisect(fun,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 97.1 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit -n1 scipy.optimize.newton(fun,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 111 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit -n1 scipy.optimize.newton(fun,0.6, fun_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For comparison, our implementation of Newton's method above takes about the same running time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40829350427936706"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_newton(fun, fun_d, x, tol1 = 1e-8, tol2 = 1e-8):\n",
    "    \n",
    "    eps = 1\n",
    "    it = 0\n",
    "    maxit = 100\n",
    "    \n",
    "    while eps > tol1 and it < maxit:\n",
    "        it += 1\n",
    "        x_new = g_newton(fun, fun_d, x)\n",
    "        eps = abs(x - x_new)\n",
    "        x = x_new\n",
    "    \n",
    "    if abs(fun(x)) < tol2: \n",
    "        return x\n",
    "    else:\n",
    "        print(\"No solution found!\")\n",
    "        \n",
    "\n",
    "my_newton(fun, fun_d, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 120 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit -n1 my_newton(fun, fun_d, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiple Dimensions: NGM Revisited\n",
    "\n",
    "As an example for a multidimensional system of nonlinear equation, let's go back to our NGM model. Recall that a steady state is given by $(k_s, h_s)$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[\n",
    "    \\begin{array}{c}\n",
    "        S_1 \\\\\n",
    "        S_2\n",
    "    \\end{array}\n",
    "    \\right] =    \n",
    "    \\left[\n",
    "    \\begin{array}{c}\n",
    "        \\beta \\left[f_k(k_s, h_s) + 1 - \\delta \\right]  - 1 \\\\\n",
    "        \\left[ f(k_s, h_s) - \\delta k \\right]^{-\\nu} f_h(k_s, h_s) - B h_s^{\\eta}\n",
    "    \\end{array}\n",
    "    \\right] = \n",
    "    \\left[\n",
    "    \\begin{array}{c}\n",
    "        0 \\\\\n",
    "        0\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "\\end{equation}\n",
    "\n",
    "To solve this system numerically, we first need to assign values to the model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## utility\n",
    "beta = 0.8      # discount factor\n",
    "nu = 2       # risk-aversion coefficient for consumption\n",
    "eta = 1         # elasticity parameter for labor supply\n",
    "eps = 1e-6      # lower bound of consumption and labor supply\n",
    "## production\n",
    "alpha = 0.25\n",
    "delta = 0\n",
    "## derived\n",
    "# A = (1 - beta * (1 - delta))/(alpha*beta) # normalization parameter for production function => steady state k = 1\n",
    "# B = (1 - alpha) * A * (A - delta)**nu      # parameter for utility function\n",
    "B = 0.8\n",
    "A = 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Functions\n",
    "\n",
    "Next, it will be useful to define some auxiliary functions that implement the Cobb-Douglas production function, as well as its first and second derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def cobb_douglas(x, alpha, A):\n",
    "    \"\"\"\n",
    "    Evaluates the Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return A * x[0]**alpha * x[1]**(1 - alpha)\n",
    "\n",
    "def cd_diff(x, alpha, A):\n",
    "    \"\"\"\n",
    "    Evaluates the first derivatives (returned as a tuple) of the Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return (A * alpha * cobb_douglas(x, alpha, A) / x[0], \n",
    "            A * (1 - alpha) * cobb_douglas(x, alpha, A) / x[1])\n",
    "\n",
    "def cd_diff2(x, alpha, A):\n",
    "    \"\"\"\n",
    "    Evaluates the second derivative (returned as a tuple, with the cross derivative as the last element) of the Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return (A * alpha * (alpha - 1) * cobb_douglas(x, alpha, A) / x[0]**2, \n",
    "            A * (1 - alpha) * (-alpha) * cobb_douglas(x, alpha, A) / x[1]**2,\n",
    "            A * alpha * (1 - alpha) * cobb_douglas(x, alpha, A) / (x[0] * x[1]) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, we can code up the system of nonlinear equations $S$ as a Numpy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def steady(x):\n",
    "    \"\"\"\n",
    "    Returns the vector-valued function consisting of the steady-state conditions \n",
    "    \"\"\"\n",
    "    y = np.zeros(2)\n",
    "    mp = cd_diff(x, alpha, A)\n",
    "    \n",
    "    y[0] = beta * (mp[0] + 1 - delta) - 1\n",
    "    y[1] = (cobb_douglas(x, alpha, A) - delta * x[0])**(-nu) * mp[1] - B * x[1]**eta\n",
    "    \n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For Newton's method, we also need to provide the Jacobian, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    " J(k, h) = \\left[\n",
    "\\begin{matrix}\n",
    " \\partial S_1/ \\partial k &  \\partial S_1/ \\partial h \\\\\n",
    "  \\partial S_2/ \\partial k &  \\partial S_2 / \\partial h \n",
    "\\end{matrix}  \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def steady_jac(x):\n",
    "    \"\"\"\n",
    "    Returns the Jacobian of the vector-valued function consisting of the steady-state conditions \n",
    "    \"\"\"\n",
    "    J = np.zeros((2,2))\n",
    "    mp = cd_diff(x, alpha, A)\n",
    "    mp2 = cd_diff2(x, alpha, A)\n",
    "    \n",
    "    Q = cobb_douglas(x, alpha, A) - delta * x[0]\n",
    "    \n",
    "    J[0,0] = beta * mp2[0] \n",
    "    J[0,1] = beta * mp2[2]\n",
    "    J[1,1] = -nu * Q**(-nu-1) * mp[1]**2 + Q**(-nu) * mp2[1] - B * eta * x[1]**(eta - 1)\n",
    "    J[1,0] = -nu * Q**(-nu-1) * mp[1] * (mp[0] - delta) + Q**(-nu) * mp2[2] \n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solve for the steady state\n",
    "\n",
    "Start by using our implementation of Newton's method written above. We also need to provide an initial guess $x_0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations = 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.23548993,  0.95820563])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([0.5, 0.5])\n",
    "my_newton_mult(steady, steady_jac, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, we use Scipy's **optimize.broyden1** function, an implementation of Broyden's method outlined above. As it is derivative-free, we do not have to provide the Jacobian: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.23548994  0.95820563]\n"
     ]
    }
   ],
   "source": [
    "res = scipy.optimize.broyden1(steady, x0, f_tol = 1e-8)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that alternatively, you can also call Scipy's **optimize.root** function, which is essentially a \"wrapper\" around different algorithms for solving nonlinear systems of equations, not only Broyden's method. I usually use the **root** function since it provides a more informative output, in particular on function values and number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: array([  7.35234096e-12,  -1.00914832e-11])\n",
      " message: 'A solution was found at the specified tolerance.'\n",
      "     nit: 20\n",
      "  status: 1\n",
      " success: True\n",
      "       x: array([ 1.23548993,  0.95820563])\n"
     ]
    }
   ],
   "source": [
    "res = scipy.optimize.root(steady, x0,  tol = 1e-8, method = \"broyden1\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As expected, Broyden's method takes more iterations to solve the system than Newton's method."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
