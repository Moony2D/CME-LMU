{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computational Methods in Economics\n",
    "\n",
    "## Lecture 3 - Solving Systems of Linear Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last update: 2018-11-15 09:09:15.238850\n"
     ]
    }
   ],
   "source": [
    "# Author: Alex Schmitt (schmitt@ifo.de)\n",
    "\n",
    "import datetime\n",
    "print('Last update: ' + str(datetime.datetime.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "\n",
    "import sys\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Lecture\n",
    "\n",
    "- [Review: Matrices, Linear Independence, Non-Singularity](#matrix)\n",
    "- [Systems of Linear Equations](#sle)\n",
    "- [Solving Triangular Linear Systems](#trile)\n",
    "- [LU Factorization using Gaussian Elimination](#lufac)\n",
    "- [Ill-Conditioned Matrices](#ill)\n",
    "- [Sparse Matrices](#sparse)\n",
    "- [Iterative Methods](#iterative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "<a id ='matrix'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $A$ be a m-by-n matrix:\n",
    "\n",
    "\\begin{equation}A =\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "    a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "    a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "    \\vdots & \\vdots &  & \\vdots \\\\\n",
    "    a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A n-by-n matrix is called a *square* matrix of order $n$:\n",
    "\n",
    "\\begin{equation}A =\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "    a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "    a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "    \\vdots & \\vdots &  & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} & \\cdots & a_{nn}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A matrix $D$ of order $n$ is *diagonal* if all its non-zero elements are on its diagonal (i.e. the entries $a_{ij}$ with $i = j$):\n",
    "\n",
    "\\begin{equation}D =\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "    a_{11} & 0 & \\cdots & 0 \\\\\n",
    "    0 & a_{22} & \\cdots & 0 \\\\\n",
    "    \\vdots & \\vdots &  & \\vdots \\\\\n",
    "    0 & 0 & \\cdots & a_{nn}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A matrix $L$ of order $n$ is *lower triangular* if all its non-zero elements are either diagonal entries or *strictly lower triangular* entries (i.e. the entries $a_{ij}$ for which $i > j$):\n",
    "\n",
    "\\begin{equation}L =\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "    a_{11} & 0 & \\cdots & 0 \\\\\n",
    "    a_{21} & a_{22} & \\cdots & 0 \\\\\n",
    "    \\vdots & \\vdots &  & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} & \\cdots & a_{nn}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A matrix $U$ of order $n$ is *upper triangular* if all its non-zero elements are either diagonal entries or *strictly upper triangular* entries (i.e. the entries $a_{ij}$ for which $i < j$):\n",
    "\n",
    "\\begin{equation}U =\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "    a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "    0 & a_{22} & \\cdots & a_{2n} \\\\\n",
    "    \\vdots & \\vdots &  & \\vdots \\\\\n",
    "    0 & 0 & \\cdots & a_{nn}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Matrices in Python\n",
    "\n",
    "In Python, matrices are best coded as *Numpy arrays*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  0 -1  2]\n",
      " [ 4  2 -1  4]\n",
      " [ 2 -2 -2  3]\n",
      " [-2  2  7 -3]]\n"
     ]
    }
   ],
   "source": [
    "## define matrix\n",
    "A = np.array([[2, 0, -1, 2],\n",
    "              [4, 2, -1, 4],\n",
    "              [2, -2, -2, 3],\n",
    "              [-2, 2, 7, -3]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "## get dimension of matrix\n",
    "print( A.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "## access elements by indices\n",
    "print(A[2,1])\n",
    "print(A[3,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that the **transpose** $A'$ (or $A^T$) of matrix $A$ is formed by replacing $a_{ij}$ with $a_{ji}$ for every $i$ and $j$. If $A = A'$, the matrix is **symmetric**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  4  2 -2]\n",
      " [ 0  2 -2  2]\n",
      " [-1 -1 -2  7]\n",
      " [ 2  4  3 -3]]\n"
     ]
    }
   ],
   "source": [
    "## transpose matrix\n",
    "print(A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Matrix addition and scalar multiplication are straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  0 -3  6]\n",
      " [12  6 -3 12]\n",
      " [ 6 -6 -6  9]\n",
      " [-6  6 21 -9]]\n"
     ]
    }
   ],
   "source": [
    "## scalar multiplication\n",
    "B = 3 * A\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8   0  -4   8]\n",
      " [ 16   8  -4  16]\n",
      " [  8  -8  -8  12]\n",
      " [ -8   8  28 -12]]\n"
     ]
    }
   ],
   "source": [
    "## matrix addition\n",
    "print( A + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that when multiplying two matrices $A$ and $B$, their product $A \\cdot B$ is formed by computing the *inner product* of the $i$th row of $A$ and the $j$th column of $B$, and assigning the result to the $i,j$th element. Matrix multiplication in Python 3 is implemented using **@**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -6   18   42  -15]\n",
      " [  18   42   72    3]\n",
      " [ -42   18   75  -57]\n",
      " [  72  -48 -105  102]]\n"
     ]
    }
   ],
   "source": [
    "## matrix multiplication\n",
    "print( A @ B )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that $AB$ and $BA$ are in general not the same. \n",
    "\n",
    "Also recall that multiplying a matrix $A$ with the identity matrix gives $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  0. -1.  2.]\n",
      " [ 4.  2. -1.  4.]\n",
      " [ 2. -2. -2.  3.]\n",
      " [-2.  2.  7. -3.]]\n"
     ]
    }
   ],
   "source": [
    "## with identity matrix\n",
    "print( np.eye(4) @ A )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When multiplying a matrix and a vector, make sure that the vector has the right dimension -\"flat\" arrays work either way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 21  4 11]\n",
      "[ 8  6 19  7]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "print(A @ x)\n",
    "print(x @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  6 19  7]]\n"
     ]
    }
   ],
   "source": [
    "## \"row vector\" (1-by-4)\n",
    "x = np.array([[1, 2, 3, 4]])\n",
    "print(x @ A) \n",
    "# print(A @ x) # this one won't work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7]\n",
      " [21]\n",
      " [ 4]\n",
      " [11]]\n"
     ]
    }
   ],
   "source": [
    "## \"column vector\" (4-by-1)\n",
    "x = np.array([[1], [2], [3], [4]])\n",
    "print(A @ x) \n",
    "# print(x @ A) # this one won't work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----------------------------------------------------------------------------------------------------------------------\n",
    "<a id ='span'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Review: Span and Linear Independence\n",
    "\n",
    "For what follows below, it is useful to review the definition of linear independence of a collection of *vectors*. For this, we first need to define a *span*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Span\n",
    "\n",
    "Let $A$ be a collection of (column) vectors: $A = \\{ a_1, a_2, ..., a_n \\}$ where\n",
    "\n",
    "\\begin{equation}a_j =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    a_{1j} \\\\\n",
    "    a_{2j}  \\\\\n",
    "    \\vdots \\\\\n",
    "    a_{mj} \n",
    "\\end{array}\n",
    "\\right] \\in \\mathbb{R}^m\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$y \\in \\mathbb{R}^m$ is a **linear combination** of $A$ if \n",
    "\n",
    "\\begin{equation}\n",
    "    y = \\beta_1 a_1 + \\beta_2 a_2 + ... + \\beta_n a_n\n",
    "\\end{equation}\n",
    "\n",
    "for some (scalar) coefficients $\\beta_1, ..., \\beta_n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The set of *all* linear combinations of $A$ is called the **span** of $A$. In other words, the span is the set of vectors that can be created by applying vector addition and scalar multiplications on the vectors in $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A special case is the set of *canonical vectors* $A = \\{ e_1, e_2, ..., e_n \\}$ where\n",
    "\n",
    "\\begin{equation}e_1 =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    1 \\\\\n",
    "    0  \\\\\n",
    "    \\vdots \\\\\n",
    "    0 \n",
    "\\end{array}\n",
    "\\right],\\ e_2 =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    0 \\\\\n",
    "    1  \\\\\n",
    "    \\vdots \\\\\n",
    "    0 \n",
    "\\end{array}\n",
    "\\right],\\ ...\\ \\in \\mathbb{R}^n \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this case, the span of $A$ is $\\mathbb{R}^n$: for any $y = (y_1, ..., y_n) \\in \\mathbb{R}^n$, we can write \n",
    "\n",
    "\\begin{equation}\n",
    "    y = y_1 e_1 + ... + y_n e_n.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Linear Independence\n",
    "\n",
    "A collection of vectors $A = \\{ a_1, a_2, ..., a_n \\}$ in $\\mathbb{R}^m$ is \n",
    "- **linearly dependent** if some strict subset of $A$ has the same span as $A$\n",
    "- **linearly independent** if it is not linearly dependent. In other words, a set of vectors is linearly independent if none of the vectors is redundant to the span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, the following is true for a linearly independent set of vectors $A$:\n",
    "1. no vector in $A$ can be formed as a linear combination of the other vectors\n",
    "2. if \n",
    "\\begin{equation} \n",
    "     \\beta_1 a_1 + \\beta_2 a_2 + ... + \\beta_n a_n = 0,\n",
    "\\end{equation}\n",
    "\n",
    "then $\\beta_1 = ... = \\beta_n = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. if\n",
    "\\begin{equation}\n",
    "    y = \\beta_1 a_1 + \\beta_2 a_2 + ... + \\beta_n a_n,\n",
    "\\end{equation}\n",
    "\n",
    "then no other coefficient sequence $\\gamma_1, ... , \\gamma_n$ will produce the same vector $y$.\n",
    "\n",
    "Linear independence is useful, since it implies that the set of vectors have a \"large span\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id ='sing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Review: Nonsingular vs. singular matrix\n",
    "\n",
    "As you may recall from your linear algebra class, an important feature of a square matrix is whether or not it is *invertible*. An invertible matrix is also called *non-singular*, while a *singular* matrix cannot be inverted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This concept is closely related to linear independence of a collection of vectors. If the column vectors of a square matrix are linearly independent, it has **full column rank** (an analogous definition exists for the row vectors). \n",
    "\n",
    "A matrix $A$ is nonsingular *if and only if it has full column rank*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An alternative way to check whether a square matrix has an inverse is computing its **determinant** (I skip the definition here, but feel free to look it up if you don't remember). \n",
    "\n",
    "A matrix $A$ is nonsingular if and only if its determinant is not zero. In other words, a square matrix with linearly independent column vectors has a non-zero determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### In Python\n",
    "\n",
    "Numpy's **linalg** module has functions to compute the determinant (**det()**) and the rank (**matrix_rank()**) of a matrix, as well as for inverting a matrix (**inv()**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-19.99999999999999\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2, 0, -1, 2],\n",
    "              [4, 2, -1, 4],\n",
    "              [2, -2, -2, 3],\n",
    "              [-2, 2, 7, -3]])\n",
    "\n",
    "## compute the determinant of a matrix\n",
    "print( np.linalg.det(A) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "## compute the rank of a matrix\n",
    "print( np.linalg.matrix_rank(A) )\n",
    "print( np.linalg.matrix_rank(A.T) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.50000000e+00 -1.00000000e+00 -9.00000000e-01  1.00000000e-01]\n",
      " [-1.00000000e+00  5.00000000e-01 -1.00000000e-01 -1.00000000e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  2.00000000e-01  2.00000000e-01]\n",
      " [-3.00000000e+00  1.00000000e+00  1.00000000e+00 -1.66533454e-17]]\n"
     ]
    }
   ],
   "source": [
    "## compute the inverse of a matrix\n",
    "print( np.linalg.inv(A) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are examples of singular matrices. It is easy to see that in both cases, the column vectors are not linearly independent.\n",
    "\n",
    "Hence, the determinants are zero, and the ranks are not full. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0],\n",
    "              [1, 2]])\n",
    "\n",
    "print(np.linalg.det(X))\n",
    "print(np.linalg.matrix_rank(X)) \n",
    "print(np.linalg.matrix_rank(X.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 0, 0],\n",
    "              [4, 0, 0],\n",
    "              [2, 3, 5]])\n",
    "\n",
    "print( np.linalg.det(A) )\n",
    "print(np.linalg.matrix_rank(A)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id ='sle'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## System of Linear Equations\n",
    "\n",
    "A system of $m$ linear equations in $n$ unknowns $x_1, x_2, ... , x_n$ can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{array}{c}\n",
    "    a_{11} x_1 + \\cdots + a_{1n} x_n = b_1\\\\\n",
    "    \\vdots \\\\\n",
    "    a_{m1} x_1 + \\cdots + a_{mn} x_n = b_m\n",
    "\\end{array}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $x$ be a n-by-1 vector: $x = [x_1, x_2, ... , x_n]'$. Then, the system can be written in matrix form as $Ax = b$, since\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}A x\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "    a_{11} &  \\cdots & a_{1n} \\\\\n",
    "    \\vdots & \\vdots  & \\vdots \\\\\n",
    "    a_{m1} &  \\cdots & a_{mn}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    x_{1}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    x_{n}\n",
    "\\end{array}\n",
    "\\right] \n",
    "&=\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    a_{11} x_1 + \\cdots + a_{1n} x_n \\\\\n",
    "    \\vdots \\\\\n",
    "    a_{m1} x_1 + \\cdots + a_{mn} x_n\n",
    "\\end{array}\n",
    "\\right]\n",
    " = \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    b_{1}  \\\\\n",
    "    \\vdots \\\\\n",
    "    b_{m} \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{split}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem we face is to find $x \\in \\mathbb{R}^n$ that solves the expression above for a given $A$ and $b$. \n",
    "\n",
    "An important question is whether such an $x$ exists and whether it is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before dealing with these questions, why are SLEs (and being able to solve them) important? SLEs arise in many computational economics problems, either directly or indirectly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The prime example for a problem that directly involves a SLE is computing equilibrium prices and quantities in a model with multiples goods and linear demand and supply functions (see below and Q4 in this week's problem set);\n",
    "- Solving SLEs is part of numerous algorithms when analyzing more complicated, non-linear problems; we will get to applications later in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Existence and Uniqueness\n",
    "\n",
    "\n",
    "To see what properties of $A$ give us existence and uniqueness, first note the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}A x\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "    a_{11} &  \\cdots & a_{1n} \\\\\n",
    "    \\vdots & \\vdots  & \\vdots \\\\\n",
    "    a_{m1} &  \\cdots & a_{mn}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    x_{1}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    x_{n}\n",
    "\\end{array}\n",
    "\\right] \n",
    "&=\n",
    "x_1\\left[\n",
    "\\begin{array}{c}\n",
    "    a_{11} \\\\\n",
    "    \\vdots \\\\\n",
    "    a_{m1} \n",
    "\\end{array}\n",
    "\\right]  +\n",
    "...\n",
    "+ x_n\\left[\n",
    "\\begin{array}{c}\n",
    "    a_{1n}  \\\\\n",
    "    \\vdots \\\\\n",
    "    a_{mn} \n",
    "\\end{array}\n",
    "\\right] \n",
    "\\end{split}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In words, $Ax$ can be written as a linear combination of the set of the column vectors in $A$, $\\{ a_1, a_2, ..., a_n \\}$ where\n",
    "\n",
    "\\begin{equation}a_j =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    a_{1j} \\\\\n",
    "    a_{2j}  \\\\\n",
    "    \\vdots \\\\\n",
    "    a_{mj} \n",
    "\\end{array}\n",
    "\\right] \\in \\mathbb{R}^m \\end{equation}\n",
    "\n",
    "and where the coefficients are given by $x = (x_1, ... , x_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, for any $x \\in \\mathbb{R}^n$, $y = Ax$ is in the span of the column vectors of $A$. \n",
    "\n",
    "Put differently, if you define a function $f$ such that $f(x) = Ax$, the range of $f$ is the span of the columns of $A$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Square Matrices\n",
    "\n",
    "Let $A$ be an n-by-n matrix. The system of linear equations has exactly as many unknown variables as the number of equations:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}A x\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "    a_{11} &  \\cdots & a_{1n} \\\\\n",
    "    \\vdots & \\vdots  & \\vdots \\\\\n",
    "    a_{n1} &  \\cdots & a_{nn}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    x_{1}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    x_{n}\n",
    "\\end{array}\n",
    "\\right] \n",
    " = \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    b_{1}  \\\\\n",
    "    \\vdots \\\\\n",
    "    b_{n} \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{split}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the case that the column vectors in $A$ are linearly independent. Then, we know that:\n",
    "- since we need $n$ linearly independent vectors in $\\mathbb{R}^n$ to span $\\mathbb{R}^n$, the span of the columns of $A$ is $\\mathbb{R}^n$; hence, there must *exist* a vector $x$ such that $Ax = b$ for any $b \\in \\mathbb{R}^n$;\n",
    "- since no other vector $y$ will satisfy $Ay = b$ (see above), $x$ is *unique*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To summarize, *if the columns of a square matrix $A$ are linearly independent ($A$ has full column rank), the system of linear equations $Ax = b$ has a unique solution*. Hence, we can compute the determinant or the column rank of $A$ in order to check for existence and uniqueness of a solution for a square SLE. \n",
    "\n",
    "Moreover, knowing that the inverse of $A$ exists, we can compute the solution to the SLE as\n",
    "\\begin{equation}\n",
    "    x = A^{-1} b\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Illustration in Python\n",
    "\n",
    "Consider the system\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{array}{c}\n",
    "    x_1 + x_2 = 3\\\\\n",
    "    2x_1 +  x_2 = 4\n",
    "\\end{array}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2.]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 1],\n",
    "              [2, 1]])\n",
    "b = np.array([3, 4])\n",
    "\n",
    "## solve system Ax = b using the inverse\n",
    "print( np.linalg.inv(A) @ b )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also find the solution by rearranging the equations to $x_2 = 3 - x_1$ and $x_2 = 4 - 2x_1$, and plotting the resulting linear functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a187ef358>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1 =  lambda x: 3 - x\n",
    "f2 =  lambda x: 4 - 2 * x\n",
    "x = np.linspace(0, 2, 100)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, f1(x) , linewidth= 2)\n",
    "ax.plot(x, f2(x) , linewidth= 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### More Rows than Columns\n",
    "\n",
    "Next, consider the case where $A$ is not a square matrix, and the number of rows exceed the number of columns ($m > n$). In other words, there are more equations than unknown variables.\n",
    "\n",
    "This is the case that applies to OLS regression in econometric analysis: the number of observations $m$ is typically much higher than the number of regressors ($n$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this case, it is *very unlikely that a solution to $Ax = b$ exists*. To get some intuition, let $m = 3$ and $n = 2$, and let  \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}A\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "    1 &  0 \\\\\n",
    "    0 &  1  \\\\\n",
    "    0 &  0 \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{split}\n",
    "\\end{equation} \n",
    "\n",
    "Hence, the column vectors of $A$ are two of the canonical basis vectors; we therefore know that the columns are linearly independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, the span of $A$ contains only vectors $y \\in \\mathbb{R}^3$ such that $y_3 = 0$. In other words, the span of $A$ is a two-dimensional plane in the three-dimensional space $\\mathbb{R}^3$ (compare figure), and thus the vast majority of points in $\\mathbb{R}^3$ is not in the span of $A$. \n",
    "\n",
    "Therefore, for an arbitrary $b$, it is highly unlikely that we can find a sequence of coefficients (i.e. a vector) $x$ such that the linear combination $Ax$ gives us $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## figure in class\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you may recall from linear regression, when doing OLS we do not aim to find an exact solution, but instead compute the $x$ that minimizes the residuals. Using matrix notation and the vector norm, one way to express this is to minimize\n",
    "\\begin{equation}\n",
    "    ||\\ b - Ax\\ ||\n",
    "\\end{equation}\n",
    "\n",
    "which is the distance between $y$ and $Ax$. This results in the *best approximation* for $x$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{x} = (A'A)^{-1} A'b.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### More Columns than Rows\n",
    "\n",
    "Finally, consider the case with more columns than rows ($n > m$) or more unknowns than equations. In numerical analysis, this setting is not really relevant.   \n",
    "\n",
    "In this case, somewhat intuitively, if a solution exists, then there are infinitely many other solutions. In other words, uniqueness never holds.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As an example, consider $m = 2$ and $n = 3$, and let  \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}A\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "    1 &  0 &  2\\\\\n",
    "    0 &  1 &  3\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{split}\n",
    "\\end{equation} \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Although the first two vectors are linearly independent, the third one is not. In fact, it is not possible to find three vectors in $\\mathbb{R}^2$ that are linearly independent. \n",
    "\n",
    "In this example, we know that\n",
    "\\begin{equation}\n",
    "    a_3 = 2a_1 + 3a_2\n",
    "\\end{equation} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, if we have found an $x$ such that $Ax = b$ is satisfied, we know that \n",
    "\n",
    "\\begin{equation}\n",
    "    b = Ax = x_1a_1 + x_2 a_2 + x_3 a_3 = x_1a_1 + x_2 a_2 + x_3 (2a_1 + 3a_2) = (x_1 + 2x_3) a_1 + (x_2 + 3x_3) a_2.\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words, the vector \n",
    "\n",
    "\\begin{equation}\n",
    "   (x_1 + 2x_3, x_2 + 3x_3, 0) \n",
    "\\end{equation} \n",
    "\n",
    "also solves the system, and hence uniqueness is not satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id ='trile'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solving Triangular Linear Systems\n",
    "\n",
    "For the remainder of this class, we focus on SLEs with square matrices. In order to derive the common popular approach to solving general SLEs, we first look at how to solve *triangular* SLEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Forward substitution\n",
    "\n",
    "Suppose $A$ is an *lower triangular* square matrix; hence the SLE has the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}A x\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{ccccc}\n",
    "    a_{11} & 0 & 0 & \\cdots & 0 \\\\\n",
    "    a_{21} & a_{22} & 0 & \\cdots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} & a_{n3} &\\cdots & a_{nn}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    x_{1}  \\\\\n",
    "    \\vdots  \\\\\n",
    "    x_{n}\n",
    "\\end{array}\n",
    "\\right] \n",
    " = \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    b_{1}  \\\\\n",
    "    \\vdots \\\\\n",
    "    b_{n} \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{split}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is easy to see that $a_{11} x_1 = b_1$, and hence\n",
    "\n",
    "\\begin{equation}\n",
    "    x_1 = \\frac{b_1}{a_{11}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we have $a_{21} x_1 + a_{22} x_2 = b_2$, and hence, knowing $x_1$, we can compute $x_2$ *recursively* as \n",
    "\n",
    "\\begin{equation}\n",
    "    x_2 = \\frac{b_2 - a_{21}x_1}{a_{22}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can continue this logic all the way to $x_{n}$ (compare Miranda & Fackler, p. 8). A general formula for $x_i$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_i = \\frac{b_i - \\sum_{j = 1}^{i - 1} a_{ij}x_j}{a_{ii}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This way of computing the elements of $x$ in a lower triangular system of equations recursively is called **forward-substitution**. It is straightforward to implement in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def forward_sub(A, b):\n",
    "    \"\"\"\n",
    "    Implements the forward-substitution algorithm to solve a lower triangular system of equations\n",
    "    \"\"\"\n",
    "    n, m = A.shape\n",
    "    \n",
    "    assert n == m, \"A must be a square matrix\"\n",
    "    \n",
    "    x = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        \n",
    "        summ = 0\n",
    "        for j in range(i):\n",
    "            summ += A[i, j] * x[j]\n",
    "        \n",
    "        x[i] = (b[i] - summ) / A[i, i]   \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         -0.4        -0.08888889]\n"
     ]
    }
   ],
   "source": [
    "## Example\n",
    "A = np.array([[1, 0, 0],\n",
    "              [4, 5, 0],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "b = np.array([1, 2, 3]) \n",
    "\n",
    "## solve system\n",
    "x = forward_sub(A, b)\n",
    "print( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Backward substitution\n",
    "\n",
    "If $A$ is an *upper triangular matrix*, we can use *backward-substitution*, which works analogously to forward-substitution. I leave both the derivation and numerical implementation to this week's problem set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id ='lufac'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LU Factorization using Gaussian Elimination\n",
    "\n",
    "Let $L$ denote a *lower triangular* square matrix:\n",
    "\n",
    "\\begin{equation}L =\n",
    "\\left[\n",
    "\\begin{array}{ccccc}\n",
    "    a_{11} & 0 & 0 & \\cdots & 0 \\\\\n",
    "    a_{21} & a_{22} & 0 & \\cdots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} & a_{n3} &\\cdots & a_{nn}\n",
    "\\end{array}\n",
    "\\right]\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly, let $U$ denote an *upper triangular* square matrix:\n",
    "\n",
    "\\begin{equation}U =\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "    a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "    0 & a_{22} & \\cdots & a_{2n} \\\\\n",
    "    \\vdots & \\vdots &  & \\vdots \\\\\n",
    "    0 & 0 & \\cdots & a_{nn}\n",
    "\\end{array}\n",
    "\\right]\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gaussian elimination is an algorithm to factor any square matrix into the form \n",
    "\n",
    "\\begin{equation}\n",
    " A = \\tilde{L}U,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\tilde{L}$ is a *permuted* lower triangular matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A permuted lower triangular matrix has some of its rows interchanged. For example,\n",
    "\n",
    "\\begin{equation}L =\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "    a_{11} & 0 & 0 & 0 \\\\\n",
    "    a_{21} & a_{22} & 0 & 0 \\\\\n",
    "    a_{41} & a_{42} & a_{43} & a_{44} \\\\\n",
    "    a_{31} & a_{32} & a_{33} & 0 \n",
    "\\end{array}\n",
    "\\right]\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key idea of Gaussian elimination is that you can *subtract multiples of one row of a linear equation from another row without altering the solution of the linear equation*. Similarly, you can *interchange two rows of a linear solution*, again without altering the solution (compare the example in Miranda and Fackler, section 2.2).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Simple Example\n",
    "\n",
    "As an example, consider a market where the inverse supply function is given by \n",
    "\n",
    "\\begin{align}\n",
    "    p^s = c + d q,\n",
    "\\end{align}\n",
    "\n",
    "and the inverse demand function is given by\n",
    "\n",
    "\\begin{align}\n",
    "    p^d = a - b q.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In equilibrium, $p^d = p^d = p$, and solving for $q$ gives \n",
    "\n",
    "\\begin{align}\n",
    "    q = \\frac{a + c}{b - d}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that we can write the market in equilibrium as a system of two linear equations in two unknowns:\n",
    "\n",
    "\\begin{align}\n",
    "    p + b q &= a \\\\\n",
    "    p - d q &= c\n",
    "\\end{align}\n",
    "or\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "    1 & b \\\\\n",
    "    1 & - d \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    p \\\\\n",
    "    q \\\\\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    a \\\\\n",
    "    c \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let \n",
    "\n",
    "\\begin{equation}\n",
    "A \\equiv \n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "    1 & b \\\\\n",
    "    1 & - d\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "and recall that $ A = IA$, where $I$ is the identity matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "A = \n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{array}\n",
    "\\right] \\left[\n",
    "\\begin{array}{cc}\n",
    "    1 & b \\\\\n",
    "    1 & - d \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea of Gaussian elimination is to start with this expression and then go through each column of $A$, transforming it such that the elements below the diagonal are zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this simple example, there is only one step: we need to transform the first column of $A$. We can do this by subtracting $1$ times the first row from the second row, which gives us an upper triangular matrix $U$:\n",
    "\n",
    "\\begin{equation}\n",
    " U =\n",
    " \\left[\n",
    "\\begin{array}{cc}\n",
    "    1 & b \\\\\n",
    "    0 & - d - b \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "In order to keep the equality above, we update the identity matrix by the coefficient $1$:\n",
    "\n",
    "\\begin{equation}\n",
    "A = \\left[\n",
    "\\begin{array}{cc}\n",
    "    1 & 0 \\\\\n",
    "    1 & 1 \\\\\n",
    "\\end{array}\n",
    "\\right] \\left[\n",
    "\\begin{array}{cc}\n",
    "    1 & b \\\\\n",
    "    0 & - d - b \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\equiv LU\n",
    "\\end{equation}\n",
    "\n",
    "It is easy to verify that the equality still holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why is it useful to write $A$ as $LU$, and hence why do we need Gaussian elimination? Going back to our system of linear equations, let's make the following substitutions:\n",
    "\n",
    "\\begin{equation}\n",
    "Ax = LUx = L(Ux) = Ly = b\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we will see, this is easy to solve. Start with $Ly = b$. In our example above:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "    1 & 0 \\\\\n",
    "    1 & 1 \\\\\n",
    "\\end{array}\n",
    "\\right] \\left[\n",
    "\\begin{array}{c}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "\\end{array}\n",
    "\\right] = \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    a \\\\\n",
    "    c \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "From this, it is easy to see that $y_1 = a$ and\n",
    "\n",
    "\\begin{equation}\n",
    "    y_1 + y_2 = c\\ \\Rightarrow\\ y_2 = c - a.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, we have a solution for $y$ above. Recall that we had defined $Ux = y$ or\n",
    "\n",
    "\\begin{equation}\n",
    " \\begin{split}\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "    1 & b \\\\\n",
    "    0 & - d - b \\\\\n",
    "\\end{array}\n",
    "\\right] \\left[\n",
    "\\begin{array}{c}\n",
    "    p \\\\\n",
    "    q \\\\\n",
    "\\end{array}\n",
    "\\right] = \n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "    a \\\\\n",
    "    c - a \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From this, it follows that\n",
    "\n",
    "\\begin{equation}\n",
    "q  (- d - b) = c - a\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "p + bq = a.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rearranging gives the solution\n",
    "\n",
    "\\begin{equation}\n",
    "q  = \\frac{a - c}{b + d}, \\ p = a - b \\frac{a - c}{b + d},\n",
    "\\end{equation}\n",
    "\n",
    "which equals the solution found above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gaussian Elimination with Pivoting\n",
    "\n",
    "Gaussian elimination as described above can lead to inaccurate results on a computer due to roundoff errors, which we discussed in the previous lecture. Roundoff errors can be a problem if the elements on the diagonal of matrix $A$ are very small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A remedy to this problem is *pivoting*, that is, interchanging rows during Gaussian elimination, in order to make the diagonal elements as large as possible (in absolute values).\n",
    "\n",
    "The example in Miranda and Fackler, section 2.3, and in question 5 of this week's problem set illustrates the problem and gives an intuition for the why pivoting works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "Before moving to solving SLEs in Python, let's summarize the key insights of this section:\n",
    "\n",
    "1. A SLE with a lower or upper triangular matrix can be easily solved using forward or backward substitution, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. An arbitrary matrix $A$ can be factorized into $A = \\tilde{L} U$, where $U$ is an upper triangular matrix and $\\tilde{L}$ is a permuted lower triangular matrix, using Gaussian elimination. We can then use a combination of forward and backward substitution to solve the system.\n",
    "\n",
    "3. There are different ways to implement Gaussian elimination. In practice, Python uses a variant called $PA = LU$ factorization (see appendix), which includes partial pivoting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solving a system of linear equations in Python\n",
    "\n",
    "All the steps above - Gaussian elimination, forward- and backward substitution - are implemented in one Python function, NumPy's **linalg.solve** function. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.14285714],\n",
       "       [7.14285714]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b, c, d = 5, 0.4, 0, 0.3\n",
    "\n",
    "A = np.array([[1, b], [1, -d]])\n",
    "y = np.array([[a], [c]])\n",
    "\n",
    "np.linalg.solve(A, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.33333333e-01,  6.66666667e-01,  3.17206578e-17])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2, 4],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "b = np.array([1, 2, 3]) \n",
    "\n",
    "np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate that LU factorization is a faster way to solve a SLE for large matrices than using the inverse, we can use Numpy's **random.uniform** function to create a large ($n = 5000$) square matrix $A$ and vector $b$, both with random elements (here between 0 and 5). \n",
    "\n",
    "A quick way to measure the running time of a piece of code is Jupyter notebook's **%%timeit** \"magic\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "A = np.random.uniform(0,5,(5000,5000))\n",
    "b = np.random.uniform(0,5,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.06 s  0 ns per loop (mean  std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.29 s  0 ns per loop (mean  std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n1 np.linalg.inv(A) @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### SciPy\n",
    "\n",
    "SciPy is built on top of NumPy (using its array data type and related functionality) and provides additional tools for scientific programming, for example for linear algebra, optimization and statistics. \n",
    "\n",
    "Its documentation can be found here: http://docs.scipy.org/doc/scipy/reference/index.html. We will use different SciPy packages extensively throughout this course. It is good practice to import the specific subpackages to be used, rather than the whole package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some of the functions used above, in particular **linalg.solve** and **linalg.inv** are also available in SciPy. For this course, it does not really matter which variant we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.58 s  0 ns per loop (mean  std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import scipy.linalg\n",
    "\n",
    "%timeit -r1 -n1 scipy.linalg.inv(A) @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id ='ill'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ill-conditioned Matrices\n",
    "\n",
    "A matrix $A$ is said to be *ill-conditioned* if in the SLE $Ax =b$, a small perturbation in $b$ (or in $A$) induces a large change in $x$. \n",
    "\n",
    "Recall that most real numbers, when represented as floating point numbers, are approximated by DP numbers. Hence, for an ill-conditioned matrix, small roundoff errors in $b$ can lead to large errors in the computed solution vector $x$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A notorious example of an ill-conditioned matrix is the so-called *Vandermonde* matrix:\n",
    "\n",
    "\\begin{equation}A =\n",
    "\\left[\n",
    "\\begin{array}{ccccc}\n",
    "    a_{1}^0 & a_{1}^1 & a_{1}^2 &\\cdots & a_{1}^{n-1} \\\\\n",
    "    a_{2}^0 & a_{2}^1 & a_{2}^2 &\\cdots & a_{2}^{n-1} \\\\\n",
    "    \\vdots & \\vdots &  & \\vdots \\\\\n",
    "    a_{n}^0 & a_{n}^1 & a_{n}^2 &\\cdots & a_{n}^{n-1}\n",
    "\\end{array}\n",
    "\\right]\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For $a_j = j$ and $n = 5$, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   1   1   1   1]\n",
      " [  1   2   4   8  16]\n",
      " [  1   3   9  27  81]\n",
      " [  1   4  16  64 256]\n",
      " [  1   5  25 125 625]]\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "A = np.array([i**j for i in range(1,n+1) for j in range(0, n)] )\n",
    "A.shape = (n, n)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate the pitfalls of an ill-conditioned matrix, consider solving $Ax =b$ where $A$ is a Vandermonde matrix and $b$ is set such that the solution vector $x$ consists of only of ones. \n",
    "\n",
    "E.g. if $n = 5$, under exact arithmetic, $x = [1, 1, 1, 1, 1]'$ and so on. Below, we solve the system for different values of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def vm(n):\n",
    "    ## define matrix\n",
    "    A = np.array([i**j for i in range(1,n+1) for j in range(0, n)] )\n",
    "    A.shape = (n, n)\n",
    "    ## determine the solution vector\n",
    "    b = A @ np.ones(n)\n",
    "    ## solve SLE\n",
    "    x = np.linalg.solve(A, b)\n",
    "    \n",
    "    return x, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = 5, x = [1. 1. 1. 1. 1.]\n",
      "For n = 5, the condition number is 26169.68797063433\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "x, A = vm(n)\n",
    "print(\"For n = {}, x = {}\".format(n, x) )\n",
    "print(\"For n = {}, the condition number is {}\".format(n, np.linalg.cond(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = 10, x = [1.00000064 0.99999871 1.00000078 0.99999993 0.99999989 1.00000005\n",
      " 0.99999999 1.         1.         1.        ]\n",
      "For n = 10, the condition number is 2106245945721.575\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "x, A = vm(n)\n",
    "print(\"For n = {}, x = {}\".format(n, x) )\n",
    "print(\"For n = {}, the condition number is {}\".format(n, np.linalg.cond(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n = 15, x = [ 1.23061043e+04 -3.93051732e+04  5.31333157e+04 -4.09524104e+04\n",
      "  2.03382320e+04 -6.93956632e+03  1.69049953e+03 -2.98555551e+02\n",
      "  4.00433004e+01 -2.73635796e+00  1.25925409e+00  9.87316999e-01\n",
      "  1.00041455e+00  9.99991879e-01  1.00000007e+00]\n",
      "For n = 15, the condition number is 9.095497295485408e+20\n"
     ]
    }
   ],
   "source": [
    "n = 15\n",
    "x, A = vm(n)\n",
    "print(\"For n = {}, x = {}\".format(n, x) )\n",
    "print(\"For n = {}, the condition number is {}\".format(n, np.linalg.cond(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we can see, for larger $n$, the solution vector computed numerically is very different from the exact solution, and hence the approximation error is large.\n",
    "\n",
    "Above, we also compute the *condition number* $\\kappa \\ge 1$ (using Numpy's **linalg.cond** function) which is an indicator for an ill-conditioned matrix: the greater $\\kappa$, the more severe is ill-conditioning and hence the lower is the accuracy of the computed solution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Without going into further details, the intuition when a matrix in an SLE is ill-conditioned is that it is close to a singular matrix. \n",
    "\n",
    "That said, it is important to note that the size of the determinant of a matrix - which indicates whether a matrix is singular in case it is zero - is *not* a good indicator whether a matrix is ill-conditioned or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id ='sparse'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Matrices\n",
    "\n",
    "A matrix is called *sparse* if it contains mainly zeros. Since it would be a waste of memory and processing power to store and process a lot of zeros when doing matrix operations, most programming languages, including Python/Scipy, have special formats for *sparse matrices*.\n",
    "\n",
    "Essentially, when using sparse matrices, the computer keeps track only of the non-zero elements and their location in the original matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following example is taken from the SciPy documentation on sparse matrices: https://docs.scipy.org/doc/scipy/reference/sparse.html#example-1. It illustrates how to define a sparse matrices in SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "from scipy.sparse.linalg import spsolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Constructing a sparse matrix is a two-step process, due to different formats for sparse matrices. First, we define a *list-of-list* matrix with SciPy's **lil_matrix** function.\n",
    "\n",
    "The code below then fills its diagonal and some other elements with random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "A = lil_matrix((1000, 1000))\n",
    "\n",
    "A.setdiag(np.random.rand(1000))\n",
    "A[0, :100] = np.random.rand(100)\n",
    "A[1, 100:200] = A[0, :100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, convert the matrix to a *compressed sparse row* (CSR) matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "A = A.tocsr()\n",
    "print( type(A) )\n",
    "\n",
    "b = np.random.rand(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Solving an SLE where $A$ is defined as a sparse matrix is considerably faster than when $A$ is a full matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.45 ms  0 ns per loop (mean  std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "## solving the system with A as a sparse matrix\n",
    "%timeit -r1 -n1 spsolve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.1 ms  0 ns per loop (mean  std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "## solving the system with A as a standard matrix\n",
    "A_ = A.toarray()\n",
    "%timeit -r1 -n1  np.linalg.solve(A_, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check if both give the same result\n",
    "x = spsolve(A, b)\n",
    "x_ = np.linalg.solve(A_, b)\n",
    "np.allclose(x, x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, as suggested above, the matrix takes much less storage space when it is stored as a sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000112"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(A_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Despite these advantages of using sparse matrices, there are also downsides (cp. M&F, appendix 2B). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id ='iterative'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iterative Methods\n",
    "\n",
    "Many important algorithms in numerical analysis, some of which we will learn about during this course, are based on  *iteration*. The basic idea of iterative methods is to generate a sequence of approximations to the object of interest, in case of SLEs the solution vector $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ideally, these approximations become more and more precise with an increasing number of iterations. However, it is important to emphasize that iterative methods, in contrast to direct methods like solving SLEs with LU factorization, do not yield an exact solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On a very general level, iterative methods can be formalized by the following iteration rule:\n",
    "\n",
    "\\begin{equation}\n",
    "    x^{(k+1)} = g( x^{(k)} ).\n",
    "\\end{equation}\n",
    "\n",
    "$k$ here is an indicator counting the number of iterations. Hence, in words, the value for $x$ in the $k+1$-iteration is obtained by applying function $g$ on the value for $x$ in the $k$-iteration. Naturally, the functional form of $g$ depends on the problem you want analyze. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For more concreteness, let us go back to the solving the SLE. Starting at $Ax = b$, first note that rearranging and adding $Qx$ on both sides, where $Q$ is also a square matrix of order $n$ gives:\n",
    "\n",
    "\\begin{equation}\n",
    "    Qx = Qx + b - Ax\\ \\Rightarrow \\ Qx = b + (Q - A)x\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Multiplying both sides with the inverse of $Q$, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    x = Q^{-1} b + (I - Q^{-1} A)x\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This will be our iteration rule. In other words, the function $g$ above is given by the RHS, $Q^{-1} b + (I - Q^{-1} A)x$, and hence\n",
    "\n",
    "\\begin{equation}\n",
    "    x^{(k+1)} = g( x^{(k)} ) = Q^{-1} b + (I - Q^{-1} A)x^{(k)}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that taking the inverse of a matrix can be computationally demanding, in particular for large matrices. Hence, in order for this iteration rule to be useful, we need $Q$ to be easily invertible. \n",
    "\n",
    "One way to achieve this is to require $Q$ to be diagonal or triangular. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is the idea of the **Gauss-Jacobi** and **Gauss-Seidel** methods, respectively:\n",
    "- the Gauss-Jacobi method set $Q$ equal to to a diagonal matrix with the diagonal elements of $A$; \n",
    "- the Gauss-Seidel method forms $Q$ from upper triangular elements of $A$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Python, the **diag** and **triu** functions are useful when implementing these algorithms (the former has to be applied twice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 5 0]\n",
      " [0 0 9]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2, 4],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "Q = np.diag( np.diag(A) )\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 4]\n",
      " [0 5 6]\n",
      " [0 0 9]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2, 4],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "Q = np.triu(A)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following piece of code implements the Gauss-Seidel method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def gauss_seidel(A, b, x0):\n",
    "    \"\"\" \n",
    "    Implements the Gauss-Seidel method with a over-relaxation parameter\n",
    "    \"\"\"\n",
    "    ## tolerance level for stopping rule\n",
    "    tol = 1e-8\n",
    "    eps = 1\n",
    "    ## iteration counter and max number of iterations\n",
    "    it = 0\n",
    "    maxit = 100\n",
    "    \n",
    "    ## initialize x\n",
    "    x = x0\n",
    "    Q = np.triu(A)\n",
    "    Q_inv = np.linalg.inv(Q)\n",
    "    \n",
    "    while eps > tol and it < maxit:\n",
    "        it += 1 \n",
    "        x_new = Q_inv @ b + ( np.eye(len(b)) - Q_inv @ A) @ x\n",
    "    \n",
    "        eps = np.linalg.norm(x_new - x)\n",
    "        \n",
    "        x = x_new\n",
    "        \n",
    "    return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A common feature of iteration-based algorithms is a *stopping rule*, that tells the algorithm when to stop iterating. Often, we check the difference between the current and the next iteration of $x$, and then stop if this difference is smaller than a *tolerance level*, e.g. half machine epsilon:\n",
    "\n",
    "\\begin{equation}\n",
    "    |x^{(k+1)}- x^{(k)}| < tol\n",
    "\\end{equation}    \n",
    "\n",
    "When implemeting an iteration method with a stopping rule, we usually use a **while** loop, where the conditional expression captures the rule; that is, the loop runs as long as the rule is not satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note also that when using iterative methods, we also have to provide the algorithm with an \"initial guess\", that is, a starting value for the vector of interest, $x^{(0)}$.\n",
    "\n",
    "We then apply this function on the example used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.33439602e+44 -9.47028130e+42  3.80950410e+43]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2, 4],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "b = np.array([1, 2, 3]) \n",
    "\n",
    "## initial guess\n",
    "x0 = np.array([-0.33, 0.66, 0]) \n",
    "\n",
    "print( gauss_seidel(A, b, x0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Applying the Gauss-Seidel method on our example from above, we see that the algorithm does not converge, even if we choose an initial $x$ close to the exact solution (which we know in this case). This is a drawback of using iterative methods: convergence to a good approximation is not guarantueed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the case of solving an SLE, it can be shown that we get convergence if the following condition holds:\n",
    "\n",
    "\\begin{equation}\n",
    "    || I - Q^{-1} A || < 1.\n",
    "\\end{equation}\n",
    "\n",
    "For now, we will not look at why this is the case, but just take this property as a given. We can evaluate this expression for our example and find that the condition is not satisfied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5586791834702094\n"
     ]
    }
   ],
   "source": [
    "print( np.linalg.norm(np.eye(len(b)) - np.linalg.inv(np.triu(A)) @ A) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To consider an example where convergence is guaranteed, *independent from the starting value for* $x$, modify $A$ by increasing the elements on the diagonal (cp. Miranda and Fackler for why this works):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6065175640948542"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[10, 2, 4],\n",
    "              [4, 15, 6],\n",
    "              [7, 8, 20]])\n",
    "\n",
    "np.linalg.norm(np.eye(len(b)) - np.linalg.inv(np.triu(A)) @ A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The condition above is satisfied; applying the Gauss-Seidel algorithm finds a solution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04275093 0.08085502 0.10269517]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([1, 1, 1]) \n",
    "b = np.array([1, 2, 3]) \n",
    "print( gauss_seidel(A, b, x0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can verify that this solution is found for different values for the starting value **x0**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When should you use iterative methods rather than computing an exact solution to a SLE? A rule of thumb is for large matrices, in particular if they are sparse, iterative methods may be a faster way to find $x$. For this course, all problems that involve solving a SLE will be of moderate size, hence LU factorization will do fine."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
