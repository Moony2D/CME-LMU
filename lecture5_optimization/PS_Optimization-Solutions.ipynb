{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Methods in Economics\n",
    "\n",
    "## Problem Set 5 - Numerical Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last update: 2018-12-14 14:43:09.821885\n"
     ]
    }
   ],
   "source": [
    "# Author: Alex Schmitt (schmitt@ifo.de)\n",
    "\n",
    "import datetime\n",
    "print('Last update: ' + str(datetime.datetime.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import scipy.linalg\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (N)\n",
    "\n",
    "*From Judd(1998), chapter 4, question 2*. One of the classical uses of optimization is the computation of the *Pareto frontier*. Consider the endowment economy with $m$ goods and $n$ agents. Assume that agent $i$'s utility function over the $m$ goods is \n",
    "\n",
    "\\begin{equation}\n",
    "    u^{i}(x^i) = \\sum^{m}_{j = 1} a^i_j (x^i_j)^{v^i_j + 1} (1 + v^i_j)^{-1} \n",
    "\\end{equation}\n",
    "\n",
    "Suppose that agent $i$'s endowment of good $j$ is $e^i_j$. Assume that $a^i_j, e^i_j > 0 > v^i_j$ (for $v^i_j =-1$, we replace $(x^i_j)^{v^i_j + 1} (1 + v^i_j)^{-1}$ with $\\ln x^i_j$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Write a program using Scipy's BFGS implementation that will read in the $v^i_j$, $a^i_j$ and $e^i_j$ and the social weights $\\lambda^i$, and output the solution to the social planner's problem. Choose $m = n = 2$ and solve the problem *analytically* for $\\lambda_1 = \\lambda_2 = 0.5$ and the following values for the remaining parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V = [[-4. -2.]\n",
      " [-3. -3.]]\n",
      "A = [[1. 4.]\n",
      " [1. 8.]]\n",
      "E = [[6. 4.]\n",
      " [5. 1.]]\n"
     ]
    }
   ],
   "source": [
    "E = np.array([[6., 4.], [5., 1.]])\n",
    "V = np.array([[-4., -2.], [-3., -3.]])\n",
    "A = np.array([[1., 4.], [1., 8.]])\n",
    "\n",
    "print(\"V = {}\".format(V) )\n",
    "print(\"A = {}\".format(A) )\n",
    "print(\"E = {}\".format(E) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to read these matrices is that a good corresponds to a row and an agent to a column. For example, agent 1's endowment of good 2, $e^1_2$, would be the element in the second row and first column of matrix **E**, and hence $e^1_2 = 5$.\n",
    "\n",
    "With these parameter values, confirm your analytical result equals the numerical output of your program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Test your program for higher numbers of goods and agents. You can create the parameter matrices above using Numpy's **np.random.uniform** function. Can your program handle $m = n = 5$? $m = n = 10$?      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: A slightly tricky issue when answering this question using *unconstrained* numerical optimization methods is how to deal with the constraint that aggregate consumption of good $j$ must equal aggregate endowments, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum^{n}_{i = 1} x_j^i = \\sum^{n}_{i = 1} e_j^i\n",
    "\\end{equation}\n",
    "\n",
    "One way to address this is to have the algorithm solve for the optimal consumption for $n - 1$ agents and evaluate the consumption and hence the utility of the last agent *as the residual*. formally, for good $j$,\n",
    "\n",
    "\\begin{equation}\n",
    "    x_j^n = \\sum^{n}_{i = 1} e_j^i - \\sum^{n-1}_{i = 1} x_j^i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "For the analytical solution to the 2-by-2 case, compare the notes in class. \n",
    "\n",
    "For the numerical implementation, start by addressing the issue of the endowment constraints discussed above. The function **consum(x, E)** takes a vector **x** (the consumption levels for the first $n-1$ agents) and the endowment matrix **E** and returns a consumption matrix **X**, in which the last column (the consumption of agent $n$) is computed as the residual.\n",
    "\n",
    "As a side note, why do we define **consum** in a way that it takes a vector (i.e. a one-dimensional array) rather than a two-dimensional matrix as an input? Note that **scipy.optimize.minimize** returns a vector (even if we use a matrix as an initial, as seen below!) and hence we need to write **consum** in a way that it accommodates this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consum(x, E):\n",
    "    \"\"\"\n",
    "    Computes an m-by-n consumption matrix from solution vector x and endowments E;\n",
    "    Agent n's consumption is computed as the residual comsumption \n",
    "    \"\"\"\n",
    "    ## get dimension of the endowment matrix\n",
    "    m, n = E.shape \n",
    "    ## check that x and E have consistent dimensions\n",
    "    assert len(x) == m * (n-1), \"The length is not consistent with the dimensions of E!\"\n",
    "    \n",
    "    ## reshape x into matrix X \n",
    "    X = x.copy()\n",
    "    X.shape = m, n - 1\n",
    "    \n",
    "    ## compute residual consumption\n",
    "    x_res = E.sum(axis = 1) - X.sum(axis = 1)\n",
    "    \n",
    "    ## combine X with x_res and return\n",
    "    return np.column_stack( (X, x_res) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **consum**, it is straightforward to implement the objective function $U = \\sum^n_{i = 1} \\lambda^i u^{i}(x^i)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x, V, A, E, lam):\n",
    "    \"\"\"\n",
    "    Computes the objective function for the m-by-n social planner problem\n",
    "    \"\"\"\n",
    "    ## get dimension of the problem \n",
    "    m, n = V.shape ## number of agents, number of good\n",
    "\n",
    "    ## check if the parameter matrices have the correct dimensions \n",
    "    assert A.shape == (m, n), \"The dimensions of A and V must coincide!\"\n",
    "    assert E.shape == (m, n), \"The dimensions of E and V must coincide!\"\n",
    "    assert len(lam) == n, \"The length of lam is not consistent with the dimensions of V!\"\n",
    "    \n",
    "    ## compute matrices of consumption using x and endowment matrix E\n",
    "    X = consum(x, E)\n",
    "    \n",
    "    ## loop over agents and evaluate their utility\n",
    "    u = np.zeros(n)\n",
    "    for ind in range(n):\n",
    "        v, a = V[:, ind], A[:, ind]\n",
    "        Q = a / (1 + v)\n",
    "        u[ind] = Q @ X[:,ind]**(1 + v)\n",
    "\n",
    "    ## compute welfare given weights lam (NB: we could use summation instead of vector multiplication )\n",
    "    return - lam @ u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve the problem using the BFGS algorithm for 2-by-2 case. As an initial guess, use the agents' endowment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.458333\n",
      "         Iterations: 16\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 18\n",
      "Consumption matrix: \n",
      "[[1.99999254 8.00000746]\n",
      " [2.00000424 3.99999576]]\n"
     ]
    }
   ],
   "source": [
    "x0 =  E[:, :-1]\n",
    "lam = np.array([0.5, 0.5])\n",
    "objective(x0, V, A, E, lam)\n",
    "res = scipy.optimize.minimize(objective, x0, method = 'BFGS', args = (V, A, E, lam), options = {'disp': True})\n",
    "x = res.x\n",
    "\n",
    "print(\"Consumption matrix: \")\n",
    "print( consum(x, E) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### m = 3, n = 2\n",
    "\n",
    "We can check that the algorithm works in a setting with more goods than agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.329515\n",
      "         Iterations: 8\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 9\n",
      "Consumption matrix: \n",
      "[[1.69723441 1.30276559]\n",
      " [1.47265462 2.52734538]\n",
      " [1.34772323 3.65227677]]\n"
     ]
    }
   ],
   "source": [
    "E = np.array([[2., 1.], [2., 2.], [2., 3.]])\n",
    "V = np.array([[-2., -4.], [-3., -2.], [-5., -2.]])\n",
    "A = np.array([[1., 1.], [1., 2.], [1., 3.]])\n",
    "\n",
    "x0 =  E[:, :-1]\n",
    "lam = np.array([0.5, 0.5])\n",
    "res = scipy.optimize.minimize(objective, x0, method = 'BFGS', args = (V, A, E, lam), options = {'disp': True})\n",
    "x = res.x\n",
    "\n",
    "print(\"Consumption matrix: \")\n",
    "print( consum(x, E) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### m = 6, n = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 6, 6\n",
    "E = np.random.uniform(1, 5, (m, n))\n",
    "V = np.random.uniform(-5, -1.001, (m, n))\n",
    "A = np.random.uniform(1, 3, (m, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 6.663442\n",
      "         Iterations: 91\n",
      "         Function evaluations: 2976\n",
      "         Gradient evaluations: 93\n",
      "Consumption matrix: \n",
      "[[4.47993552 2.25127022 3.30683165 3.27816417 2.58160917 5.27907715]\n",
      " [9.62593192 1.56279095 1.66457392 2.08378085 2.746518   1.72673169]\n",
      " [5.12384792 2.93458866 1.96895529 3.9901752  2.3849288  2.13699173]\n",
      " [2.04864918 8.30810872 1.88714786 2.00644759 2.9753713  2.4970475 ]\n",
      " [2.22313084 5.81201221 3.83504054 2.32292384 6.36966501 4.01918305]\n",
      " [2.04415783 2.279997   3.26053463 1.99764592 4.26553828 6.22829564]]\n"
     ]
    }
   ],
   "source": [
    "x0 = E[:, :-1]\n",
    "lam = np.ones(n) * (1/n)\n",
    "res = scipy.optimize.minimize(objective, x0, method = 'BFGS', args = (V, A, E, lam), options = {'disp': True})\n",
    "x = res.x\n",
    "\n",
    "print(\"Consumption matrix: \")\n",
    "print( consum(x, E) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (N)\n",
    "\n",
    "Consider the neoclassical growth model from the lecture. In this question, we extend it so that the production function contains *energy* $m_t$ as a third production factor in addition to capital and labor. Hence, output is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    y_t = f(k_t, h_{y,t}, m_t) = A k_t^\\alpha m_t^\\gamma h_{y,t}^{1-\\alpha-\\gamma}\n",
    "\\end{equation}\n",
    "\n",
    "Energy is itself produced by using a part of the labor supply:\n",
    "\n",
    "\\begin{equation}\n",
    "    m_t = \\rho h_{m,t}\n",
    "\\end{equation}\n",
    "\n",
    "which implies that one unit of labor supply creates $\\rho$ units of energy.\n",
    "\n",
    "Solve the planner problem numerically for $T = 30$. Note that lifetime utility is still given by \n",
    "\n",
    "\\begin{equation}\n",
    "    u(c_t, h_t) = \\frac{c^{1-\\nu}}{1-\\nu} - B \\frac{h_t^{1+\\eta}}{1+\\eta}\n",
    "\\end{equation}\n",
    "\n",
    "with $h_t = h_{y,t} + h_{m,t}$. You can use the parameter values from the lecture, and $\\gamma = 0.05$ and $\\rho = 0.9$. \n",
    "\n",
    "In addition, compute the steady state using a root finding algorithm and verify that the planner's sequences for $k_t$, $h_{y,t}$ and $h_{m,t}$ converge to their steady state values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utility\n",
    "beta = 0.9      # discount factor\n",
    "nu = 2       # risk-aversion coefficient for consumption\n",
    "eta = 1         # elasticity parameter for labor supply\n",
    "\n",
    "## production\n",
    "alpha = 0.25\n",
    "gamma = 0.05\n",
    "rho = 0.9\n",
    "delta = 0.1\n",
    "## derived\n",
    "A = (1 - beta * (1 - delta))/(alpha*beta)  # productivity\n",
    "B = (1 - alpha) * A * (A - delta)**nu      # parameter for utility function\n",
    "## initial capital stock\n",
    "k0 = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd(k, m, hy):\n",
    "    \"\"\"\n",
    "    Evaluates the Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return A * k**alpha * m**gamma * hy**(1 - alpha - gamma)\n",
    "\n",
    "def u(c, h):\n",
    "    \"\"\"\n",
    "    Utility function\n",
    "    \"\"\"\n",
    "    return c**(1 - nu)/(1 - nu) - B * h**(1 + eta)/(1 + eta)\n",
    "\n",
    "def objective(x):\n",
    "    \"\"\"\n",
    "    Objective function: lifetime utility\n",
    "    \"\"\"\n",
    "    kp = np.exp( x[:int(len(x)/3)] )\n",
    "    hy = np.exp( x[int(len(x)/3): 2 * int(len(x)/3)] )\n",
    "    hm = np.exp( x[2 * int(len(x)/3):] )\n",
    "\n",
    "    k = np.insert(kp[:T-1], 0, k0)\n",
    "    \n",
    "    return - ( beta**(np.array(range(T))) @ u( cd(k, rho * hm, hy) + (1 - delta) * k - kp, hy + hm)\n",
    "              + (beta**T/(1 - beta)) *  u( cd(kp[-1], rho * hm[-1], hy[-1]) - delta * kp[-1] , hy[-1] + hm[-1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving the steady state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cobb_douglas(x, alpha, gamma, A):\n",
    "    \"\"\"\n",
    "    Evaluates the Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return A * x[0]**alpha * (rho * x[2])**(gamma) * x[1]**(1 - alpha - gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd_diff(x, alpha, gamma, A):\n",
    "    \"\"\"\n",
    "    Returns the first derivative of the cobb douglas wrt k, hy and hm\n",
    "    \"\"\"\n",
    "    return (alpha * cobb_douglas(x, alpha, gamma, A) / x[0], \n",
    "            (1 - alpha - gamma) * cobb_douglas(x, alpha, gamma, A) / x[1],\n",
    "            gamma * cobb_douglas(x, alpha, gamma, A) / x[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steady(x):\n",
    "    \"\"\"\n",
    "    Returns the vector-valued function consisting of the steady-state conditions \n",
    "    \"\"\"\n",
    "    y = np.zeros(3)\n",
    "    X = np.exp(x)\n",
    "    mp = cd_diff(X, alpha, gamma, A)\n",
    "    \n",
    "    y[0] = beta * (mp[0] + 1 - delta) - 1\n",
    "    y[1] = (cobb_douglas(X, alpha, gamma, A) - delta * X[0])**(-nu) * mp[1] - B * (X[2] + X[1])**eta\n",
    "    y[2] = mp[1] - mp[2]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: array([ 0.00000000e+00, -1.26660680e-06, -2.77555756e-16])\n",
      " message: 'A solution was found at the specified tolerance.'\n",
      "     nit: 94\n",
      "  status: 1\n",
      " success: True\n",
      "       x: array([ 0.22552077,  0.40848196, -2.23057537])\n",
      "Steady state: (K_ss, hy_ss, hm_ss) = [1.25297506 1.50453212 0.10746658]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.log([1.2, 1.4, 0.2])\n",
    "res_ss = scipy.optimize.root(steady, x0, method = 'broyden1', options = {'line_search' : None, 'jac_options': {'alpha': 1}})\n",
    "print(res_ss)\n",
    "\n",
    "kss = np.exp(res_ss.x)[0]\n",
    "hyss = np.exp(res_ss.x)[1]\n",
    "hmss = np.exp(res_ss.x)[2]\n",
    "\n",
    "print( \"Steady state: (K_ss, hy_ss, hm_ss) = {}\".format(np.exp(res_ss.x) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving the model numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "T = 30\n",
    "\n",
    "## set initial guess    \n",
    "x0 = np.concatenate([np.log( 1.2 ) * np.ones(T), np.log( 1.5 ) * np.ones(T), np.log( 0.09 ) * np.ones(T)])\n",
    "\n",
    "## solve model\n",
    "res = scipy.optimize.minimize(objective, x0, method = 'BFGS', tol = 1e-6)\n",
    "print(res.message)\n",
    "print(res.success)\n",
    "x = res.x\n",
    "kp = np.exp( x[:int(len(x)/3)] )\n",
    "hy = np.exp( x[int(len(x)/3): 2 * int(len(x)/3)] )\n",
    "hm = np.exp( x[2 * int(len(x)/3):] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 15.95642654445973\n",
       " hess_inv: array([[ 4.47769626e-01,  2.56326478e-01,  1.58080848e-01, ...,\n",
       "        -5.02637971e-02, -1.50139151e-01, -3.87164324e-02],\n",
       "       [ 2.56326478e-01,  5.66210374e-01,  3.39401024e-01, ...,\n",
       "        -9.34447024e-02, -2.20502904e-01, -8.10468754e-02],\n",
       "       [ 1.58080848e-01,  3.39401024e-01,  6.35835087e-01, ...,\n",
       "        -7.56924113e-02, -1.81125458e-01, -9.91180563e-02],\n",
       "       ...,\n",
       "       [-5.02637971e-02, -9.34447024e-02, -7.56924113e-02, ...,\n",
       "         4.02693032e+01,  4.31456538e+01, -8.82324233e-01],\n",
       "       [-1.50139151e-01, -2.20502904e-01, -1.81125458e-01, ...,\n",
       "         4.31456538e+01,  1.16751638e+02, -4.46488225e+00],\n",
       "       [-3.87164324e-02, -8.10468754e-02, -9.91180563e-02, ...,\n",
       "        -8.82324233e-01, -4.46488225e+00,  8.85287977e+00]])\n",
       "      jac: array([ 2.38418579e-07,  4.76837158e-07, -4.76837158e-07,  2.38418579e-07,\n",
       "        0.00000000e+00,  2.38418579e-07,  1.19209290e-07,  0.00000000e+00,\n",
       "        0.00000000e+00,  2.38418579e-07,  2.38418579e-07,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.19209290e-07,  2.38418579e-07,  2.38418579e-07,\n",
       "        0.00000000e+00, -1.19209290e-07, -1.19209290e-07, -3.57627869e-07,\n",
       "        0.00000000e+00,  2.38418579e-07,  3.57627869e-07,  1.19209290e-07,\n",
       "        2.38418579e-07,  0.00000000e+00, -3.57627869e-07,  0.00000000e+00,\n",
       "        2.38418579e-07,  4.76837158e-07,  0.00000000e+00,  2.38418579e-07,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -2.38418579e-07,  0.00000000e+00,  2.38418579e-07,  0.00000000e+00,\n",
       "        0.00000000e+00, -2.38418579e-07,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  2.38418579e-07, -4.76837158e-07, -2.38418579e-07,\n",
       "        0.00000000e+00, -9.53674316e-07,  0.00000000e+00, -7.15255737e-07,\n",
       "        5.96046448e-07, -4.76837158e-07,  4.76837158e-07,  2.38418579e-07,\n",
       "       -2.38418579e-07,  2.38418579e-07, -2.38418579e-07,  2.38418579e-07,\n",
       "        0.00000000e+00,  2.38418579e-07, -2.38418579e-07,  3.57627869e-07,\n",
       "       -7.15255737e-07,  7.15255737e-07,  0.00000000e+00, -4.76837158e-07,\n",
       "        2.38418579e-07,  4.76837158e-07, -2.38418579e-07, -5.96046448e-07,\n",
       "        0.00000000e+00,  4.76837158e-07,  3.57627869e-07, -1.19209290e-07,\n",
       "       -4.76837158e-07, -4.76837158e-07,  0.00000000e+00,  2.38418579e-07,\n",
       "        4.76837158e-07,  0.00000000e+00, -2.38418579e-07, -4.76837158e-07,\n",
       "       -2.38418579e-07,  2.38418579e-07,  3.57627869e-07,  3.57627869e-07,\n",
       "       -4.76837158e-07, -3.57627869e-07])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 12880\n",
       "      nit: 134\n",
       "     njev: 140\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-0.09705535, -0.00977033,  0.05218465,  0.09692936,  0.1296418 ,\n",
       "        0.153771  ,  0.17168259,  0.18504174,  0.1950415 ,  0.20254608,\n",
       "        0.2081881 ,  0.21243544,  0.21563808,  0.21805521,  0.2198805 ,\n",
       "        0.22125883,  0.22229706,  0.223082  ,  0.22367747,  0.2241264 ,\n",
       "        0.22447047,  0.22472755,  0.22492687,  0.22507142,  0.22518582,\n",
       "        0.22527438,  0.22534422,  0.22540773,  0.22545294,  0.22547701,\n",
       "        0.50575637,  0.48029643,  0.46183335,  0.44829934,  0.43829966,\n",
       "        0.4308665 ,  0.42531771,  0.42116274,  0.41804251,  0.41569563,\n",
       "        0.41392845,  0.41259538,  0.41159032,  0.4108302 ,  0.41025735,\n",
       "        0.40982537,  0.40949436,  0.40924847,  0.40906276,  0.40891531,\n",
       "        0.40881491,  0.40872848,  0.40868034,  0.40862386,  0.40859795,\n",
       "        0.40856745,  0.4085421 ,  0.40853867,  0.40852666,  0.40848495,\n",
       "       -2.13330257, -2.15875945, -2.17722789, -2.19074899, -2.20077527,\n",
       "       -2.20817346, -2.21373591, -2.21791012, -2.22100612, -2.22334396,\n",
       "       -2.22513686, -2.22648874, -2.22747526, -2.22819952, -2.22877207,\n",
       "       -2.22923777, -2.22960133, -2.22984878, -2.22999475, -2.23009555,\n",
       "       -2.23018972, -2.23030904, -2.23042496, -2.23051203, -2.23052637,\n",
       "       -2.2304666 , -2.23039516, -2.2304032 , -2.23065437, -2.2305859 ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x1c231c93c8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl8FfW5+PHPM2fLDmRhXwKKgICgAioq4tKr9bZi1bbaWnexi722vbZae39dbL1d1F5va9VatYpVaavi0utuRevCEpRVFpE1QBIgkH3P8/vjTMIhJCEkOZmzPO/X67xm5jvfM/N8M8k8me07oqoYY4wxjtcBGGOMiQ2WEIwxxgCWEIwxxrgsIRhjjAEsIRhjjHFZQjDGGANYQjDGGOOyhGCMMQawhGCMMcbl9zqAI5Gbm6v5+fleh2GMMXFl2bJle1Q173D14ioh5OfnU1BQ4HUYxhgTV0Rka1fqxVVC6InZs2d7HYIx3aIA4qA4IAIireMqDhAuA0HdYbiOWy5OeBxa67XORwjPOvi7bqFbhjsdrtv6vdbl0WYe7c9vaU/r6KHzDtQ/UK4dlHdIDp2v7X5HDhp0vOz26kUuu90gOoruEP12Lj5s7YULF3Z5eT2RNAnBmPYogjp+1Beg2QmExx0/6o43O35wfKj4UceHis+dHzEuPtTxgfhQcdxxJ1wuTrjcaRl3DhqGd+gtZXJIHcQu8yW6fjuX0FFa6WtJkxD6KsOa6GtuVipqGymvbaCs5uBPZW0jVfWNVNU1UlnXRFVdeDxc1uSWN1LT0ERtQxMNTd3/QxSBoM8h5A9/Ar7wx+8Tgu4w4HMIOA4Bv+B3HAK+8NDnE/yO4HOEQJtpvyM47tAnB8Ydd9rnCE7LsLUMRFrKwRFBxP2+hOeFv9dykBGu60hLXXcIOE54KG65cHCdlrYLguOEhy31xD3aODAtreXCge9G/gxb6tBOvciDiXbL3XW4VQ5arhtJROHB8yK/IxGFkbEcKDs09sPpat2g7/yD1u+lpEkIJnbV1Dexp7KOvVX17KmoY29VHXsq69lbWc/eqjr2Vtazv6Y+vNOvbqCirpHD9dqeGvCRHvKTEfKRFvSTEfKTmxFkZE4aGUE/qUFf+BPwkRJw3GH4kxoIz0sJOIT8PneH7yPodwi6O/+g38HvSMz8IRvTGywhmKhRVcprG9m5v4ZdZTXs2F/Lrv017CqrZcf+GorKatlTWUd1fVO7388I+cnJCJKdHmRgZgpH52XQLzVAv9QAWe6w9ZMWICslQGaKn7SgH59jO2pjjpQlBNMjzc3KzrIaNu2uYtPuSjbtqWLL3mp27a9h5/4aqtrs7P2OMCgrhWH9U5k6oj95mSFyMoLkpofIzQySkx6ezkkPkRr0edQqY5KTJQTTJTX1TWwormDTnkp351/Fp7sr2bK3itqG5tZ6GSE/+blpjMlL57SxuQztl8qQ/ikM7Z/K0H6p5GWG7L93Y2JUVBKCiDwCfA4oUdVJ7cz/KnCLO1kJfENVV0QjFnPkauqb+HhXGasKy1i5o4zVO8rYWFJJs3ve3ucIIwakMiYvg9OOzmVMXgajc9M5Ki+dvMyQnVc3Jk5F6wjhUeBeYF4H8zcDZ6jqPhH5LPAgcFKUYjGdqG1oYs3OclbvKGNlYXjn/0lJRevOPzcjxHHD+3HepCEcOySLowemMzI7naDfboc0JtFEJSGo6jsikt/J/PcjJhcBw6MRhzlUU7OyZmcZ727cw3sb91CwZR91jeFTPrkZISYPy+LcSYOZPKwfk4f1Y1CW/cdvTLKIhWsI1wIvdzRTROYCcwFGjhzZVzElDFVly95q3t24h/c37uH9T/dSVtMAwPjBmVx+8ihmjM7muOH9GJyVYjt/Y5KYpwlBRM4knBBO66iOqj5I+JQS06ZNi43H+WJcbUMT/1xXwsL1Jby3cS879tcAMLRfCudOHMSpR+cy86hc8jJDHkdqjIklniUEETkOeAj4rKru9SqORNHQ1My7G/fw4vKdvLqmiKr6JvqlBph5VA7fmH0Upx6dS35Omh0BGGM65ElCEJGRwLPA11R1gxcxJILmZmXZtn08v3wHL60qorSqnqwUP5+fMpQLpgzlpDE5dounMabLonXb6VPAbCBXRAqBnwABAFV9APgxkAPc5/7H2qiq06IRS6JRVdbuquD5FTt4cflOdpbVkhJwOGfCIOZMHcasY3IJ+e2BLmPMkYvWXUaXHWb+dcB10Vh3oqprbOL55Tt5+F+bWV9cgd8RZh2Txw/OG89njh1EeigW7g8wxsQz24vEuPLaBp5avI1H3ttMcXkdE4Zk8YsLJ3H+5CFkpwe9Ds8Yk0AsIcSokvJaHn5vM08u2kZFXSOnHp3DnZdM4fSxuXZh2BgTFZYQYszGkkr+9M4mFny0g8bmZs6fPIQbZh3F5OH9vA7NGJPgLCHEiGVb9/HA25/y+sfFhPwOX54+gutPH8PInDSvQzPGJAlLCB4rLq/lF/+3lhdX7KR/WoCbzh7LFaeMIifDHhozxvQtSwgeaWxq5rEPtvI/r2+gvqmZ75wzlrmzxpAWtE1ijPGG7X08sGxrKT9asJp1RRWccUwet8+ZyKicdK/DMsYkOUsIfai0qp5fvbyWvxUUMqRfCg9cfgLnThxsdw0ZY2KCJYQ+0NyszF+6nd+8uo7K2kZuOGMM/3HWWHuYzBgTU2yPFGWrd5Txo+dWs2L7fk4anc3PL5zEMYMyvQ7LGGMOYQkhSlSV+9/+lLteXU92epD/+fIULpw6zE4PGWNiliWEKKiqa+T7T6/gpVVFfO64Idzxhcn0Sw14HZYxxnTKEkIv27KnihseX8YnJRXcdv54rj99jB0VGGPigiWEXrRwfQn/8dRHOI7w2DUzOH1sntchGWNMl1lC6AWqyn0LP+Wu19YzblAmf7piGiOyrcsJY0x8sYTQQ5HXCz4/ZSi/vniyPW1sjIlLtufqAbteYIxJJJYQusmuFxhjEo0lhG54ask2bluwyq4XGGMSiiWEI7Tgo0JuW7CKM47J476vnmDXC4wxCcP2ZkfgldW7uPnvKzl5dA4PXH4iKQGf1yEZY0yvcbwOIF4sXF/Ct5/6iCnD+/HQldMsGRhjEo4lhC5YtGkvNzy+jLEDM/nz1TOsl1JjTEKyhHAYy7fv59pHlzIiO43Hr51hfRIZYxJWVBKCiDwiIiUisrqD+SIivxORjSKyUkROiEYcPfXxznKueHgxORkhnrjuJHvPsTEmoUXrCOFR4LxO5n8WGOt+5gL3RymObttYUsnXHl5MesjPE9edxKCsFK9DMsaYqIpKQlDVd4DSTqrMAeZp2CKgv4gMiUYs3bG9tJrLH1qMCPzlupPsOQNjTFLw6hrCMGB7xHShW+a5orJavvrQYmoamnj82pM4Ki/D65CMMaZPeJUQ2uvwR9utKDJXRApEpGD37t1RDWpvZR1ffWgRpVX1zLtmBhOGZEV1fcYYE0u8SgiFwIiI6eHAzvYqquqDqjpNVafl5UWvv6CmZuWbT3zIjv01PHzlNKaM6B+1dRljTCzyKiG8AFzh3m10MlCmqrs8igWAP7y1kcWbS7njwsmcNCbHy1CMMcYTUXnCSkSeAmYDuSJSCPwECACo6gPAS8D5wEagGrg6GnF0VcGWUu55YwMXTh3KRSfExKUMY4zpc1FJCKp62WHmK/CtaKz7SJVVN3DT/OWMyE7j5xdOsvcZGGOSVlL3waCq3PrsSorLa3nmGzPJTLGnkI0xySupu654csk2Xl5dxPfPHWcXkY0xSS9pE8KG4gpuf/FjTh+by/Wnj/E6HGOM8VxSJoTahiZufPJDMlP83P2lKTiOXTcwxpikvIbwi//7mA3FlTx2zQwGZlofRcYYA0l4hPDK6iL+smgbc2eN4YxjovegmzHGxJukSgg79tdwyzMrOW54P27+t3Feh2OMMTElaRJCY1Mz352/nMamZn536fEE/UnTdGOM6ZKkuYbw+39uZMmWUu758lTyc9O9DscYY2JOUvybvHjTXn7/z0+46IRhXHi8dU1hjDHtSfiEoKrc9dp6RuWkc/ucSV6HY4wxMSvhTxmJCA9dMZ3dlXVkhBK+ucYY021JsYfslxagX5r1U2SMMZ2RcMej8UFEdgNbu/n1XGBPL4bjpURpS6K0A6wtsSpR2tLTdoxS1cM+eBVXCaEnRKRAVad5HUdvSJS2JEo7wNoSqxKlLX3VjoS/qGyMMaZrLCEYY4wBkishPOh1AL0oUdqSKO0Aa0usSpS29Ek7kuYagjHGmM4l0xGCMcaYTlhCMMYYAyRJQhCR80RkvYhsFJFbvY6nu0Rki4isEpHlIlLgdTxHQkQeEZESEVkdUZYtIq+LyCfucICXMXZVB235qYjscLfNchE538sYu0JERojIWyKyVkTWiMhNbnncbZdO2hKP2yVFRJaIyAq3LT9zy0eLyGJ3u/xVRIK9vu5Ev4YgIj5gA/AZoBBYClymqh97Glg3iMgWYJqqxt2DNiIyC6gE5qnqJLfsN0Cpqv7KTdQDVPUWL+Psig7a8lOgUlXv8jK2IyEiQ4AhqvqhiGQCy4ALgauIs+3SSVu+RPxtFwHSVbVSRALAu8BNwPeAZ1V1vog8AKxQ1ft7c93JcIQwA9ioqptUtR6YD8zxOKako6rvAKVtiucAj7njjxH+A455HbQl7qjqLlX90B2vANYCw4jD7dJJW+KOhlW6kwH3o8BZwNNueVS2SzIkhGHA9ojpQuL0F4XwL8VrIrJMROZ6HUwvGKSquyD8Bw0M9DienrpRRFa6p5Ri/jRLJBHJB44HFhPn26VNWyAOt4uI+ERkOVACvA58CuxX1Ua3SlT2Y8mQEKSdsng9T3aqqp4AfBb4lnvqwsSG+4GjgKnALuBub8PpOhHJAJ4BvqOq5V7H0xPttCUut4uqNqnqVGA44bMcE9qr1tvrjatrCLm5uZqfn+91GMYYE1eWLVu2pyud28VV99f5+fkUFHTv5hpVJXytxhhjkouIdKmX6GQ4ZcSdr67jR8+tPnxFY4xJYkmREBqblScXb+P9jXF3t6YxxvSZpEgI3z3nGEblpPHDBauoqW/yOhxjjIlJSZEQUgI+fnnRZLbureaeNzZ4HY4xxsSkpEgIADOPyuXS6SP40782sXpHmdfhGGNMzEmahADww/MnkJMR4gdPr6ShqdnrcIwxJqYkVULolxrg53Mm8vGucv70r01eh2OMMTElqRICwHmThnDexMHc88YnbN5T5XU4xhgTM5IuIQDcPmciKX6HW59ZSXNz/DypbYwx0ZSUCWFgVgo/+vcJLN5cyvyl2w//BWOMSQJJmRAAvjRtBKeMyeGXL62luLzW63CMMcZzSZsQRIRfXjSZ+qZm/t9zq4mnTv6MMSYakjYhAOTnpvO9zxzDax8X8/LqIq/DMcYYTyV1QgC49rTRTBqWxY+fX0NZdYPX4RhjjGeSPiH4fQ6/vvg49lXXc8dLcfeaZWOM6TVJnxAAJg7tx9xZY/hbQSHvWY+oxpgkZQnBddPZYxmdm84Pnl7J7oo6r8Mxxpg+ZwnBlRLwcc+Xp1JaVc9Vf15CRa1dTzDGJBdLCBGmjOjPfZefwLqiCm54fBl1jfbuBGNM8ohKQhCRR0SkREQ6fG+liMwWkeUiskZE3o5GHN1x5riB3HnJcbz/6V6+99cVNFnXFsaYJOGP0nIfBe4F5rU3U0T6A/cB56nqNhEZGKU4uuWiE4azt7KeO15aS05GkJ9dMBER8TosY4yJqqgkBFV9R0TyO6nyFeBZVd3m1i+JRhw9cf2sMeyurOPBdzYxMDPEjWeN9TokY4yJqmgdIRzOMUBARBYCmcD/qmq7RxNeuvW88eypqOOu1zaQkxHishkjvQ7JGGOixquE4AdOBM4GUoEPRGSRqh7ywmMRmQvMBRg5sm93yI4j/PqS4yitrudHC1aRnR7k3ImD+zQGY4zpK17dZVQIvKKqVaq6B3gHmNJeRVV9UFWnqeq0vLy8Pg0SIOBzuO+rJzB5eH++/dRHLN60t89jMMaYvuBVQngeOF1E/CKSBpwErPUolsNKC/r581XTGT4glevmFbCuqNzrkIwxptdF67bTp4APgHEiUigi14rI10Xk6wCquhZ4BVgJLAEeUtUOb1GNBdnpQeZdM4P0oJ8rHl7C9tJqr0MyxpheJfH0HoBp06ZpQUGBpzGsL6rgiw+8T05GiEevns6onHRP4zHGmMMRkWWqOu1w9exJ5SM0bnAmf756OqVV9Xz+9++ycH3M3TFrjDHdYgmhG04clc2LN57GsAFpXP3oUv7w1kZ745oxJu5ZQuimkTlpPPuNmVwwZSh3vrqer/9lGZV1jV6HZYwx3WYJoQdSg+EeUv/r3yfwxtoSLvzDe3y6u9LrsIwxplssIfSQiHDd6WN4/NoZlFbVc+G97/H6x8Veh2WMMUfMEkIvmXlULi9++zTyc9O5fl4Bv319A83WU6oxJo5YQuhFw/qn8vevn8LFJwznd29+wvXzCiirsRftGGPigyWEXpYS8HHXF4/j9jkTeXvDbi78w3sssu4ujDFxwBJCFIgIV5ySz5PXn0x9YzOXPriIbz3xIYX77OlmY0zssoQQRTNGZ/Pmf57Bd885hjfXFXP23W/z29c3UFNvr+Y0xsQeSwhRlhLwcdM5Y3nzP2fzmWMH8bs3P+Hsuxfy4oqd9jCbMSamWELoI8P6p3LvV07gbzecQv+0IN9+6iO+/MdFrN5R5nVoxhgDWELoczNGZ/Pit0/jlxdNZuPuSj5/77v88NlV7K2s8zo0Y0yS8+qNaUnN5wiXzRjJ+ZOH8L9vfMK8D7bwj5U7ueKUUVx+8iiG9Ev1OkRjTBKy7q9jwMaSCu58dT2vfVyMI8J5kwZz1cx8po0agIh4HZ4xJs51tftrSwgxZHtpNY8v2sr8Jdsor21k4tAsrpyZzwVThpIS8HkdnjEmTllCiGPV9Y0899FOHn1/MxuKK8lOD3Lp9BFcfvIohva300nGmCNjCSEBqCofbNrLo+9t4Y21xYgI504cxCUnDufUo3MJ+e2owRhzeF1NCHZROYaJCDOPymXmUblsL63mL4u2Mn/pdl5aVURGyM/scXmcO3EwZ44fSEbINqUxpmfsCCHO1DU28f6ne3ltTRGvrSlmb1U9Qb/DaUfncu7EQZwzYRA5GSGvwzTGxBA7ZZQEmpqVZVv38eqaIl5dU0Thvhocgen52Zw7cTCzx+UxOjfd7lQyJslZQkgyqsqaneW8tqaIV9cUs764AoDcjBDT8wcwLT+bGfnZTBiSid9nzyMak0wsISS5LXuq+GDTXpZuLmXp1lK2l9YAkB70ccKoAUwblc300QM4fsQAUoN2cdqYRGYJwRxkV1kNBVv2sXRLKUu37GNdUTmq4HeE8UMyGTcoiwlDMhk3OJPxg7PIy7TrEMYkCksIplNlNQ18uG0fSzeXsmpHGeuKKthdcaA/pZz0IOMGhxPEhMFZjBucydhBGaQF7W4mY+KN3XZqOtUvNcCZ4wZy5riBrWV7K+tYX1TBuqIK1hWVs76ogvlLtlPTcOD9DXmZIUZmpzEyO40R7rDlMzAzhOPYBWxj4pUlBNMqJyPEzKNDzDw6t7WsqVnZVlrN+qJyNpZUsr20hm2l1SzZXMrzy3fQHHGAGfQ7jBiQyvABaQzKCjEoK4WBmSEGusNBWSnkZYYI2EVtY2KSJQTTKZ8jjM5NZ3Ru+iHz6hub2bk/nCC2lVaz3R0W7qthXVE5uyvqDkoYLXLSg+S5iSI7LUD/tCDZ6UEGuOMD0oIMSA+Eh2lBu+htTB+xhGC6Leh3yM9NJ7+dZAHho4u9VXWUlNdRXF5LScWBYYk73Lynkn1VDVTWNXa4npDfISs1QGbIT2aKn4wUP5mhwIHxlAPzUoM+0oJ+0oI+d9xHWsDfOp4a8NlpLWM6EJWEICKPAJ8DSlR1UjvzZwPPA5vdomdV9fZoxGK843OEgZkpDMxMYdKwfp3WrW9sZn9NPfurG9hXVc++6nr2VTewrzpcVlHbQEVtIxW1jVTWNbK7opLKlun6Ro7k3oiUgENqwEfI7yMUcAj5HVICPkJ+J1zmd9zy8HjAF/4E/Q5Bn4Sn/QeX+R0HvzvP74SHPkcOKfP7BJ8IPufQj99xwvPcOo4DjrSMWxIz0RetI4RHgXuBeZ3U+Zeqfi5K6z/E7Nmz+2pVpo8EgRwgG1BfkGZfiGYngPoCqBOg2R1GjoeHQRodHw2OnwrHj4ofdXyo4z/wkYhpcdxpHzgeHlRrM6gihIfh8fAQIqYjykQjpzm4PnRcjoKCtI5HlLtD0cjpluLI6QPj4i6v7fIj60mb77RddDsFHa5fOpp/SFFv3WXZ/nLkkP9UDo03Z/NrHC7dL1y4sLuBHZGo/Har6jsikh+NZRvTlgDSVI/TVB/1dbXuPt1EgeMLJwvxhafFOVCOgzoRZa11BKWlXA7MR1rnIw6KhMdx67nT6paFl9NSRw6Zbv1O6zwO1I0Yb92Fti6H8HJaf7otdd22S+Qw4gaBg7pIkQO7PolYziHLa2ddbUYP1WamtE0l7X1Z2pkVvaMubfOz6DSWzcQML68hnCIiK4CdwM2quiaaK+urDGuMMUfmUq8DaOVVQvgQGKWqlSJyPvAcMLa9iiIyF5gLMHLkyL6L0BhjkkzUnlR2Txn9o72Lyu3U3QJMU9U9h6m3G9jazZBygU6XH0cSpS2J0g6wtsSqRGlLT9sxSlXzDlfJkyMEERkMFKuqisgMwici9x7ue11pUCfrLOjKo9vxIFHakijtAGtLrEqUtvRVO6J12+lTwGwgV0QKgZ8AAQBVfQC4BPiGiDQCNcClGk+dKhljTAKK1l1Glx1m/r2Eb0s1xhgTI5KpU5kHvQ6gFyVKWxKlHWBtiVWJ0pY+aUdcdX9tjDEmepLpCMEYY0wnkiIhiMh5IrJeRDaKyK1ex9NdIrJFRFaJyHIRias3BYnIIyJSIiKrI8qyReR1EfnEHQ7wMsau6qAtPxWRHe62We4+XxPTRGSEiLwlImtFZI2I3OSWx9126aQt8bhdUkRkiYiscNvyM7d8tIgsdrfLX0Uk2OvrTvRTRiLiAzYAnwEKgaXAZar6saeBdUNXn9eIRSIyC6gE5rU8myIivwFKVfVXbqIeoKq3eBlnV3TQlp8Clap6l5exHQkRGQIMUdUPRSQTWAZcCFxFnG2XTtryJeJvuwiQ7j64GwDeBW4Cvke4I9D5IvIAsEJV7+/NdSfDEcIMYKOqblLVemA+MMfjmJKOqr4DlLYpngM85o4/RvgPOOZ10Ja4o6q7VPVDd7wCWAsMIw63SydtiTsaVulOBtyPAmcBT7vlUdkuyZAQhgHbI6YLidNfFMK/FK+JyDK3S494N0hVd0H4DxoYeJj6se5GEVnpnlKK+dMskdyeBY4HFhPn26VNWyAOt4uI+ERkOVACvA58CuxX1ZYXh0RlP5YMCaG9Lg3j9TzZqap6AvBZ4FvuqQsTG+4HjgKmAruAu70Np+tEJAN4BviOqpZ7HU9PtNOWuNwuqtqkqlOB4YTPckxor1pvrzcZEkIhMCJiejjhHlbjjqrudIclwALCvyjxrNg999tyDrjE43i6TVWL3T/iZuBPxMm2cc9RPwM8oarPusVxuV3aa0u8bpcWqrofWAicDPQXkZaHiaOyH4uri8q5ubman5/vdRjGGBNXli1btidmO7frrvz8fAoKjvxuy6Zmpaymgez0Xr9LyxhjYp6IdKmX6GQ4ZcT18wqYO6+A5ub4ORoyxpi+lhQJ4fzJQyjYuo+/LO7uqxSMMSbxJUVCuPiEYZw+Npdfv7yOnftrvA7HGGNiUlIkBBHhv78wmWaF/3puNfF0Id0YY/pKUiQEgBHZafznvx3DP9eV8MKKuLzr1BhjoippEgLA1aeOZsqI/vzsxY8prar3OhxjjIkpSZUQfI7w64snU17TwM//EXd92xljTFQlVUIAGD84i2/OPooFH+1g4fq4eADTGGP6RNIlBIBvnXU0Rw/M4EcLVlNV13j4LxhjTBJIyoQQ8vv49cWT2VlWw52vrvc6HGOMiQlJmRAAThyVzRUnj+KxD7awbOs+r8MxxhjPJW1CAPj+eeMZkpXCrc+spK6xyetwjDHGU0mdEDJCfu74wmQ+Kankvrc+9TocY4zxVFInBIAzxw9kztSh3LdwIxuKK7wOxxhjPJP0CQHgx587loyQn1ueWUmT9YhqjElSXUoIInKeiKwXkY0icms782eJyIci0igil7SZ94qI7BeRf7Qpf1RENovIcvcztWdN6b6cjBA/+fxEPtq2n3kfbPEqDGOM8dRhE4KI+IA/EH6P77HAZSJybJtq24CrgCfbWcSdwNc6WPz3VXWq+1ne5aijYM7Uocwel8evXl5HwZZSL0MxxhhPdOUIYQawUVU3qWo9MB+YE1lBVbeo6kqgue2XVfVNIOZPzosId39xCsP6p3LNo0vteoIxJul0JSEMA7ZHTBe6Zb3hDhFZKSL/IyKhXlpmt+VkhHjsmhmkBHxc8fASdti7E4wxSaQrCUHaKeuNK68/BMYD04Fs4JZ2Vy4yV0QKRKRg9+7dvbDazo3ITuOxa2ZQVd/I1x5ebL2iGmOSRlcSQiEwImJ6ONDjFwqo6i4NqwP+TPjUVHv1HlTVaao6LS8vr6er7ZIJQ7J4+MrpFO6r4ZpHl1Jdb/0dGWMSX1cSwlJgrIiMFpEgcCnwQk9XLCJD3KEAFwKre7rM3jRjdDa/v+x4Vhbu55tPfEhD0yGXR4wxJqEcNiGoaiNwI/AqsBb4m6quEZHbReQCABGZLiKFwBeBP4rImpbvi8i/gL8DZ4tIoYic6856QkRWAauAXOAXvdmw3nDuxMH89xcms3D9bm55eiXN9oyCMSaB+btSSVVfAl5qU/bjiPGlhE8ltffd0zsoP6vrYXrn0hkj2VNZx12vbSA3M8Rt50/wOiRjjImKLiWEZPetM49md0UdD76zidyMIHNnHeV1SMYY0+ssIXSBiPDjz09kT1U9//3SOnLSQ1x8YrsHRMYYE7csIXSRzxF++6Up7K+u5wfPrCQ7PciZ4wd6HZYxxvQa69zuCIT8Ph64/EQmDMnlZ+6CAAAMdklEQVTkG08s4/nlO7wOyRhjeo0lhCOUmRLgsatnMHlYP26av5xf/ONjGu2WVGNMArCE0A05GSGeuO5krjxlFA+9u5mvPbyEvZV1XodljDE9Ygmhm4J+h5/NmcRdX5zCsm37+Pzv32VVYZnXYRljTLdZQuihS04czjNfn4mIcPED7/P3gu2H/5IxxsQgSwi9YPLwfrxw46lMGzWA7z+9kv/33GrqG+26gjEmvlhC6CU5GSHmXTODubPG8PiirXzlT4soKa/1OixjjOkySwi9yO9zuO38CfzusuNZs7Ocz/3+XZZt3ed1WMYY0yWWEKLggilDefabM0kJ+Lj0wQ/45ctrqayzLrSNMbHNEkKUTBiSxYs3nsYFU4bxx7c3ceZdC3l6WaH1mGqMiVmWEKKoX1qAu780hQXfnMnQ/qnc/PcVXHT/+yzfvt/r0Iwx5hCWEPrA8SMHsOAbM7nri1PYsb+GC//wHjf/fQUlFXbR2RgTOywh9BHHES45cThv3TybG84Yw/PLd3DWXW/zx7c/tVtUjTExwRJCH8sI+fnhZyfw2nfP4KTR2fzy5XWce887vLm2GFW7vmCM8Y4lBI+Mzk3n4aum8+jV0xGBax8r4IJ73+PpZYXUNjR5HZ4xJgl1KSGIyHkisl5ENorIre3MnyUiH4pIo4hc0mbeKyKyX0T+0aZ8tIgsFpFPROSvIhLsWVPi0+xxA3nlplnc8YVJ1DY0cfPfVzDzV//kzlfXsausxuvwjDFJRA53mkJEfMAG4DNAIbAUuExVP46okw9kATcDL6jq0xHzzgbSgBtU9XMR5X8DnlXV+SLyALBCVe/vLJZp06ZpQUHBETUwnqgqH3y6l0ff38Iba4sREc6dOIgrT8lnxuhsRMTrEI0xcUhElqnqtMPV68ob02YAG1V1k7vg+cAcoDUhqOoWd94hV0dV9U0Rmd0mOAHOAr7iFj0G/BToNCEkOhFh5tG5zDw6l+2l1fxl0VbmL93OS6uKGD84k6tm5jNn6jBSgz6vQzXGJKCunDIaBkR24VnolvVEDrBfVVse3+1wmSIyV0QKRKRg9+7dPVxt/BiRncYPz5/Aoh+eza8umgzArc+u4uRfvsltC1bxzobddneSMaZXdeUIob3zFD29HabLy1TVB4EHIXzKqIfrjTupQR+XzhjJl6ePYMnmUh5ftJXnPtrBk4u3kZni55wJgzh34iBmHZNHWtBekW2M6b6u7EEKgRER08OBnT1c7x6gv4j43aOE3lhmQhMRThqTw0ljcqhtaOLdT/bwypoi3lhbzIKPdpAScJg1No9zJw7mnAmD6JcW8DpkY0yc6UpCWAqMFZHRwA7gUg6c++8WVVUReQu4BJgPXAk835NlJpOUgI9zjh3EOccOorGpmSWbS3l1TRGvrinmtY+L8TvCyWNymD0uj+n52UwcmoXfZ3cYG2M6d9i7jABE5HzgHsAHPKKqd4jI7UCBqr4gItOBBcAAoBYoUtWJ7nf/BYwHMoC9wLWq+qqIjCGcDLKBj4DLVbXTFxMn+l1GPdXcrKzcUeYmhyI27a4CIC3o44SRA5ien8300QM4fsQAuzBtTBLp6l1GXUoIscISwpEpLq9lyeZSCraUsmTLPtYVlaMKfkeYNKwfM0ZnMz0/m+NH9ic3I+R1uMaYKLGEYA5RVtPAh1v3sWRLKUs3l7KysIz6pvCdSrkZQcYNzmTcoCzGD85k3OBMxg7KsAvVxiSA3nwOwSSIfqkBzhw/kDPHDwSgtqGJlYVlrNpRxvqictYXVfDUkm3UuF1niMDI7DTGDcpk/OBMxg7KZGR2GiOz0+ifFrAH5YxJMJYQklhKwMeM0dnMGJ3dWtbcrGwrrWZdUQXriypYXxxOFG+sLSby3T6ZIT8j3OQwIjvVHYanhw1IJeS3axTGxBtLCOYgjiPk56aTn5vOeZMGt5bXNjSxZW8V20tr2FZazfbSaraVVvPp7kreWl9CXZuH5LLTgwzMDDEoK4VBWSEGZrrDrJTWstyMEAG7+8mYmGEJwXRJSsDH+MFZjB+cdci85mZlT2Ud29wkUbivhuLyWorL6yipqGVdUTl7Kutpauf1oVkpfrLTg/RPC7rDANlpQQakBxmQFmRAWoD+aUGyUv1khgJkpPjJCPkJ+i2RGNPbLCGYHnMcYWBWCgOzUpiWn91unaZmZW9VHSXlda3Jori8ln3V9eyrbmBfVT3F5bWs21XOvuqG1usYHQn5HTLd5JCRciBZpAV9pAV9pATCw9SAj9Sgn9TAgfJUtzzkdwgFHEJ+HynuMOR3CPkde27DJCVLCKZP+BxhYGYKAzNTmDSs32Hr1zY0hZNFVQP7quupqG2koraByrpGKmsbqaxrpMIdbynfXlpNdX0TNQ1N1NY3Ud3Q1O5RSVfjbUkOQb9DwBceBtsMI8sDPsHfMnTC88JlLdNCwOfgc+Tgj4SHfp/gSLiuzwFHwtOOE35SPTwNPhF3OpyMhfB8kXCfMI477rgX/VvGW4dumciB7zoCgrsMObDuyOmWZTvuipy2y3WXEVke6zceHOldlrHenp5KmoQwe/Zsr0MwfcAB0t1PNoA4NDsB1AmgvsCBcceHOv4DH2kz7fhQ8dPo+GlwfO788JCI8cgh4kPFAXFQccJ1HXcoSXzEoQqoOwRQBHV7L9OIOhHTETru+CxizkE76nDC0tb50qaOtKl/hJqbEJrDMWsz0jJ0y0Sb3QgPrEOlTSxujCAM//CB8M+jEwsXLux+vEcgaRKCST4CoM34muqgqQ4avItFwU0UPhBBaUkc0ppAQNyE4pa17rjE/c6BaY0ob9nJHLzTidwpttkBtiyrpW7E9/SQnaa7q4pYT9tYDl1X5LIOXY+2WeeBZbansx23Hjz3kKQSTjrSZvrgpKNIFw8S1G0X4m4nDt1+reMibdblprC2McbYc2BJkxD6KsMaY8yRuc3rAFol8XGsMcaYSHHVdYWI7Aa2dvPruYS73U4EidKWRGkHWFtiVaK0paftGKWqeYerFFcJoSdEpKArfXnEg0RpS6K0A6wtsSpR2tJX7bBTRsYYYwBLCMYYY1zJlBAe9DqAXpQobUmUdoC1JVYlSlv6pB1Jcw3BGGNM55LpCMEYY0wnkiIhiMh5IrJeRDaKyK1ex9NdIrJFRFaJyHIRiatXx4nIIyJSIiKrI8qyReR1EfnEHQ7wMsau6qAtPxWRHe62We6+hzymicgIEXlLRNaKyBoRucktj7vt0klb4nG7pIjIEhFZ4bblZ275aBFZ7G6Xv4pIsNfXneinjETEB2wAPgMUAkuBy1T1Y08D6wYR2QJMU9W4u69aRGYBlcA8VZ3klv0GKFXVX7mJeoCq3uJlnF3RQVt+ClSq6l1exnYkRGQIMERVPxSRTGAZcCFwFXG2XTppy5eIv+0iQLqqVopIAHgXuAn4HvCsqs4XkQeAFap6f2+uOxmOEGYAG1V1k6rWA/OBOR7HlHRU9R2gtE3xHOAxd/wxwn/AMa+DtsQdVd2lqh+64xXAWmAYcbhdOmlL3NGwSncy4H4UOAt42i2PynZJhoQwDNgeMV1InP6iEP6leE1ElonIXK+D6QWDVHUXhP+ggYEex9NTN4rISveUUsyfZokkIvnA8cBi4ny7tGkLxOF2ERGfiCwHSoDXgU+B/ara6FaJyn4sGRJCx73nxp9TVfUE4LPAt9xTFyY23A8cBUwFdgF3extO14lIBvAM8B1VLfc6np5opy1xuV1UtUlVpwLDCZ/lmNBetd5ebzIkhEJgRMT0cGCnR7H0iKrudIclwALCvyjxrNg999tyDrjE43i6TVWL3T/iZuBPxMm2cc9RPwM8oarPusVxuV3aa0u8bpcWqrofWAicDPQXkZYeqqOyH0uGhLAUGOteoQ8ClwIveBzTERORdPdiGSKSDvwbsLrzb8W8F4Ar3fErgec9jKVHWnagri8QB9vGvXj5MLBWVX8bMSvutktHbYnT7ZInIv3d8VTgHMLXRN4CLnGrRWW7JPxdRgDurWb3AD7gEVW9w+OQjpiIjCF8VADh91g8GU/tEJGngNmEe20sBn4CPAf8DRgJbAO+qKoxf7G2g7bMJnxaQoEtwA0t5+FjlYicBvwLWAU0u8W3ET73HlfbpZO2XEb8bZfjCF809hH+p/1vqnq7uw+YT/hlgB8Bl6tqXa+uOxkSgjHGmMNLhlNGxhhjusASgjHGGMASgjHGGJclBGOMMYAlBGOMMS5LCMYYYwBLCMYYY1yWEIwxxgDw/wGGLBbR3ggmRwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(3,1)\n",
    "ax[0].plot(kp)\n",
    "ax[1].plot(hy)\n",
    "ax[2].plot(hm)\n",
    "ax[0].hlines(kss, 0, T)\n",
    "ax[1].hlines(hyss, 0, T)\n",
    "ax[2].hlines(hmss, 0, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 3 (A)\n",
    "\n",
    "Show that for an arbitrary $m$-by-$n$ matrix $A$ with $m \\ge n$ and rank $n$ - i.e., that has linearly independent columns - the matrix product $A^T A$ is positive definite and symmetric. Recall that a square matrix $B$ of order $n$ is *positive definite* if there is a positive scalar such that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{x}^T B x \\ge \\alpha \\mathbf{x}^T \\mathbf{x} \\quad \\text{for all} \\quad \\mathbf{x} \\in \\mathbb{R}^n.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 4 (N)\n",
    "\n",
    "Use gradient descent to solve the monopoly example from lecture 5a, which is summarized in the cells below. That is, define a **step** function for gradient descent that can be used use in the **my_opt** routine, also given below. The distance $\\alpha$ should be fixed for a given run of the algorithm, but you can play around with different values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### functions and parameters\n",
    "alpha = 0.98\n",
    "eta = 0.85\n",
    "\n",
    "cy = 0.62\n",
    "cz = 0.6\n",
    "\n",
    "Cy = lambda x : cy * x\n",
    "Cz = lambda x : cz * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def u(Y, Z):\n",
    "    \"\"\"\n",
    "    Returns the u function.\n",
    "    \"\"\"\n",
    "    return (Y**alpha + Z**alpha)**(eta/alpha)\n",
    "\n",
    "def ud(Y, Z):\n",
    "    \"\"\"\n",
    "    Returns the derivative of the u function.\n",
    "    \"\"\"\n",
    "    return eta * (Y**alpha + Z**alpha)**(eta/alpha - 1) * Y**(alpha - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def obj(x):\n",
    "    \"\"\"\n",
    "    Implements the objective function (here profit) for the monopoly example: Y * ud(Y, Z) + Z * ud(Z, Y) - Cy(Y) - Cz(Z)\n",
    "    \"\"\"\n",
    "    Y = np.exp(x[0])\n",
    "    Z = np.exp(x[1])\n",
    "    \n",
    "    return - (Y * ud(Y, Z) + Z * ud(Z, Y) - Cy(Y) - Cz(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_opt(x, obj, step, maxit = 100, eps = 1e-8, delta = 1e-4):\n",
    "    \"\"\"\n",
    "    Implements the iterative procedure for all of the optimization algorithms considered below. \n",
    "    Mandatory inputs are \n",
    "    -> x: initial guess for the minimizing vector\n",
    "    -> obj: objective, defined a function of x\n",
    "    -> step: a function implementing how the step from x(k) to x(k+1) is determined\n",
    "    \"\"\"\n",
    "    dist = 1\n",
    "    it = 0\n",
    "    \n",
    "    lx = []\n",
    "    while dist > eps and it < maxit:\n",
    "        lx.append(x)\n",
    "        it += 1\n",
    "    \n",
    "        s, alpha = step(x, obj)\n",
    "    \n",
    "        dist = np.linalg.norm(s) / (1 + np.linalg.norm(x))\n",
    "\n",
    "        x = x + s\n",
    "    \n",
    "    ## check for optimality\n",
    "    gr = sm.tools.numdiff.approx_fprime(x, obj)\n",
    "    if np.linalg.norm(gr) > delta * (1 + abs(obj(x))):\n",
    "        print('Solution does not appear an optimum!')\n",
    "    \n",
    "    return x, lx  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def descent(x, obj):\n",
    "    \n",
    "    gr = sm.tools.numdiff.approx_fprime(x, obj)\n",
    "    p = -gr\n",
    "    \n",
    "    alpha = 2\n",
    "    \n",
    "    return (alpha * p , alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equilibrium quantities: [0.56984271 2.9356177 ]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([1.5, 2])\n",
    "x, lx = my_opt(x0, obj, descent, maxit = 500)\n",
    "print(\"Equilibrium quantities: {}\".format(np.exp(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (N)\n",
    "\n",
    "In this question, we are going to apply the gradient descent minimization algorithm on a least-squares regression problem. Consider the Bundesliga data set used in the *Introduction to Python* section of this class. Let's assume we would like to regress a player's market value on his age, his number of goals and assists. Running the following cell (i) reads in the relevant columns of the data set; (ii) creates a matrix **'X'** with the explanatory variables; and (iii) creates an array **'y'** containing the dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'value')\n",
      "(4, 'age')\n",
      "(5, 'goals')\n",
      "(6, 'assists')\n",
      "(291, 3)\n",
      "(291, 1)\n",
      "[[28. 30.  4.]\n",
      " [27. 29.  2.]\n",
      " [27.  5. 12.]\n",
      " [27.  5.  4.]\n",
      " [26.  4.  3.]\n",
      " [20.  6. 11.]\n",
      " [26.  2.  2.]\n",
      " [28. 10.  2.]\n",
      " [20.  2.  1.]\n",
      " [20.  2.  1.]]\n"
     ]
    }
   ],
   "source": [
    "cols=(2,4,5,6)\n",
    "D = np.loadtxt('BundesligaData.txt', delimiter=';',usecols=(cols), skiprows=1)\n",
    "D[:10, :]\n",
    "\n",
    "description = ['name', 'position', 'value', 'valuemax', 'age', 'goals','assists', 'yellow', 'red', 'shotspergame','passsuccess','aerialswon', 'rating', 'positioncode']\n",
    "for i in cols:\n",
    "    print((i,description[i]))\n",
    "\n",
    "X = D[:,1:]\n",
    "## dependent variable\n",
    "y = D[:,0] \n",
    "y.shape=(D.shape[0], 1)\n",
    "# Before regressing the values, we should check whether X and y have the right shape\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(X[:10, :])     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Scale the values in **X** and add a column of ones. \n",
    "\n",
    "(b) For comparison, use the normal equation to compute $\\mathbf{b}$.\n",
    "\n",
    "(c) Implement the gradient descent algorithm outlined above to find $\\mathbf{b}$. Assume that the step size $\\alpha$ is constant. You may have to play around with $\\alpha$ to find a value that gives you convergence. *Hint*: Recall that in the context of gradient descent, convergence may be slow. When implementing the algorithm above with a **while** loop, you should (as we always do) include a condition that the loop stops after a certain number of iterations, **maxit**. Make sure to set **maxit** sufficiently high in order to get convergence.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use scikit-learn for feature scaling\n",
    "X = preprocessing.scale(X)    \n",
    "## add column of ones to X    \n",
    "X = np.column_stack((np.ones( X.shape[0] ), X ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.12285223]\n",
      " [-1.31745985]\n",
      " [ 3.78177336]\n",
      " [ 1.37891467]]\n"
     ]
    }
   ],
   "source": [
    "## run OLS manually\n",
    "b = np.linalg.inv((X.T @ X)) @ X.T @ y\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.12285223 -1.31745985  3.78177336  1.37891467]]\n"
     ]
    }
   ],
   "source": [
    "## use scikit-learn\n",
    "lr = LinearRegression(fit_intercept = False) # we've already added the intercept term in X\n",
    "lr.fit(X, y)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_linear(b, X, y, alpha, show_it = False, maxit = 50000, eps = 1e-8):\n",
    "    \"\"\"\n",
    "    Implements the iterative procedure for gradient descent in the context of linear regression. Inputs are \n",
    "    -> b: initial guess for the minimizing coefficient vector\n",
    "    -> X: the m-by-n regression matrix, with each row containing the n features for one observation\n",
    "    -> y: the m-by-1 vector of target values \n",
    "    -> alpha: a scalar indicating the step size from b(k) to b(k+1)\n",
    "    \"\"\"\n",
    "    dist = 1\n",
    "    it = 0\n",
    "    \n",
    "    m = len(y)\n",
    "    \n",
    "    while dist > eps and it < maxit:\n",
    "        it += 1\n",
    "    \n",
    "        s = -alpha * m**(-1) * X.T @ ( X @ b - y )\n",
    "    \n",
    "        dist = np.linalg.norm(s) / (1 + np.linalg.norm(b))\n",
    "  \n",
    "        b = b + s\n",
    "    \n",
    "    if show_it:\n",
    "        print(\"Gradient descent has converged in {} iterations\".format(it))\n",
    "    \n",
    "    return b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initial guess for coefficients\n",
    "b0 = np.array([[0, -0.1, 0.3, 0.1]]).T\n",
    "## set learning rate\n",
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent has converged in 15327 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8.12285046],\n",
       "       [-1.31743475],\n",
       "       [ 3.7816535 ],\n",
       "       [ 1.37902944]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_linear(b0, X, y, alpha, show_it = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
