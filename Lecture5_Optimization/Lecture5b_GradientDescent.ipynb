{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computational Methods in Economics\n",
    "\n",
    "## Lecture 5b - Numerical Optimization and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last update: 2018-12-05 14:08:07.928452\n"
     ]
    }
   ],
   "source": [
    "# Author: Alex Schmitt (schmitt@ifo.de)\n",
    "\n",
    "import datetime\n",
    "print('Last update: ' + str(datetime.datetime.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(precision=4, suppress = True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import scipy.optimize\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Lecture\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [The Scikit-Learn Package](#scikit)\n",
    "- [Notation and Terminology](#not)\n",
    "- [Linear Least-Squares Problems](#lls)\n",
    "- [Gradient Descent](#grad)\n",
    "- [Nonlinear LS: LASSO](#lasso)\n",
    "- [Classification: Logistic Regression](#logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In the previous lecture we have discussed how to solve general optimization problems numerically. In this class, we are going to look at particular optimization problems that (among other applications) arise frequently in the context of data analysis.\n",
    "\n",
    "In addition, we also introduce more functionality for Python, in particular the scikit-learn package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Scikit-learn is most commonly used in the context of \"data science\" and \"machine learning\". An additional goal of this class is to give a short introduction in particular to machine learning (also called \"statistical learning\"), and show how it differs (and what it has in common) with more standard (from an economist's point of view) approaches to data analysis in econometrics.\n",
    "\n",
    "It goes without saying that given the complexity of the topic and the limited amount of time, we will not be able to go in much depth. That said, the idea is to introduce some core ideas and terminology, and leave the rest to more specialized courses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'scikit'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Scikit-Learn Package\n",
    "\n",
    "Scikit-learn (**sklearn**) is a very popular package for data analysis in  general and machine learning in particular. It builds on the packages we have already seen - NumPy, SciPy and Matplotlib - and provides a lot of additional functionality (of which we will only scratch the surface).\n",
    "\n",
    "Its documentation and an extensive amount of tutorials and other material can be found here: https://scikit-learn.org/stable/index.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar to SciPy, to use Scikit-Learn, we import individual subpackages rather than the whole package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For now, just focus on the first statement in the cell above. Scikit-Learn includes a few \"toy datasets\" (as well as easy access to some larger \"real-world\" datasets), one of which we will use as a workhorse example below. It contains data on house prices and features in the Boston area. \n",
    "\n",
    "We load the dataset - which comes in the form of what is essentially a dictionary - and display its **DESCR** key, which contains a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "print( boston.DESCR )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can check all key-value pairs in the dictionary by running the **keys()** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'feature_names', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print( boston.keys() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The data is contained in the **data** and the **target** field. We can check that the number of data points in **data** are consistent with the description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (506, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape: {}\".format(boston.data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'not'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Notation and Terminology\n",
    "\n",
    "In the following, we use the notation that is standard in data analysis, in particular econometrics and data science. While this makes it consistent with other courses that you might have taken in the past, the notation will be different compared to the previous lectures in this class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In particular, let $(y_i, \\mathbf{x}_i)$ be a given tuple of data corresponding to observation $i$. \n",
    "\n",
    "$y_i$ is a scalar and referred to as a dependent variable, a *label* (if it comes from a discrete set of values) or *target variable* (if it is a continuous variable).\n",
    "\n",
    "$\\mathbf{x}_i = [x_{i1}, ... , x_{in}]^T \\in \\mathbb{R}^n$ is a vector of dimension $n$. Each element of $\\mathbf{x}_i$ is referred to as an independent variable, a *regressor* or a *feature*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a data set consisting of $m$ observations, $\\mathbf{y} = [y_{1}, ... , y_{m}]^T  \\in \\mathbb{R}^m$ is a vector of outcome variables, while\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{X} =  \\left[ \\begin{array}{c}\n",
    "    \\mathbf{x}_1^T  \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{x}_m^T \n",
    "    \\end{array} \\right] =  \n",
    "    \\left[ \\begin{array}{cccc}\n",
    "    x_{11} & x_{12} & ... & x_{1n}  \\\\\n",
    "    x_{21} & x_{22} & ... & x_{2n}  \\\\\n",
    "    \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & ... & x_{mn}  \\\\\n",
    "    \\end{array} \\right] \n",
    "\\end{equation}\n",
    "\n",
    "is an *m*-by-*n* matrix containing all realizations of the feature variables. It is also referred to as the regression or feature matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the context of our example dataset above, we define **y** as the vector of target variables (in **boston.target**) and **X** as the feature matrix containing the regressors in **boston.data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = boston.target\n",
    "X = boston.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.0063  18.       2.31     0.       0.538    6.575   65.2      4.09\n",
      "    1.     296.      15.3    396.9      4.98  ]\n",
      " [  0.0273   0.       7.07     0.       0.469    6.421   78.9      4.9671\n",
      "    2.     242.      17.8    396.9      9.14  ]\n",
      " [  0.0273   0.       7.07     0.       0.469    7.185   61.1      4.9671\n",
      "    2.     242.      17.8    392.83     4.03  ]\n",
      " [  0.0324   0.       2.18     0.       0.458    6.998   45.8      6.0622\n",
      "    3.     222.      18.7    394.63     2.94  ]\n",
      " [  0.0691   0.       2.18     0.       0.458    7.147   54.2      6.0622\n",
      "    3.     222.      18.7    396.9      5.33  ]]\n"
     ]
    }
   ],
   "source": [
    "## display first five observations\n",
    "print( X[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Hypothesis and Residual Function\n",
    "\n",
    "On a very general level, when analyzing data, we compare the realizations of the target variable in the sample, $\\mathbf{y}$, with a vector $[\\hat{y}_{1}, ... , \\hat{y}_{m}]^T$ of *predicted* outcomes. \n",
    "\n",
    "The predicted value of the target variable for observation $i$ is given by a function $h$, which (in machine learning terminology) is called the *hypothesis*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More precisely, the function $\\hat{y}_{i} = h(\\mathbf{x}_i; \\mathbf{b})$ returns the prediction for the target variable $y_i$ given the feature vector $\\mathbf{x}_i$ and a vector of *parameters* $\\mathbf{b}$. \n",
    "\n",
    "No matter what the purpose of an empirical analysis is, e.g. causal estimation (in econometrics) or prediction (in statistical/machine learning), it usually requires determining the parameter values given a data set, $(\\mathbf{y}, \\mathbf{X})$. This step is done by setting it as a minimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is, we choose $\\mathbf{b}$ by minimizing a measure of the difference between the observed values of the outcome variable and its predicted values when using the hypothesis function. For a single observation, this difference is called the *residual (function)*:\n",
    "\n",
    "\\begin{equation}\n",
    "    h(\\mathbf{x}_i; \\mathbf{b})- y_i \\equiv r_i(\\mathbf{b}; y_i, \\mathbf{x}_i) = r_i(\\mathbf{b})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note the following:\n",
    "\n",
    "- We have defined $h$ as a function of the data $\\mathbf{x}_i$, given parameter values $\\mathbf{b}$. In contrast, we write the residual as a function of $\\mathbf{b}$. Of course, $r_i$ also depends on the data, but we usually omit these as arguments.\n",
    "\n",
    "- Related to this, we have different residual functions $r_i$ for every observation $i$.\n",
    "\n",
    "- Every $r_i$ is a scalar function, with $r_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, let $\\mathbf{r}$ denote the vector of residuals (i.e. a vector-valued function $\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$) across the sample:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{r}(\\mathbf{b})  =  \\left[ \\begin{array}{c}\n",
    "    r_1(\\mathbf{b})  \\\\\n",
    "    \\vdots \\\\\n",
    "    r_m(\\mathbf{b}) \n",
    "    \\end{array} \\right] \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Least-Squares Minimization\n",
    "\n",
    "There are different alternatives with respect to how to set up the objective function $L(\\mathbf{b})$ in the minimization problem to find the parameter values $\\mathbf{b}$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\min_{\\mathbf{b}} L(\\mathbf{b})\n",
    "\\end{equation}\n",
    "\n",
    "$L$ is also referred to as the *loss function* or *cost function*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#####  $\\mathcal{l}_\\infty$-norm\n",
    "\n",
    "Define $L$ as \n",
    "\n",
    "\\begin{equation}\n",
    "    L(\\mathbf{b}) = \\left| \\left| \\mathbf{r}(\\mathbf{b}) \\right| \\right|_{\\infty} = \\max_{i = 1,...,m} \\left| r_i(\\mathbf{b}) \\right|\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the loss function is determined by the data point with the largest deviation (in absolute value) from its predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#####  $\\mathcal{l}_1$-norm\n",
    "\n",
    "Define $L$ as \n",
    "\n",
    "\\begin{equation}\n",
    "    L(\\mathbf{b}) = \\left| \\left| \\mathbf{r}(\\mathbf{b}) \\right| \\right|_{1} = \\sum_{i = 1}^m \\left| r_i(\\mathbf{b}) \\right|\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the loss function is the sum of absolute values of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#####  $\\mathcal{l}_2$/Euclidean norm\n",
    "\n",
    "Define $L$ as \n",
    "\n",
    "\\begin{equation}\n",
    "    L(\\mathbf{b}) = \\frac{1}{2} \\left(\\ \\left| \\left|\\ \\mathbf{r}(\\mathbf{b}) \\right| \\right|_{2}\\ \\right)^2 = \\frac{1}{2}\\sum_{i = 1}^m \\left[r_i(\\mathbf{b})\\right]^2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a side note, recall that for a vector $\\mathbf{v} \\in \\mathbb{R}^n$, the Euclidean norm is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "     \\left| \\left|\\ \\mathbf{v}\\ \\right| \\right|_2 = \\left| \\left|\\ \\mathbf{v}\\ \\right| \\right| = \\sqrt{ \\sum_{j = 1}^n v_i^2} = \\sqrt{ \\mathbf{v}^T \\mathbf{v} }\n",
    "\\end{equation}\n",
    "\n",
    "Above, we use one half of the square of the norm; of course, this does not affect the result of the minimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, the loss function is the sum of squares of the residuals. Minimizing this function is the by far most common method for finding $\\mathbf{b}$.\n",
    "\n",
    "In other words, we set up the problem of determining $\\mathbf{b}$ as a *least-squares (LS) problem*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "LS problems are an extremely important class of unconstrained optimization problems. Compared to a generic minimization problem, as discussed in the previous lecture, the special form of the objective function in \n",
    "\n",
    "\\begin{equation}\n",
    "   \\min_{\\mathbf{b}} L(\\mathbf{b}) = \\min_{\\mathbf{b}} \\frac{1}{2}\\sum_{i = 1}^m \\left[r_i(\\mathbf{b})\\right]^2\n",
    "\\end{equation}\n",
    "\n",
    "makes LS problems easier to solve than more general problems (see below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before moving on, let's recap the different types of functions defined above:\n",
    "- the hypothesis function $h(\\mathbf{x}_i; \\mathbf{b})$ gives a prediction of the target value for data point $\\mathbf{x}_i$;\n",
    "- the difference between the actual and predicted value gives rise to the residual function $r_i(\\mathbf{b})$;\n",
    "- in LS, the loss function is defined as the sum of squared residuals; minimizing it determines $\\mathbf{b}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the LS method imposes a functional form on $L$, but does not prescribe what $h$ (and hence $r$) looks like. If $h$ is linear in the parameters $\\mathbf{b}$, the problem becomes the familiar LS linear regression framework that you are familiar with.\n",
    "\n",
    "It is important to understand though that this is one particular class of LS problems. In other words, the use LS is not restricted to linear hypothesis functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'lls'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Least-Squares Problems\n",
    "\n",
    "In linear LS problems, the hypothesis function is *linear in the parameters* $\\mathbf{b}$ (but not necessarily linear in the features $\\mathbf{x}$!).\n",
    "\n",
    "Hence,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y}_i = h(\\mathbf{x}_i; \\mathbf{b}) = b_1 x_{i1} + b_2 x_{i2} + ... + b_{n} x_{in} = \\sum_{j = 1}^n b_j x_{ij} = \\mathbf{x}_i^T \\mathbf{b}. \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that while this notation makes it look like all the $x_{ij}$'s are distinct features, some can be a function of the others, e.g.\n",
    "\n",
    "\\begin{equation}\n",
    "    x_{i2} = x_{i1}^2. \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the whole sample, we have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{y}}  = \\left[ \\begin{array}{c}\n",
    "    \\hat{y}_1\\\\\n",
    "    \\vdots \\\\\n",
    "    \\hat{y}_m\n",
    "    \\end{array} \\right]= \\left[ \\begin{array}{c}\n",
    "    \\mathbf{x}_1^T \\mathbf{b} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{x}_m^T \\mathbf{b}\n",
    "    \\end{array} \\right] = \\mathbf{X} \\mathbf{b}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, in matrix notation, we can express the loss function in a number of equivalent ways:\n",
    "\n",
    "\\begin{equation}\n",
    "    L(\\mathbf{b}) = \\frac{1}{2}\\sum_{i = 1}^m \\left[\\hat{y}_i - y_i\\right]^2 = \\frac{1}{2} \\ \\left| \\left|\\ \\hat{\\mathbf{y}} - \\mathbf{y}\\ \\right| \\right|^2 = \\frac{1}{2} \\left| \\left|\\ \\mathbf{X} \\mathbf{b} - \\mathbf{y}\\ \\right| \\right|^2 = \\frac{1}{2} \\left| \\left|\\ \\begin{array}{c}\n",
    "    \\mathbf{x}_1^T \\mathbf{b} - y_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{x}_m^T \\mathbf{b} - y_m\n",
    "    \\end{array}\n",
    "    \\ \\right| \\right|^2 = \\frac{1}{2} (\\mathbf{X} \\mathbf{b} - \\mathbf{y})^T (\\mathbf{X} \\mathbf{b} - \\mathbf{y})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When implementing the loss function, the last definition is computationally most efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## define loss function\n",
    "def loss(b, X):\n",
    "    predicted_values = X @ b\n",
    "    return ( predicted_values - y ).T @ ( predicted_values - y ) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gradient\n",
    "\n",
    "The derivative of the loss function w.r.t. to $b_j$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial L}{\\partial b_j} = \\sum_{i = 1}^m \\left[ \\hat{y}_i - y_i \\right] x_{ij} = \\mathbf{X}_j^T \\cdot (\\hat{\\mathbf{y} } - \\mathbf{y}),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{X}_j$ denotes the $j$th column in the regression matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, the gradient of the loss function is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla L = \\left[ \n",
    "        \\begin{array}{c}\n",
    "        \\frac{\\partial L}{\\partial b_1} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\frac{\\partial L}{\\partial b_m}\n",
    "        \\end{array}\n",
    "    \\right] =  \\mathbf{X}^T \\cdot(\\hat{\\mathbf{y} } - \\mathbf{y}) = \\mathbf{X}^T \\cdot(\\mathbf{X} \\mathbf{b} - \\mathbf{y}) \n",
    "\\end{equation}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, it is straightforward to see that the Hessian of the loss function is:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla^2 L = \\left[ \n",
    "        \\begin{array}{ccc}\n",
    "        \\frac{\\partial^2 L}{\\partial b_1 \\partial b_1} & ... & \\frac{\\partial^2 L}{\\partial b_1 \\partial b_n} \\\\\n",
    "        \\vdots & \\ddots & \\vdots \\\\\n",
    "        \\frac{\\partial^2 L}{\\partial b_n \\partial b_1} & ... & \\frac{\\partial^2 L}{\\partial b_n \\partial b_n}\n",
    "        \\end{array}\n",
    "    \\right]  = \\mathbf{X}^T \\mathbf{X}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two things are noteworthy here:\n",
    "\n",
    "- the Hessian depends solely on the feature matrix $\\mathbf{X}$ and is independent of $\\mathbf{b}$; hence, it needs to be computed only once;\n",
    "- it is positive definite, implying that the loss function is convex. Hence, any $\\mathbf{b}^*$ such that $\\nabla L(\\mathbf{b}^*) = 0$ is the *global* minimizer of $L$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It should be emphasized that these properties are particular for linear LS problems, and do not necessarily hold for non-linear problems (and of course not for general minimization problems either). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a side note, $\\mathbf{X}^T \\mathbf{X}$ being positive definite may not be obvious at first glance. However, you can show that for an arbitrary $m$-by-$n$ matrix $A$ with $m \\ge n$ and rank $n$ - i.e., that has linearly independent columns - the matrix product $A^T A$ is positive definite. This will be shown in this week's problem set. \n",
    "\n",
    "Moreover, by construction, $A^T A$ is also symmetric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Normal Equations\n",
    "\n",
    "We can use the gradient directly for computing a minimum of the loss function, by setting it equal to zero: $\\nabla L  = 0$, or \n",
    "\n",
    "\\begin{equation}\n",
    "     \\mathbf{X}^T \\cdot(\\hat{\\mathbf{y} } - \\mathbf{y}) = \\mathbf{X}^T \\cdot(\\mathbf{X} \\mathbf{b} - \\mathbf{y}) \\overset{!}{=} 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, $\\mathbf{b}^*$ must satisfy the following linear system of equations:\n",
    "\n",
    "\\begin{equation}\n",
    "     (\\mathbf{X}^T \\mathbf{X}) \\mathbf{b} = \\mathbf{X}^T \\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "These are called the *normal equations*. Solving them gives a closed-form solution for $\\mathbf{b}^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Solving for $\\mathbf{b}^*$ by using the inverse of $\\mathbf{X}^T \\mathbf{X}$ gives the familiar formula for the OLS estimator:\n",
    "\n",
    "\\begin{equation}\n",
    "     \\mathbf{b}^* = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In practice, statistical programs do not use the inverse to solve the normal equations. Instead, they rely on methods that follow the same idea as LU factorization and triangular back- and forward substitution that we have encountered in lecture 3.\n",
    "\n",
    "Instead of LU factorization, however, the fact that $\\mathbf{X}^T \\mathbf{X}$ is a symmetric and positive definite matrix, as established above, allows to use *Cholesky factorization*, which requires only half the number operations compared to LU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In more detail, for any symmetric positive definite $n$-by-$n$ matrix $A$, Cholesky factorization produces the following decomposition:\n",
    "\n",
    "\\begin{equation}\n",
    "    A = U^T U,\n",
    "\\end{equation}\n",
    "\n",
    "where $U$ is an $n$-by-$n$ upper triangular matrix with positive diagonal elements. Once we have obtained $U$, we can easily apply triangular substitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a side note, Cholesky factorization can be used to check if a symmetric matrix is positive definite. If the algorithm returns a matrix $U$ where the diagonal elements are well-defined and positive, positive definiteness is verified.\n",
    "\n",
    "In SciPy, we have a function **scipy.linalg.cholesky** that returns $U$. As an example, let's use the feature matrix **X** from our data set, perform Cholesky factorization and check the diagonal elements of **U**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 13)\n"
     ]
    }
   ],
   "source": [
    "A = X.T @ X\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 209.4204  583.1158  247.9859    5.6858    4.2375   26.1879  402.8817\n",
      "   31.232   130.9807 1295.7138   49.1592 1854.1449   98.1221]\n"
     ]
    }
   ],
   "source": [
    "U = scipy.linalg.cholesky(A)\n",
    "print( np.diag( U ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the sake of completeness, if you try to apply Cholesky factorization on a matrix that is not positive definite, SciPy raises an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1., 2.], \n",
    "              [2., 4.], \n",
    "              [3., 6.]])\n",
    "\n",
    "print( np.linalg.matrix_rank(A) )\n",
    "## this line will throw an error, since A has rank 1 < n and hence A.T @ A is not positive definite!\n",
    "# print( scipy.linalg.cholesky(A.T @ A) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Going back on how to solve the normal equations for $\\mathbf{b}^*$, an important disadvantage of Cholesky factorization is that the condition number of $\\mathbf{X}^T \\mathbf{X}$ is the square of the condition number of $\\mathbf{X}$. \n",
    "\n",
    "Recall that the higher the condition number, the less well-conditioned a matrix is, and the higher the numerical error in the computed solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, the Cholesky-based method can result in less accurate solutions than when using methods that avoid a squaring of the condition number. When $\\mathbf{X}$ is ill-conditioned, Cholesky factorization may even break down.\n",
    "\n",
    "In addition, Cholesky will also not work if $\\mathbf{X}$ is \"rank-deficient\", i.e. has rank less than $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That's why in practice, solving the normal equations is often done by means of factorization of $\\mathbf{X}$ rather than $\\mathbf{X}^T \\mathbf{X}$. Among the alternative approaches are *QR factorization* and *singular-value decomposition* (SVD). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While potentially more computationally expensive, both methods approach are more robust and reliable than Cholesky, since avoid squaring the condition number. SVD also allows for rank-deficient matrices. \n",
    "\n",
    "While we will not go into more detail here, note that the SVD-approach is the default in SciPy's **scipy.linalg.lstsq** function for solving linear least-squares problems.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Solving Linear LS Problems in Python\n",
    "\n",
    "In Python, there are several alternative packages and functions for solving linear LS problems. We have already seen that the **statsmodels** package has a OLS routine, which is very convenient in connection with Pandas (see the tutorial on Pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As mentioned above, SciPy has a function **scipy.linalg.lstsq** that returns the parameter vector $\\mathbf{b}^*$ for a given feature matrix $\\mathbf{X}$ and target vector $\\mathbf{y}$, along with some additional information as a tuple. \n",
    "\n",
    "Before running the function, we add a column to our feature matrix that consists of ones. This takes care of the intercept in the linear regression problem. In other words, we assume that $x_{i1} = 1$ for all observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.      0.0063 18.      2.31    0.    ]\n",
      " [ 1.      0.0273  0.      7.07    0.    ]\n",
      " [ 1.      0.0273  0.      7.07    0.    ]\n",
      " [ 1.      0.0324  0.      2.18    0.    ]\n",
      " [ 1.      0.0691  0.      2.18    0.    ]]\n"
     ]
    }
   ],
   "source": [
    "X_1 = np.column_stack( (np.ones( X.shape[0] ), X ) )\n",
    "print(X_1[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 36.4911  -0.1072   0.0464   0.0209   2.6886 -17.7958   3.8048   0.0008\n",
      "  -1.4758   0.3057  -0.0123  -0.9535   0.0094  -0.5255]\n"
     ]
    }
   ],
   "source": [
    "res = scipy.linalg.lstsq(X_1, y)\n",
    "## print parameter values\n",
    "print( res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alternatively, we can set up the linear LS problem using scikit-learn's **LinearRegression()** class (which we have imported above). Note that this a \"wrapper\" around **scipy.linalg.lstsq**, i.e. it uses this function for the minimization, but then presents the output in a different, nicer way.\n",
    "\n",
    "Note also that we do not have to add a column of ones to the feature matrix; this is done by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 36.49110328036103\n",
      "Coefficients: [ -0.1072   0.0464   0.0209   2.6886 -17.7958   3.8048   0.0008  -1.4758\n",
      "   0.3057  -0.0123  -0.9535   0.0094  -0.5255]\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "print(\"Intercept: {}\".format(lr.intercept_) )\n",
    "print(\"Coefficients: {}\".format(lr.coef_) )\n",
    "## store results for later comparison\n",
    "b_lls = np.insert( lr.coef_, 0, lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 36.4911  -0.1072   0.0464   0.0209   2.6886 -17.7958   3.8048   0.0008\n",
      "  -1.4758   0.3057  -0.0123  -0.9535   0.0094  -0.5255]\n"
     ]
    }
   ],
   "source": [
    "## for comparison: compute b by hand, using the inverse\n",
    "b = np.linalg.inv((X_1.T @ X_1)) @ X_1.T @ y\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this syntax is typical of scikit-learn, and we will see more examples below. We start by initiating an instance of a **LinearRegression()** class, which does not take any arguments. Solving the linear LS problem happens in the second line where we use the **fit** method, and give it the feature matrix and the target vector as inputs.\n",
    "\n",
    "The parameter values can then be found in the **coef_** and **intercept_** attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, let's emphasize once more that the closed-form solution coming from the normal equations exists only in the case of linear LS. For nonlinear LS problem, this direct approach is not (typically) available.\n",
    "\n",
    "Moreover, there are LS problems where finding $\\mathbf{b}^*$ by solving the normal equations is not the most efficient. In all these cases, we will have to resort to iterative methods once again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'grad'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quasi-Newton Methods: Gradient Descent\n",
    "\n",
    "Before moving on to nonlinear LS problems, we will get to know one more algorithm for numerical optimization, *gradient descent* or *steepest descent*. \n",
    "\n",
    "While this is essentially a simple variant of Quasi-Newton methods and can be used to solve general optimization problems, its use is particularly common in the context of machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a quick recap, we have introduced the class of line search method. Recall that line search is an iterative procedure, generating a sequence of iterates $\\{ x^{(k)} \\}$ that are aimed to converge towards the minimum $x^*$. The update rule is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\mathbf{s}^{(k)} = \\mathbf{x}^{(k)} + \\alpha^{(k)} \\mathbf{d}^{(k)}\n",
    "\\end{equation}\n",
    "\n",
    "with *search direction* $\\mathbf{d}^{(k)}$ and *distance* $\\alpha^{(k)}$, i.e. the length of the *step* $\\mathbf{s}^{(k)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On a general level, line search algorithms take the direction $\\mathbf{d}^{(k)}$ as given and then choose $\\alpha$ by solving the following problem:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\min_{\\alpha > 0} f(\\mathbf{x}^{(k)} + \\alpha \\mathbf{d}^{(k)})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have also noted that different variants of line search methods differ in their choice of the search direction (and some also fix $\\alpha$), and hence with respect to how to determine $\\mathbf{s}^{(k)} = \\alpha^{(k)} \\mathbf{d}^{(k)}$. For example, Newton's method uses the following step, with a fixed $\\alpha$:\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathbf{s}^{(k)} =  - H(\\mathbf{x}^{(k)}) ^{-1}\\nabla  f(\\mathbf{x}^{(k)}) \n",
    "\\end{equation}\n",
    "\n",
    "Other than this, they all follow the same iterative procedure (which we had implemented in the function **my_opt**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The gradient descent algorithm uses the gradient as the search direction:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbf{d}^{(k)} = - \\nabla  f(\\mathbf{x}^{(k)})\n",
    "\\end{equation}\n",
    "\n",
    "and then looks for the optimal $\\alpha^{(k)}$, for example by using Armijo search and backtracking. \n",
    "\n",
    "Hence, we can interpret gradient descent as a simple Quasi-Newton method, where the Hessian in the Newton-Raphson update rule is *approximated by the identity matrix*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gradient descent is very intuitive: in order to minimize, we move in the direction of a local minimum. The main problem of gradient descent is that convergence is in general very slow, in particular for objective function that are highly non-linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Rosenbrock function is good example: even when using an optimal $\\alpha$ in every iteration, it can take thousands of iterations to get to the minimum. While gradient descent gets down to the Rosenbrock valley very quickly, it then makes very slow progress towards to the minimum (compare http://www.benfrederickson.com/numerical-optimization/ for a visualization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gradient Descent for Linear Regression\n",
    "\n",
    "However, gradient descent is popular in applications where the objective is more regular (say, quadratic, and hence convex) and hence the gradient is linear. \n",
    "\n",
    "The obvious example is linear LS problems, for whcih gradient descent can be used as an alternative to the standard approach based on normal equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Related to that, gradient descent is very important and heavily used in the context of machine learning, where you usually have a very large number of observations and explanatory variables/features (where the so-called \"normal equations\" approach may not be applicable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Going back to the notation used in this lecture, gradient descent uses the update rule\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{b}^{(k+1)} = \\mathbf{b}^{(k)} + \\alpha^{(k)} \\mathbf{d}^{(k)} = \\mathbf{b}^{(k)} - \\alpha^{(k)} \\nabla  L(\\mathbf{b}^{(k)} ) = \\mathbf{b}^{(k)} - \\alpha^{(k)} \\left[ \\mathbf{X}^T \\cdot(\\mathbf{X} \\mathbf{b}^{(k)} - \\mathbf{y}) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where we have used the gradient for linear LS problems as derived above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words, the search direction used by gradient descent is the gradient of the objective function (here the loss function) at the current guess for $\\mathbf{b}^{(k)}$.\n",
    "\n",
    "Without matrix notation, one can compute the coefficient sequentially:\n",
    "\\begin{equation}\n",
    "    b_j^{(k+1)} = b_j^{(k)} + \\alpha^{(k)} \\frac{\\partial L(\\mathbf{b})}{\\partial b_j} = b_j^{(k)} - \\alpha^{(k)} \\sum_{i = 1}^m \\left[ \\hat{y}_i - y_i \\right] = b_j^{(k)} - \\alpha^{(k)} \\mathbf{X}_j^T \\cdot (\\mathbf{X} \\mathbf{b}^{(k)} - \\mathbf{y})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Numerical Implementation\n",
    "\n",
    "For general optimization problems, we have used the function **my_opt** and defined the update rule corresponding to the specific algorithm in a separate function. Here, we combine those two steps in a single function, called **gd_linear**. Note that we use the precise gradient of the loss function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def gd_linear(b, X, y, alpha, show_it = False, maxit = 50000, eps = 1e-8):\n",
    "    \"\"\"\n",
    "    Implements the iterative procedure for gradient descent in the context of linear regression. Inputs are \n",
    "    -> b: initial guess for the minimizing coefficient vector\n",
    "    -> X: the m-by-n regression matrix, with each row containing the n features for one observation\n",
    "    -> y: the m-by-1 vector of target values \n",
    "    -> alpha: a scalar indicating the step size from b(k) to b(k+1)\n",
    "    \"\"\"\n",
    "    ## input check\n",
    "    m = len(y)\n",
    "    y.shape = (m, 1)\n",
    "    assert X.shape[0] == m; \"Dimension mismatch between y and X\" \n",
    "    \n",
    "    b.shape = (X.shape[1], 1)\n",
    "    \n",
    "    dist = 1\n",
    "    it = 0\n",
    "    \n",
    "    while dist > eps and it < maxit:\n",
    "        it += 1\n",
    "    \n",
    "        s = -alpha * X.T @ ( X @ b - y )\n",
    "\n",
    "        dist = np.linalg.norm(s) / (1 + np.linalg.norm(b))\n",
    "\n",
    "        b = b + s\n",
    "    \n",
    "    if show_it:\n",
    "        print(\"Gradient descent has converged in {} iterations\".format(it))\n",
    "    \n",
    "    return b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that since the loss function is convex, we know that convergence to a minimum is guaranteed for a sufficiently small **alpha**. Hence, we do not need to check whether the outcome of our iterative procedure is a minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Feature Scaling\n",
    "\n",
    "While the idea of the GD algorithm is straightforward, there are some issues to consider when running it in practice. GD may run into problems when the features are not on a similar scale/magnitude. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the Boston data set, for example, we have some features that are measured on a scale between approximately 3 and 9 (average number of rooms per dwelling), while others are on a scale from 0 to 400. We can see this by looking at the min and max values for each columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 88.9762, 100.    ,  27.74  ,   1.    ,   0.871 ,   8.78  ,\n",
       "       100.    ,  12.1265,  24.    , 711.    ,  22.    , 396.9   ,\n",
       "        37.97  ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.0063,   0.    ,   0.46  ,   0.    ,   0.385 ,   3.561 ,\n",
       "         2.9   ,   1.1296,   1.    , 187.    ,  12.6   ,   0.32  ,\n",
       "         1.73  ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.min(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A simplest remedy against this is feature scaling, i.e. bringing the features onto the same scale. There are different ways to do this, but the most common is *standardization*:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_{ij}^{std} = \\frac{x_{ij} - \\mu_j}{\\sigma_j},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu_j$ is the (sample) mean of one feature (i.e. for one column of $\\mathbf{X}$) and $\\sigma_j$ is the corresponding standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using standardization, features are centered around 0 and have a standard deviation of 1; their distribution (across a column) is normal.\n",
    "\n",
    "We can easily implement standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4177,  0.2848, -1.2879, ..., -1.459 ,  0.4411, -1.0756],\n",
       "       [-0.4153, -0.4877, -0.5934, ..., -0.3031,  0.4411, -0.4924],\n",
       "       [-0.4153, -0.4877, -0.5934, ..., -0.3031,  0.3964, -1.2087],\n",
       "       ...,\n",
       "       [-0.4114, -0.4877,  0.1157, ...,  1.1765,  0.4411, -0.983 ],\n",
       "       [-0.4057, -0.4877,  0.1157, ...,  1.1765,  0.4032, -0.8653],\n",
       "       [-0.4129, -0.4877,  0.1157, ...,  1.1765,  0.4411, -0.6691]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## standardize X\n",
    "(X - X.mean(axis = 0)) / X.std(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Scikit-learn has a function **preprocessing.scale** that does precisely that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sc = preprocessing.scale(X, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check if the scale function gives the same result as manual standardization\n",
    "(X_sc == (X - X.mean(axis = 0)) / X.std(axis = 0)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can verify that each column of **X_sc** has mean zero and standard deviation equal to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -0., -0.,  0.,  0., -0., -0.,  0.,  0., -0., -0.,  0., -0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sc.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sc.std(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Choosing the Learning Rate \n",
    "\n",
    "As outlined last time, \"proper\" line search algorithms look for the optimal learning rate (see above). In simple implementations of GD, the learning rate is kept fixed. \n",
    "\n",
    "The question then arises what value to choose for the learning rate. In general, for a sufficiently small learning rate, you get convergence, and hence a smaller function value, in every iteration. However, the smaller the learning rate, the slower is convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Conversely, if the learning rate is too large, the algorithm may not converge. \n",
    "\n",
    "In practice, one would start with a very small learning rate and then increase it, for example by a factor of 3. That is, say you start with *1e-5*, *3e-5*, *1e-4*, ... , until you no longer get convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Running GD\n",
    "\n",
    "Below, we add a column of 1's to the standardized feature matrix **X_sc** and then run our **gd_linear** function defined above. We use a small learning rate of *1e-4*. As initial guess, for simplicity,  we choose a vector of length $n$, filled with 0.1.\n",
    "\n",
    "Note that the standardization happens before adding the intercept term; moreover, we use scikit-learn's **preprocessing.add_dummy_feature** function for adding the 1-column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent has converged in 1218 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[22.5328],\n",
       "       [-0.9204],\n",
       "       [ 1.081 ],\n",
       "       [ 0.143 ],\n",
       "       [ 0.6822],\n",
       "       [-2.0601],\n",
       "       [ 2.6706],\n",
       "       [ 0.0211],\n",
       "       [-3.1044],\n",
       "       [ 2.6588],\n",
       "       [-2.0759],\n",
       "       [-2.0622],\n",
       "       [ 0.8566],\n",
       "       [-3.7487]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sc = preprocessing.scale(X, axis = 0)\n",
    "## add column of 1s for intercept term\n",
    "X_sc = preprocessing.add_dummy_feature(X_sc, 1)\n",
    "## alternatively (as above)\n",
    "# X_sc = np.column_stack( (np.ones( X_sc.shape[0] ), X_sc ) )\n",
    "\n",
    "gd_linear(0.1 * np.ones(X_sc.shape[1]), X_sc, y, 0.0003, maxit = 10000, show_it = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that these parameter values are different from the ones that we have computed above using the normal equations. This is due to the fact that we have not use the standardized feature matrix above. Repeating the computation with **X_sc** gives the same result. \n",
    "\n",
    "Note that since **X_sc** already contains a 1-column, we tell the algorithm not to add one automatically, by setting **fit_intercept = False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [[22.5328 -0.9204  1.081   0.143   0.6822 -2.0601  2.6706  0.0211 -3.1044\n",
      "   2.6588 -2.0759 -2.0622  0.8566 -3.7487]]\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(fit_intercept = False)\n",
    "lr.fit(X_sc, y)\n",
    "print(\"Coefficients: {}\".format(lr.coef_) )"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
