{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computational Methods in Economics\n",
    "\n",
    "## Lecture 5b - Numerical Optimization and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last update: 2020-01-07 15:19:33.132702\n"
     ]
    }
   ],
   "source": [
    "# Author: Alex Schmitt (schmitt@ifo.de)\n",
    "\n",
    "import datetime\n",
    "print('Last update: ' + str(datetime.datetime.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(precision=4, suppress = True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import scipy.optimize\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Lecture\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [The Scikit-Learn Package](#scikit)\n",
    "- [Notation and Terminology](#not)\n",
    "- [Linear Least-Squares Problems](#lls)\n",
    "- [Gradient Descent](#grad)\n",
    "- [ML vs. Econometrics: Regularization](#lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In the previous lecture we have discussed how to solve general optimization problems numerically. In this class, we are going to look at particular optimization problems that (among other applications) arise frequently in the context of data analysis.\n",
    "\n",
    "In addition, we also introduce more functionality for Python, in particular the **scikit-learn** package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**scikit-learn** is most commonly used in the context of \"data science\" and \"machine learning\". \n",
    "\n",
    "An additional goal of this class is to give a short introduction in particular to machine learning - also called \"statistical learning\" -, and show how it differs (and what it has in common) with more standard (from an economist's point of view) approaches to data analysis in econometrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, we will talk about ML methods and applications analysis in more depth later in the lecture. For now, let's just briefly define what we understand as \"machine learning\". The two most frequently definitions are probably the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*[Machine Learning is] the field of study that gives computers the ability to learn without explicitly programmed. (Arthur Samuel, 1959)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with E. (Tony Mitchell, 1997)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The classical exam for an ML application is an email spam filter (*task T*): the algorithm takes examples of emails which are flagged as \"spam\" and examples of regular emails (*experience E*), and learns how to distinguish between spam and non-spam emails.\n",
    "\n",
    "It can then *predict* whether a new email is spam or non-spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In fact, (*supervised*) machine learning is not only about learning, but also about prediction: you want the computer to learn using existing data, to make predictions about new (or not-yet-seen) data. \n",
    "\n",
    "In this way, ML methods can be used to automate certain tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It goes without saying that given the complexity of the topic and the limited amount of time, we will not be able to go in much depth. That said, the idea is to introduce some core ideas and terminology, and leave the rest to more specialized courses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Some of the material used in this lecture is following the excellent book **Hands-On Machine Learning with Scikit-Learn and Tensor Flow** by Aurelien Gideon. \n",
    "\n",
    "An excellent survey paper on the relationship between ML and econometrics and on the use of ML for economic analysis is **Machine Learning: An Applied Econometric Approach** by Sendhil Mullainathan and Jann Spiess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'scikit'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The scikit-learn Package\n",
    "\n",
    "scikit-learn (**sklearn**) is a very popular package for data analysis in  general and machine learning in particular. It builds on the packages we have already seen - NumPy, SciPy and Matplotlib - and provides a lot of additional functionality (of which we will only scratch the surface).\n",
    "\n",
    "Its documentation and an extensive amount of tutorials and other material can be found here: https://scikit-learn.org/stable/index.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similar to SciPy, to use **sklearn**, we import individual subpackages rather than the whole package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For now, just focus on the first statement in the cell above. scikit-learn includes a few \"toy datasets\" (as well as easy access to some larger \"real-world\" datasets), one of which we will use as a workhorse example below. It contains data on house prices and features in the Boston area. \n",
    "\n",
    "We load the dataset - which comes in the form of what is essentially a dictionary - and display its **DESCR** key, which contains a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "print( boston.DESCR )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can check all key-value pairs in the dictionary by running the **keys()** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "print( boston.keys() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The data is contained in the **data** and the **target** field. We can check that the number of data points in **data** are consistent with the description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (506, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data shape: {}\".format(boston.data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'not'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Notation and Terminology\n",
    "\n",
    "In the following, we use the notation that is standard in data analysis, in particular econometrics and data science. While this makes it consistent with other courses that you might have taken in the past, the notation will be different compared to the previous lectures in this class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In particular, let $(y_i, \\mathbf{x}_i)$ be a given tuple of data corresponding to observation $i$. \n",
    "\n",
    "$y_i$ is a scalar and referred to as a dependent variable, a *label* (if it comes from a discrete set of values) or *target/outcome variable* (if it is a continuous variable).\n",
    "\n",
    "$\\mathbf{x}_i = [x_{i1}, ... , x_{in}]^T \\in \\mathbb{R}^n$ is a vector of dimension $n$. Each element of $\\mathbf{x}_i$ is referred to as an independent variable, a *regressor* or a *feature*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a data set consisting of $m$ observations, $\\mathbf{y} = [y_{1}, ... , y_{m}]^T  \\in \\mathbb{R}^m$ is a vector of outcome variables, while\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{X} =  \\left[ \\begin{array}{c}\n",
    "    \\mathbf{x}_1^T  \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{x}_m^T \n",
    "    \\end{array} \\right] =  \n",
    "    \\left[ \\begin{array}{cccc}\n",
    "    x_{11} & x_{12} & ... & x_{1n}  \\\\\n",
    "    x_{21} & x_{22} & ... & x_{2n}  \\\\\n",
    "    \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & ... & x_{mn}  \\\\\n",
    "    \\end{array} \\right] \n",
    "\\end{equation}\n",
    "\n",
    "is an *m*-by-*n* matrix containing all realizations of the feature variables. It is also referred to as the *regression or feature matrix*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the context of our example dataset above, we define **'y'** as the vector of target variables (in **boston.target**) and **'X'** as the feature matrix containing the regressors in **boston.data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = boston.target\n",
    "X = boston.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.0063  18.       2.31     0.       0.538    6.575   65.2      4.09\n",
      "    1.     296.      15.3    396.9      4.98  ]\n",
      " [  0.0273   0.       7.07     0.       0.469    6.421   78.9      4.9671\n",
      "    2.     242.      17.8    396.9      9.14  ]\n",
      " [  0.0273   0.       7.07     0.       0.469    7.185   61.1      4.9671\n",
      "    2.     242.      17.8    392.83     4.03  ]\n",
      " [  0.0324   0.       2.18     0.       0.458    6.998   45.8      6.0622\n",
      "    3.     222.      18.7    394.63     2.94  ]\n",
      " [  0.0691   0.       2.18     0.       0.458    7.147   54.2      6.0622\n",
      "    3.     222.      18.7    396.9      5.33  ]]\n"
     ]
    }
   ],
   "source": [
    "## display first five observations\n",
    "print( X[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Hypothesis and Residual Function\n",
    "\n",
    "On a very general level, when analyzing data, we compare the realizations of the target variable in the sample, $\\mathbf{y}$, with a vector $\\hat{\\mathbf{y}} = [\\hat{y}_{1}, ... , \\hat{y}_{m}]^T$ of *predicted* outcomes. \n",
    "\n",
    "The predicted value of the target variable for observation $i$ is given by a function $h$, which (in machine learning terminology) is called the *hypothesis*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More precisely, the function $\\hat{y}_{i} = h(\\mathbf{x}_i; \\mathbf{b})$ returns the prediction for the target variable $y_i$ given the feature vector $\\mathbf{x}_i$ and a vector of *parameters* (or *coefficients* or *weights*) $\\mathbf{b}$. \n",
    "\n",
    "No matter what the purpose of an empirical analysis is, e.g. causal estimation (in econometrics) or prediction (in statistical/machine learning), it usually requires determining the parameter values given a data set, $(\\mathbf{y}, \\mathbf{X})$. This step is done by setting it up as a minimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is, we choose $\\mathbf{b}$ by minimizing a measure of the difference between the observed values of the outcome variable and its predicted values when using the hypothesis function. \n",
    "\n",
    "For a single observation, this difference is called the *residual (function)*:\n",
    "\n",
    "\\begin{equation}\n",
    "    h(\\mathbf{x}_i; \\mathbf{b})- y_i \\equiv r_i(\\mathbf{b}; y_i, \\mathbf{x}_i) = r_i(\\mathbf{b})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note the following:\n",
    "\n",
    "- We have defined $h$ as a function of the data $\\mathbf{x}_i$, given parameter values $\\mathbf{b}$. In contrast, we write the residual as a function of $\\mathbf{b}$. Of course, $r_i$ also depends on the data, but we usually omit these as arguments.\n",
    "\n",
    "- Related to this, we have a *different* residual function $r_i$ for every observation $i$.\n",
    "\n",
    "- Every $r_i$ is a scalar function, with $r_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, let $\\mathbf{r}$ denote the vector of residuals (i.e. a vector-valued function $\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$) across the sample:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{r}(\\mathbf{b})  =  \\left[ \\begin{array}{c}\n",
    "    r_1(\\mathbf{b})  \\\\\n",
    "    \\vdots \\\\\n",
    "    r_m(\\mathbf{b}) \n",
    "    \\end{array} \\right] \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Least-Squares Minimization\n",
    "\n",
    "There are different alternatives with respect to how to set up the objective function $L(\\mathbf{b})$ in the minimization problem to find the parameter values $\\mathbf{b}$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\min_{\\mathbf{b}} L(\\mathbf{b})\n",
    "\\end{equation}\n",
    "\n",
    "$L$ is also referred to as the *loss function* or *cost function*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#####  $\\mathcal{l}_\\infty$-norm\n",
    "\n",
    "Define $L$ as \n",
    "\n",
    "\\begin{equation}\n",
    "    L(\\mathbf{b}) = \\left| \\left| \\mathbf{r}(\\mathbf{b}) \\right| \\right|_{\\infty} = \\max_{i = 1,...,m} \\left| r_i(\\mathbf{b}) \\right|\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the loss function is determined by the data point with the largest deviation (in absolute value) from its predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#####  $\\mathcal{l}_1$-norm\n",
    "\n",
    "Define $L$ as \n",
    "\n",
    "\\begin{equation}\n",
    "    L(\\mathbf{b}) = \\left| \\left| \\mathbf{r}(\\mathbf{b}) \\right| \\right|_{1} = \\sum_{i = 1}^m \\left| r_i(\\mathbf{b}) \\right|\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the loss function is the sum of absolute values of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#####  $\\mathcal{l}_2$/Euclidean norm\n",
    "\n",
    "Define $L$ as \n",
    "\n",
    "\\begin{equation}\n",
    "    L(\\mathbf{b}) = \\frac{1}{2} \\left(\\ \\left| \\left|\\ \\mathbf{r}(\\mathbf{b}) \\right| \\right|_{2}\\ \\right)^2 = \\frac{1}{2}\\sum_{i = 1}^m \\left[r_i(\\mathbf{b})\\right]^2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a side note, recall that for a vector $\\mathbf{v} \\in \\mathbb{R}^n$, the Euclidean norm is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "     \\left| \\left|\\ \\mathbf{v}\\ \\right| \\right|_2 = \\left| \\left|\\ \\mathbf{v}\\ \\right| \\right| = \\sqrt{ \\sum_{j = 1}^n v_i^2} = \\sqrt{ \\mathbf{v}^T \\mathbf{v} }\n",
    "\\end{equation}\n",
    "\n",
    "Above, we use one half of the square of the norm; of course, this does not affect the result of the minimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, the loss function is the sum of squares of the residuals. Minimizing this function is the by far most common method for finding $\\mathbf{b}$.\n",
    "\n",
    "In other words, we set up the problem of determining $\\mathbf{b}$ as a *least-squares (LS) problem*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "LS problems are an extremely important class of unconstrained optimization problems. Compared to a generic minimization problem, as discussed in the previous lecture, the special form of the objective function in \n",
    "\n",
    "\\begin{equation}\n",
    "   \\min_{\\mathbf{b}} L(\\mathbf{b}) = \\min_{\\mathbf{b}} \\frac{1}{2}\\sum_{i = 1}^m \\left[r_i(\\mathbf{b})\\right]^2\n",
    "\\end{equation}\n",
    "\n",
    "makes LS problems easier to solve than more general problems (see below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that in machine learning, you usually divide the loss function above by the number of observations $m$. In this way, you minimze the *mean square error* (MSE):\n",
    "\n",
    "\\begin{equation}\n",
    "   \\min_{\\mathbf{b}} MSE(\\mathbf{b}) = \\min_{\\mathbf{b}} \\frac{1}{2m}\\sum_{i = 1}^m \\left[r_i(\\mathbf{b})\\right]^2\n",
    "\\end{equation}\n",
    "\n",
    "Of course, whether you use this objective or the one above does not affect the solution to your minimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before moving on, let's recap the different types of functions defined above:\n",
    "- the hypothesis function $h(\\mathbf{x}_i; \\mathbf{b})$ gives a prediction of the target value for data point $\\mathbf{x}_i$;\n",
    "- the difference between the actual and predicted value gives rise to the residual function $r_i(\\mathbf{b})$;\n",
    "- in LS, the loss function is defined as the sum of squared residuals; minimizing it determines $\\mathbf{b}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the LS method imposes a functional form on $L$, but does not prescribe what $h$ (and hence $r$) looks like. If $h$ is linear in the parameters $\\mathbf{b}$, the problem becomes the OLS linear regression framework that you are familiar with.\n",
    "\n",
    "It is important to understand though that this is one particular class of LS problems. In other words, the use of LS is not restricted to linear hypothesis functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'lls'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Least-Squares Problems\n",
    "\n",
    "In linear LS problems, the hypothesis function is *linear in the parameters* $\\mathbf{b}$ (but not necessarily linear in the features $\\mathbf{x}$!).\n",
    "\n",
    "Hence,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y}_i = h(\\mathbf{x}_i; \\mathbf{b}) = b_1 x_{i1} + b_2 x_{i2} + ... + b_{n} x_{in} = \\sum_{j = 1}^n b_j x_{ij} = \\mathbf{x}_i^T \\mathbf{b}. \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that while this notation makes it look like all the $x_{ij}$'s are distinct features, some can be functions of the others, e.g.\n",
    "\n",
    "\\begin{equation}\n",
    "    x_{i2} = x_{i1}^2. \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the whole sample, we have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{y}}  = \\left[ \\begin{array}{c}\n",
    "    \\hat{y}_1\\\\\n",
    "    \\vdots \\\\\n",
    "    \\hat{y}_m\n",
    "    \\end{array} \\right]= \\left[ \\begin{array}{c}\n",
    "    \\mathbf{x}_1^T \\mathbf{b} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{x}_m^T \\mathbf{b}\n",
    "    \\end{array} \\right] = \\mathbf{X} \\mathbf{b}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, in matrix notation, we can express the loss function in a number of equivalent ways:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    L(\\mathbf{b}) &= \\frac{1}{2}\\sum_{i = 1}^m \\left[\\hat{y}_i - y_i\\right]^2 = \\frac{1}{2} \\ \\left| \\left|\\ \\hat{\\mathbf{y}} - \\mathbf{y}\\ \\right| \\right|^2 \\\\\n",
    "    &= \\frac{1}{2} \\left| \\left|\\ \\mathbf{X} \\mathbf{b} - \\mathbf{y}\\ \\right| \\right|^2 = \\frac{1}{2} \\left| \\left|\\ \\begin{array}{c}\n",
    "    \\mathbf{x}_1^T \\mathbf{b} - y_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{x}_m^T \\mathbf{b} - y_m\n",
    "    \\end{array}\n",
    "    \\ \\right| \\right|^2 = \\frac{1}{2} (\\mathbf{X} \\mathbf{b} - \\mathbf{y})^T (\\mathbf{X} \\mathbf{b} - \\mathbf{y})\n",
    "\\end{split}    \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When implementing the loss function, the last definition is computationally most efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## define loss function\n",
    "def loss(b, X):\n",
    "    predicted_values = X @ b\n",
    "    return ( predicted_values - y ).T @ ( predicted_values - y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gradient\n",
    "\n",
    "The derivative of the loss function w.r.t. to $b_j$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial L}{\\partial b_j} = \\sum_{i = 1}^m \\left[ \\hat{y}_i - y_i \\right] x_{ij} = \\mathbf{X}_j^T \\cdot (\\hat{\\mathbf{y} } - \\mathbf{y}),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{X}_j$ denotes the $j$th column in the regression matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, the gradient of the loss function is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla L = \\left[ \n",
    "        \\begin{array}{c}\n",
    "        \\frac{\\partial L}{\\partial b_1} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\frac{\\partial L}{\\partial b_m}\n",
    "        \\end{array}\n",
    "    \\right] =  \\mathbf{X}^T \\cdot(\\hat{\\mathbf{y} } - \\mathbf{y}) = \\mathbf{X}^T \\cdot(\\mathbf{X} \\mathbf{b} - \\mathbf{y}) \n",
    "\\end{equation}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, it is straightforward to see that the Hessian of the loss function is:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla^2 L = \\left[ \n",
    "        \\begin{array}{ccc}\n",
    "        \\frac{\\partial^2 L}{\\partial b_1 \\partial b_1} & ... & \\frac{\\partial^2 L}{\\partial b_1 \\partial b_n} \\\\\n",
    "        \\vdots & \\ddots & \\vdots \\\\\n",
    "        \\frac{\\partial^2 L}{\\partial b_n \\partial b_1} & ... & \\frac{\\partial^2 L}{\\partial b_n \\partial b_n}\n",
    "        \\end{array}\n",
    "    \\right]  = \\mathbf{X}^T \\mathbf{X}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two things are noteworthy here:\n",
    "\n",
    "- the Hessian depends solely on the feature matrix $\\mathbf{X}$ and is independent of $\\mathbf{b}$; hence, it needs to be *computed only once*;\n",
    "- it is positive definite, implying that the loss function is convex. Hence, any $\\mathbf{b}^*$ such that $\\nabla L(\\mathbf{b}^*) = 0$ is the *global* minimizer of $L$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It should be emphasized that these properties are particular for linear LS problems, and do not necessarily hold for non-linear problems (and of course not for general minimization problems either)! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a side note, $\\mathbf{X}^T \\mathbf{X}$ being positive definite may not be obvious at first glance. However, you can show that for an arbitrary $m$-by-$n$ matrix $A$ with $m \\ge n$ and rank $n$ - i.e., that has linearly independent columns - the matrix product $A^T A$ is positive definite. This will be shown in this week's problem set. \n",
    "\n",
    "Moreover, by construction, $A^T A$ is also symmetric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Normal Equations\n",
    "\n",
    "We can use the gradient directly for computing a minimum of the loss function, by setting it equal to zero: $\\nabla L  = 0$, or \n",
    "\n",
    "\\begin{equation}\n",
    "     \\mathbf{X}^T \\cdot(\\hat{\\mathbf{y} } - \\mathbf{y}) = \\mathbf{X}^T \\cdot(\\mathbf{X} \\mathbf{b} - \\mathbf{y}) \\overset{!}{=} 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, $\\mathbf{b}^*$ must satisfy the following linear system of equations:\n",
    "\n",
    "\\begin{equation}\n",
    "     (\\mathbf{X}^T \\mathbf{X}) \\mathbf{b} = \\mathbf{X}^T \\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "These are called the *normal equations*. Solving them gives a closed-form solution for $\\mathbf{b}^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Solving for $\\mathbf{b}^*$ by using the inverse of $\\mathbf{X}^T \\mathbf{X}$ gives the familiar formula for the OLS estimator:\n",
    "\n",
    "\\begin{equation}\n",
    "     \\mathbf{b}^* = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In practice, statistical programs do not use the inverse to solve the normal equations. Instead, they rely on methods that follow the same idea as LU factorization and triangular back- and forward substitution that we have encountered in lecture 3.\n",
    "\n",
    "Instead of LU factorization, however, the fact that $\\mathbf{X}^T \\mathbf{X}$ is a symmetric and positive definite matrix, as established above, allows to use *Cholesky factorization*, which requires only half the number operations compared to LU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In more detail, for any symmetric positive definite $n$-by-$n$ matrix $A$, Cholesky factorization produces the following decomposition:\n",
    "\n",
    "\\begin{equation}\n",
    "    A = U^T U,\n",
    "\\end{equation}\n",
    "\n",
    "where $U$ is an $n$-by-$n$ upper triangular matrix with positive diagonal elements. Once we have obtained $U$, we can easily apply triangular substitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a side note, Cholesky factorization can be used to check if a symmetric matrix is positive definite. If the algorithm returns a matrix $U$ where the diagonal elements are well-defined and positive, positive definiteness is verified.\n",
    "\n",
    "In SciPy, we have a function **scipy.linalg.cholesky** that returns $U$. As an example, let's use the feature matrix from our data set, perform Cholesky factorization and check the diagonal elements of $U$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 13)\n"
     ]
    }
   ],
   "source": [
    "A = X.T @ X\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 209.6911  583.1158  247.5719    5.6853    4.2375   26.1753  402.8917\n",
      "   31.227   130.701  1295.7229   49.1618 1852.0389   98.0639]\n"
     ]
    }
   ],
   "source": [
    "U = scipy.linalg.cholesky(A)\n",
    "print( np.diag( U ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the sake of completeness, if you try to apply Cholesky factorization on a matrix that is not positive definite, SciPy raises an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1., 2.], \n",
    "              [2., 4.], \n",
    "              [3., 6.]])\n",
    "\n",
    "print( np.linalg.matrix_rank(A) )\n",
    "## this line will throw an error, since A has rank 1 < n and hence A.T @ A is not positive definite!\n",
    "# print( scipy.linalg.cholesky(A.T @ A) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Going back on how to solve the normal equations for $\\mathbf{b}^*$, an important disadvantage of Cholesky factorization is that the condition number of $\\mathbf{X}^T \\mathbf{X}$ is the square of the condition number of $\\mathbf{X}$. \n",
    "\n",
    "Recall that the higher the condition number, the less well-conditioned a matrix is, and the higher the numerical error in the computed solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, the Cholesky-based method can result in less accurate solutions than when using methods that avoid a squaring of the condition number. When $\\mathbf{X}$ is ill-conditioned, Cholesky factorization may even break down.\n",
    "\n",
    "In addition, Cholesky will also not work if $\\mathbf{X}$ is \"rank-deficient\", i.e. has rank less than $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That's why in practice, solving the normal equations is often done by means of factorization of $\\mathbf{X}$ rather than $\\mathbf{X}^T \\mathbf{X}$. Among the alternative approaches are *QR factorization* and *singular-value decomposition* (SVD). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While potentially more computationally expensive, both methods approach are more robust and reliable than Cholesky, since avoid squaring the condition number. SVD also allows for rank-deficient matrices. \n",
    "\n",
    "While we will not go into more detail here, note that the SVD-approach is the default in SciPy's **scipy.linalg.lstsq** function for solving linear least-squares problems.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Solving Linear LS Problems in Python\n",
    "\n",
    "In Python, there are several alternative packages and functions for solving linear LS problems. We have already seen that the **statsmodels** package has a OLS routine, which is very convenient in connection with Pandas (see the tutorial on Pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As mentioned above, SciPy has a function **scipy.linalg.lstsq** that returns the parameter vector $\\mathbf{b}^*$ for a given feature matrix $\\mathbf{X}$ and target vector $\\mathbf{y}$, along with some additional information as a tuple. \n",
    "\n",
    "Before running the function, we add a column to our feature matrix that consists of ones. This takes care of the intercept in the linear regression problem. In other words, we assume that $x_{i1} = 1$ for all observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.      0.0063 18.      2.31    0.    ]\n",
      " [ 1.      0.0273  0.      7.07    0.    ]\n",
      " [ 1.      0.0273  0.      7.07    0.    ]\n",
      " [ 1.      0.0324  0.      2.18    0.    ]\n",
      " [ 1.      0.0691  0.      2.18    0.    ]]\n"
     ]
    }
   ],
   "source": [
    "X_1 = np.column_stack( (np.ones( X.shape[0] ), X ) )\n",
    "print(X_1[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 36.4595  -0.108    0.0464   0.0206   2.6867 -17.7666   3.8099   0.0007\n",
      "  -1.4756   0.306   -0.0123  -0.9527   0.0093  -0.5248]\n"
     ]
    }
   ],
   "source": [
    "res = scipy.linalg.lstsq(X_1, y)\n",
    "## print parameter values\n",
    "print( res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alternatively, we can set up the linear LS problem using scikit-learn's **LinearRegression()** class (which we have imported above). Note that this a \"wrapper\" around **scipy.linalg.lstsq**, i.e. it uses this function for the minimization, but then presents the output in a different, nicer way.\n",
    "\n",
    "Note also that we do not have to add a column of ones to the feature matrix; this is done by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 36.459488385089855\n",
      "Coefficients: [ -0.108    0.0464   0.0206   2.6867 -17.7666   3.8099   0.0007  -1.4756\n",
      "   0.306   -0.0123  -0.9527   0.0093  -0.5248]\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "print(\"Intercept: {}\".format(lr.intercept_) )\n",
    "print(\"Coefficients: {}\".format(lr.coef_) )\n",
    "## store results for later comparison\n",
    "b_lls = np.insert( lr.coef_, 0, lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 36.4595  -0.108    0.0464   0.0206   2.6867 -17.7666   3.8099   0.0007\n",
      "  -1.4756   0.306   -0.0123  -0.9527   0.0093  -0.5248]\n"
     ]
    }
   ],
   "source": [
    "## for comparison: compute b by hand, using the inverse\n",
    "b = np.linalg.inv((X_1.T @ X_1)) @ X_1.T @ y\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that this syntax is typical of scikit-learn, and we will see more examples below. We start by initiating an instance of a **LinearRegression()** class, which does not take any arguments. Solving the linear LS problem happens in the second line where we use the **fit** method, and give it the feature matrix and the target vector as inputs.\n",
    "\n",
    "The parameter values can then be found in the **coef_** and **intercept_** attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, let's emphasize once more that the closed-form solution coming from the normal equations exists only in the case of linear LS. For nonlinear LS problem, this direct approach is not (typically) available.\n",
    "\n",
    "Moreover, there are LS problems where finding $\\mathbf{b}^*$ by solving the normal equations is not the most efficient. In all these cases, we will have to resort to iterative methods once again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'grad'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quasi-Newton Methods: Gradient Descent\n",
    "\n",
    "Before moving on to nonlinear LS problems, we will get to know one more algorithm for numerical optimization, *gradient descent* or *steepest descent*. \n",
    "\n",
    "While this is essentially a simple variant of Quasi-Newton methods and can be used to solve general optimization problems, its use is particularly common in the context of machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a quick recap, we have introduced the class of line search method. Recall that line search is an iterative procedure, generating a sequence of iterates $\\{ x^{(k)} \\}$ that are aimed to converge towards the minimum $x^*$. The update rule is\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\mathbf{s}^{(k)} = \\mathbf{x}^{(k)} + \\alpha^{(k)} \\mathbf{d}^{(k)}\n",
    "\\end{equation}\n",
    "\n",
    "with *search direction* $\\mathbf{d}^{(k)}$ and *distance* $\\alpha^{(k)}$, i.e. the length of the *step* $\\mathbf{s}^{(k)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On a general level, line search algorithms take the direction $\\mathbf{d}^{(k)}$ as given and then choose $\\alpha$ by solving the following problem:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\min_{\\alpha > 0} f(\\mathbf{x}^{(k)} + \\alpha \\mathbf{d}^{(k)})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have also noted that different variants of line search methods differ in their choice of the search direction (and some also fix $\\alpha$), and hence with respect to how to determine $\\mathbf{s}^{(k)} = \\alpha^{(k)} \\mathbf{d}^{(k)}$. For example, Newton's method uses the following step, with a fixed $\\alpha$:\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathbf{s}^{(k)} =  - H(\\mathbf{x}^{(k)}) ^{-1}\\nabla  f(\\mathbf{x}^{(k)}) \n",
    "\\end{equation}\n",
    "\n",
    "Other than this, they all follow the same iterative procedure (which we had implemented in the function **my_opt**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The gradient descent algorithm uses the gradient as the search direction:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbf{d}^{(k)} = - \\nabla  f(\\mathbf{x}^{(k)})\n",
    "\\end{equation}\n",
    "\n",
    "and then looks for the optimal $\\alpha^{(k)}$, for example by using Armijo search and backtracking. \n",
    "\n",
    "Hence, we can interpret gradient descent as a simple Quasi-Newton method, where the Hessian in the Newton-Raphson update rule is *approximated by the identity matrix*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gradient descent is very intuitive: in order to minimize, we move in the direction of a local minimum. The main problem of gradient descent is that convergence is in general very slow, in particular for objective functions that are highly non-linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Rosenbrock function is good example: even when using an optimal $\\alpha$ in every iteration, it can take thousands of iterations to get to the minimum. While gradient descent gets down to the Rosenbrock valley very quickly, it then makes very slow progress towards to the minimum. \n",
    "\n",
    "Compare http://www.benfrederickson.com/numerical-optimization/ for a visualization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gradient Descent for Linear Regression\n",
    "\n",
    "However, gradient descent is popular in applications where the objective is more regular (say, quadratic, and hence convex) and hence the gradient is linear. \n",
    "\n",
    "The obvious example is linear LS problems, for whcih gradient descent can be used as an alternative to the standard approach based on normal equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Related to that, gradient descent is very important and heavily used in the context of machine learning, where you usually have a very large number of observations and explanatory variables/features (where the normal equations approach may not be applicable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Going back to the notation used in this lecture, gradient descent uses the update rule\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{b}^{(k+1)} = \\mathbf{b}^{(k)} + \\alpha^{(k)} \\mathbf{d}^{(k)} = \\mathbf{b}^{(k)} - \\alpha^{(k)} \\nabla  L(\\mathbf{b}^{(k)} ) = \\mathbf{b}^{(k)} - \\alpha^{(k)} \\left[ \\mathbf{X}^T \\cdot(\\mathbf{X} \\mathbf{b}^{(k)} - \\mathbf{y}) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where we have used the gradient for linear LS problems as derived above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words, the search direction used by gradient descent is the gradient of the objective function (here the loss function) at the current guess for $\\mathbf{b}^{(k)}$.\n",
    "\n",
    "Without matrix notation, one can compute the coefficient sequentially:\n",
    "\n",
    "\\begin{equation}\n",
    "    b_j^{(k+1)} = b_j^{(k)} + \\alpha^{(k)} \\frac{\\partial L(\\mathbf{b})}{\\partial b_j} = b_j^{(k)} - \\alpha^{(k)} \\sum_{i = 1}^m \\left[ \\hat{y}_i - y_i \\right]x_{ij} = b_j^{(k)} - \\alpha^{(k)} \\mathbf{X}_j^T \\cdot (\\mathbf{X} \\mathbf{b}^{(k)} - \\mathbf{y})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Numerical Implementation\n",
    "\n",
    "For general optimization problems, we have used the function **my_opt** and defined the update rule corresponding to the specific algorithm in a separate function. Here, we combine those two steps in a single function, called **gd_linear**. Note that we use the precise gradient of the loss function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def gd_linear(b, X, y, alpha, show_it = False, maxit = 50000, eps = 1e-8):\n",
    "    \"\"\"\n",
    "    Implements the iterative procedure for gradient descent in the context of linear regression. Inputs are \n",
    "    -> b: initial guess for the minimizing coefficient vector\n",
    "    -> X: the m-by-n regression matrix, with each row containing the n features for one observation\n",
    "    -> y: the m-by-1 vector of target values \n",
    "    -> alpha: a scalar indicating the step size from b(k) to b(k+1)\n",
    "    \"\"\"\n",
    "    ## input check\n",
    "    m = len(y)\n",
    "    y.shape = (m, 1)\n",
    "    assert X.shape[0] == m; \"Dimension mismatch between y and X\" \n",
    "    \n",
    "    b.shape = (X.shape[1], 1)\n",
    "    \n",
    "    dist = 1\n",
    "    it = 0\n",
    "    \n",
    "    while dist > eps and it < maxit:\n",
    "        it += 1\n",
    "    \n",
    "        s = -alpha * X.T @ ( X @ b - y )\n",
    "\n",
    "        dist = np.linalg.norm(s) / (1 + np.linalg.norm(b))\n",
    "\n",
    "        b = b + s\n",
    "    \n",
    "    if show_it:\n",
    "        print(\"Gradient descent has converged in {} iterations\".format(it))\n",
    "    \n",
    "    return b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that since the loss function is convex, we know that convergence to a global minimum is guaranteed for a sufficiently small  **alpha**, which is also called the *learning rate* in the context of machine learning. Hence, we do not need to check whether the outcome of our iterative procedure is a minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Feature Scaling\n",
    "\n",
    "While the idea of the GD algorithm is straightforward, there are some issues to consider when running it in practice. GD may run into problems when the features are not on a similar scale/magnitude. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the Boston data set, for example, we have some features that are measured on a scale between approximately 3 and 9 (average number of rooms per dwelling), while others are on a scale from 0 to 400. We can see this by looking at the min and max values for each columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 88.9762, 100.    ,  27.74  ,   1.    ,   0.871 ,   8.78  ,\n",
       "       100.    ,  12.1265,  24.    , 711.    ,  22.    , 396.9   ,\n",
       "        37.97  ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.0063,   0.    ,   0.46  ,   0.    ,   0.385 ,   3.561 ,\n",
       "         2.9   ,   1.1296,   1.    , 187.    ,  12.6   ,   0.32  ,\n",
       "         1.73  ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.min(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A simplest remedy against this is feature scaling, i.e. bringing the features onto the same scale. There are different ways to do this, but the most common is *standardization*:\n",
    "\n",
    "\\begin{equation}\n",
    "    x_{ij}^{std} = \\frac{x_{ij} - \\mu_j}{\\sigma_j},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu_j$ is the (sample) mean of one feature (i.e. for one column of $\\mathbf{X}$) and $\\sigma_j$ is the corresponding standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using standardization, features are centered around 0 and have a standard deviation of 1; their distribution (across a column) is normal.\n",
    "\n",
    "We can easily implement standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4198,  0.2848, -1.2879, ..., -1.459 ,  0.4411, -1.0756],\n",
       "       [-0.4173, -0.4877, -0.5934, ..., -0.3031,  0.4411, -0.4924],\n",
       "       [-0.4173, -0.4877, -0.5934, ..., -0.3031,  0.3964, -1.2087],\n",
       "       ...,\n",
       "       [-0.4134, -0.4877,  0.1157, ...,  1.1765,  0.4411, -0.983 ],\n",
       "       [-0.4078, -0.4877,  0.1157, ...,  1.1765,  0.4032, -0.8653],\n",
       "       [-0.415 , -0.4877,  0.1157, ...,  1.1765,  0.4411, -0.6691]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## standardize X\n",
    "(X - X.mean(axis = 0)) / X.std(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Scikit-learn has a function **preprocessing.scale** that does precisely that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sc = preprocessing.scale(X, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check if the scale function gives the same result as manual standardization\n",
    "(X_sc == (X - X.mean(axis = 0)) / X.std(axis = 0)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can verify that each column of **X_sc** has mean zero and standard deviation equal to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0., -0., -0.,  0.,  0., -0., -0.,  0.,  0., -0., -0.,  0., -0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sc.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sc.std(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Choosing the Learning Rate \n",
    "\n",
    "As outlined last time, \"proper\" line search algorithms look for the optimal learning rate (see above). In simple implementations of GD, we keep the learning rate fixed. \n",
    "\n",
    "The question then arises what value to choose for the learning rate. In general, for a sufficiently small learning rate, you get convergence to the global minimum (if you wait long enough), and hence a smaller function value, in every iteration. However, the smaller the learning rate, the slower is convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Conversely, if the learning rate is too large, the algorithm may not converge. \n",
    "\n",
    "In practice, one would start with a very small learning rate and then increase it, for example by a factor of 3. That is, say you start with *1e-5*, *3e-5*, *1e-4*, ... , until you no longer get convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Running GD\n",
    "\n",
    "Below, we add a column of 1's to the standardized feature matrix **X_sc** and then run our **gd_linear** function defined above. We use a learning rate of *1e-4*. As initial guess, for simplicity,  we choose a vector of length $n$, filled with 0.1.\n",
    "\n",
    "Note that the standardization happens before adding the intercept term; moreover, we use scikit-learn's **preprocessing.add_dummy_feature** function for adding the 1-column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent has converged in 1218 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[22.5328],\n",
       "       [-0.9281],\n",
       "       [ 1.0816],\n",
       "       [ 0.1409],\n",
       "       [ 0.6817],\n",
       "       [-2.0567],\n",
       "       [ 2.6742],\n",
       "       [ 0.0195],\n",
       "       [-3.104 ],\n",
       "       [ 2.6622],\n",
       "       [-2.0768],\n",
       "       [-2.0606],\n",
       "       [ 0.8493],\n",
       "       [-3.7436]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## standardize data\n",
    "X_sc = preprocessing.scale(X, axis = 0)\n",
    "## add column of 1s for intercept term\n",
    "X_sc = preprocessing.add_dummy_feature(X_sc, 1)\n",
    "## alternatively (as above)\n",
    "# X_sc = np.column_stack( (np.ones( X_sc.shape[0] ), X_sc ) )\n",
    "\n",
    "## run gradient descent\n",
    "gd_linear(0.1 * np.ones(X_sc.shape[1]), X_sc, y, 0.0003, maxit = 10000, show_it = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that these parameter values are different from the ones that we have computed above using the normal equations. This is due to the fact that we have not used the standardized feature matrix above. Repeating the computation with **X_sc** gives the same result. \n",
    "\n",
    "Note that since **X_sc** already contains a 1-column, we tell the algorithm not to add one automatically, by setting **fit_intercept = False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [[22.5328 -0.9281  1.0816  0.1409  0.6817 -2.0567  2.6742  0.0195 -3.104\n",
      "   2.6622 -2.0768 -2.0606  0.8493 -3.7436]]\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(fit_intercept = False)\n",
    "lr.fit(X_sc, y)\n",
    "print(\"Coefficients: {}\".format(lr.coef_) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gradient Descent vs. Normal Equations\n",
    "\n",
    "Above, we have used GD to solve a linear LS problem. It is important to note that this was done just for illustrating the use of GD. \n",
    "\n",
    "In practice, in many applications, it is more efficient to use a direct, closed-form solution method based on the normal equations (e.g. SVD or Cholesky factorization) rather than an iterative method like GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the context of *linear* LS problems, the one occasion for which it is be reasonable to use GD (or another iterative method) is in the case of *very large* problems. Note that \"large\" here can refer to the number of features $n$, as well as the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In econometrics, we usually have a relatively small number of explanatory variables (as well as $m >> n$, i.e. the number of observations is much higher). \n",
    "\n",
    "In machine learning applications, this is often not the case: the number of features included in a model can go in the ten thousands (or more)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Solving the normal equations has a *computational complexity* of $O(n^3)$; this means that the number of floating-point operations necessary to find $\\mathbf{b}^*$ is of magnitude $n^3$. In other words, if you double the number of features, computational time would multiply by a factor of approximately 8.\n",
    "\n",
    "For example, if the model has 100,000 features, it would take *1e+15* operations to solve the normal equations. As a rule of thumb, if $n$ is this large, using the normal equations becomes very slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "At the same time, note that the number of operations to solve the normal equations is linear in the number of observations. Hence, even for a high number of observations, the direct approach is efficient provided that the number of features is not prohibitely high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, in this case a problem may arise if the the complete feature matrix $\\mathbf{X}$ is so large that it does not fit in memory. \n",
    "\n",
    "For example, consider a case with 1000 features and *2e+6* observations. If not sparse, $\\mathbf{X}$ would consist of $2e+9$ floating point numbers. Since each float is stored using 8 byte, this would to 16 GB, which may already be higher than your laptop's memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To summarize, using the normal equations can run into problems for a large number of features, or when there are too many observations to fit the data in memory.\n",
    "\n",
    "In either of these cases, GD may be a better suited alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With respect to the first issue, GD requires a smaller amount of computations compared to using the normal equations for a large number of features. \n",
    "\n",
    "Moreover, it can save computation time since it can be easier parallelized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is, since in a given iteration, each of the elements in $\\mathbf{b}^{(k+1)}$ can be computed independently from the others, \n",
    "\n",
    "\\begin{equation}\n",
    "    b_j^{(k+1)} = b_j^{(k)} - \\alpha^{(k)} \\mathbf{X}_j^T \\cdot (\\mathbf{X} \\mathbf{b}^{(k)} - \\mathbf{y}),\n",
    "\\end{equation}\n",
    "\n",
    "one can easily distribute these operations across multiple processors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With respect to feature matrix $\\mathbf{X}$ being too large to fit in memory, using a variant of GD can be a remedy, in particular *stochastic gradient descent* and *mini-batch gradient descent*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Stochastic Gradient Descent\n",
    "\n",
    "Note that the GD algorithm introduced above is also called *batch gradient descent*, since in every iteration you use the whole data set (i.e. all observations) to compute the gradient of the loss function and update the parameter vector:\n",
    "\n",
    "\\begin{equation}\n",
    "    b_j^{(k+1)}  = b_j^{(k)} - \\alpha^{(k)} \\sum_{i = 1}^m \\left[ \\hat{y}_i - y_i \\right]x_{ij} = b_j^{(k)} - \\alpha^{(k)} \\mathbf{X}_j^T \\cdot (\\mathbf{X} \\mathbf{b}^{(k)} - \\mathbf{y})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Stochastic gradient descent (SGD) is the opposite extreme: it picks one observation at random, and computes the gradient and updates the parameter vector based on this observations:\n",
    "\n",
    "\\begin{equation}\n",
    "    b_j^{(k+1)}  = b_j^{(k)} - \\alpha^{(k)} \\left[ \\hat{y}_i - y_i \\right] x_{ij} \n",
    "\\end{equation}\n",
    "\n",
    "where $i$ is randomly chosen among the $m$ observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a result, there are much less computations in each iteration, and hence decreases computation time. \n",
    "\n",
    "Moreover, since only one observation has to be in memory in a given iteration, you can SGD on huge datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, note that SGD will only decrease the loss function *on average*. That is, even if the current $\\mathbf{b}^{(k)}$ is very close to the optimum, the loss function may have jumps in its values from one iteration to the next, depending on the observation chosen for that particular observation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "scikit-learn has a **SGDRegressor** class that lets you solve a linear LS problem using stochastic gradient descent. To run it, we first need to import it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The syntax is similar to the **LinearRegressor** class seen above. As an additional argument, we can set the tolerance level for the stopping rule **tol**.\n",
    "\n",
    "As a side note, **SGDRegressor** implements what is called a *learning schedule*, where the learning rate is decreased over time time, allowing the algorithm to settle at the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [22.5464 -0.9245  1.0641 -0.0439  0.6987 -2.0397  2.728  -0.0344 -3.0585\n",
      "  2.2352 -1.6323 -2.0703  0.8691 -3.7649]\n",
      "Number of iterations: 49\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDRegressor(fit_intercept = False, tol = 1e-8)\n",
    "sgd.fit(X_sc, y.reshape(-1))\n",
    "print(\"Coefficients: {}\".format(sgd.coef_) )\n",
    "print(\"Number of iterations: {}\".format(sgd.n_iter_) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, note that there is a variant of GD between Batch Gradient Descent and SGD, called Mini-batch GD. Intuitively, it uses more than one observation, but instead relies on small random sets of observations. \n",
    "\n",
    "Note that neither Batch nor Mini-batch GD are implemented as own regressor classes in scitkit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'lasso'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ML vs. Econometrics\n",
    "\n",
    "So far, the discussion has been in terms of problems and methods, not about the differences between the fields of machine/statistical learning and econometrics.\n",
    "\n",
    "The simple reason for this is that everything discussed so far - the (linear) LS problem, and solving it via normal equations (if applicable) or by applying variants of gradient descent - could be used in both ML and econometric analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, the fact that these methods *could* be used, does not mean that they *are*. In more detail:\n",
    "\n",
    "- Econometrics makes heavy use of linear LS regression (OLS) problems, and finds a solution by solving the normal equations. Gradient descent is typically not used, for the simple reason that the number of features in econometric models, and hence also the number of elements in the feature matrix, is relatively small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- ML could use linear LS problems for learning and prediction, and, given the often huge amounts of data, would make use of stochastic gradient descent. However, linear LS in its standard form is rarely used, mainly because of its inflexible functional form and the lack of *regularization* parameters (*hyperparameters*). More on this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, while we have solved the linear LS problem in multiple ways above, we have not discussed yet what to do with the results, how to interpret them etc.\n",
    "\n",
    "That is because at this point, ML and econometrics go in different directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As indicated above, (*supervised*) ML learns from data in order to make predictions about not-yet-seen (or *out-of-sample*) data points/observations. Put differently, the goal of ML is to *generalize* from a given data sample to new instances.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In terms of the notation introduced above, ML is mainly interested in $\\hat{y}_l$, where $l$ indexes an out-of-sample observation. Note that since $\\hat{y}_l = h(\\mathbf{x}_l; \\mathbf{b})$, ML is also interested in the parameter vector $\\mathbf{b}$, but as a \"means to an end\", i.e. only to the extent that it is necessary for the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Central to our understanding [as empirical economists] is that machine learning not only provides new tools, it solves a different problem.\" \n",
    "\n",
    "\"Of course, prediction has a long history in economic research - machine learning provides new tools to solve this old problem.\"\n",
    "\n",
    "(Mullainathan and Spiess 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In contrast, econometrics is less interested in making predictions, but more in the estimating the parameter vector $\\mathbf{b}$ and to what extent it can be interpreted as *causal*. \n",
    "\n",
    "Given this objective, it is intuitive why econometricians work with datasets where $m >> n$: suppose you have $m \\approx n$, i.e. almost perfect fit. Then a small change in one observed variable can have a relatively large effect on your estimated parameter values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since observed data contains sampling errors, this would lead to a large degree of uncertainty surrounding the \"right\" parameter values, which makes a causal interpretation difficult.\n",
    "\n",
    "In more technical terms, too many regressors for a given sample size leads to high standard errors, and thus insignificant parameter values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Machine Learning with linear LS\n",
    "\n",
    "The goal of supervised machine learning is to predict the outcome variable for out-of-sample observations. Of course, linear LS regression (OLS) could be used for this, but is not the best choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate this, let's continue with the house data set used above, but also include all interactions terms, i.e. the products of all possible combinations of two variables out of the set of all 13 variables: $x_i x_j$, $i, j = 1,...,13$. \n",
    "\n",
    "The intuition here is of course that not only the variables themselves may matter for the house price, but also the interactions between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The number of additional variables is given by the number of ways to choose two *unordered* elements from a set of 13 elements, i.e. the binomial coefficient \"13 choose 2\", which is 78 without repetition and 91 with repetition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As a side note, in SciPy, computing the binomial coefficient can be done with the **scipy.special.comb** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import comb\n",
    "comb(13, 2, repetition=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, since we also use squared variables, we have 91 additional variables, and hence 104 total variables. The extended data set is given in the txt-file **boston_extended_data**. We read it in using NumPy's **loadtext** function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (506, 104)\n"
     ]
    }
   ],
   "source": [
    "X_ext = np.loadtxt('boston_extended_data')\n",
    "print(\"Data shape: {}\".format(X_ext.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, in order to evaluate how well an algorithm predicts out-of-sample, we need a measure for prediction quality. The typical choice, thankfully, is nothing new to an empirical economist: $R^2$, the *coefficient of determination*, which gives proportion of the variance in the dependent variable that is predictable from the independent variables (features).\n",
    "\n",
    "As in econometric analysis, we would like a high $R^2$ (i.e. as close as possible to 1), which indicates a good \"fit\" of the model to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Formally, we have \n",
    "\n",
    "\\begin{equation}\n",
    "    R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}} = 1 - \\frac{\\sum _{i}(y_{i}-{\\hat{y}_i})^{2}}{\\sum _{i}(y_{i}-{\\bar {y}})^{2}},\n",
    "\\end{equation}\n",
    "\n",
    "where $SS_{\\text{res}}$ is again the sum of squared residuals, and $SS_{\\text{tot}}$ is the *total sum of squares*, i.e. the sum of squared deviations from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, recall that we are interested in out-of-sample prediction quality, and hence the *\"i\"* subscipt above ideally would refer to out-of-sample observations. Of course, we do not have those yet when we train a model on a given sample.\n",
    "\n",
    "The solution to this is straightforward: we divide the complete sample, e.g. the 504 data points, in two parts: a *training set* that we use to train the model, i.e. to find the parameter values, and a *test set* that we apply the model on and compute $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that if we were to train the model and evaluate its performance on the same dataset (as we do in econometric analysis, where we are not primarily interested in prediction), the result would indicate a much better fit than what we could expect when applying the model to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In scikit-learn, the **train_test_split** function in the **model_selection** tool box implements splitting the data set. By default, it randomly assigns 75% of the observations to the training set and the remaining 25% to the test set. You can size the relative size with the optional **test_size** argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 104) (127, 104)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ext, y, random_state=0)\n",
    "print( X_train.shape, X_test.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can know run linear LS on the training set to find the parameter values, as above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than computing the predicted values for the *test set* using the definitions above, we can use another method in the **LinearRegression()** class, called **predict**. It takes a feature matrix and computes the predictions using the parameter values that are stored within the LR instance after running **fit**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = lr.predict(X_test)\n",
    "## for comparison: compute predictions using X @ b\n",
    "y_hat2 = np.column_stack( ( np.ones(X_test.shape[0]),  X_test) ) @ np.insert(lr.coef_, 0, lr.intercept_).reshape(105, 1)\n",
    "np.allclose(y_hat, y_hat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we can use the predictions to compute $R^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6058]]\n"
     ]
    }
   ],
   "source": [
    "ss_res = (y_test - y_hat).T @ (y_test - y_hat)\n",
    "ss_tot = (y_test - y_test.mean()).T @ (y_test - y.mean())\n",
    "print( 1 - ss_res/ss_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again, there is a method in the LR class that essentially combines the steps in the previous two cell, called **score**. It takes a feature matrix and a target vector as inputs, computes the predicted vector using the former and then computes $R^2$ using latter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Below, we compute the score for both the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.95\n",
      "Test set score: 0.61\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What we can see is that the predictive performance of the model on the training set is much better than on the test set. \n",
    "\n",
    "Again, this is by construction: we have used the training set to find the parameters $\\mathbf{b}$, by minimizing the sum of squared residuals between actual and predicted values, which is in the numerator of $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That said, linear LS typically shows a higher $R^2$ when making predictions on the training set, and a lower $R^2$ when using the test set, compared to other algorithms.\n",
    "\n",
    "In ML terms, linear LS is prone to *overfitting*: a linear LS model performs well on the training data, but does not generalize well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To get an intuition for overfitting, consider an extreme example: consider the linear LS model with two features - in the housing example above, say, average crime rate and the mean number of bedrooms - (and no intercept), and only two obervations. Hence, we have\n",
    "\n",
    "\\begin{equation}\n",
    "    \\left[ \\begin{array}{cc}\n",
    "    x_{11} & x_{12}  \\\\\n",
    "    x_{21} & x_{22}   \n",
    "    \\end{array} \\right] \n",
    "        \\left[ \\begin{array}{c}\n",
    "    b_{1}   \\\\\n",
    "    b_{2}\n",
    "    \\end{array} \\right] = \\left[ \\begin{array}{c}\n",
    "    y_{1}   \\\\\n",
    "    y_{2}\n",
    "    \\end{array} \\right]\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Of course, this is a SLE: we can easily find parameters that give us a *perfect in-sample fit*, i.e. residuals of zero, and hence an $R^2$ equal to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sm, y_sm = X[:2, (0, 7)], y[:2]\n",
    "b_sm = np.linalg.solve(X_sm, y_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In contrast, we would not expect this to generalize well to out-of-sample data. In fact, here the model is so bad that we even get negative predictions and hence a non-sensical result for $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-152141.6395]]\n"
     ]
    }
   ],
   "source": [
    "## make predictions based on b_sm for remaining observations\n",
    "z = X[2:, (0, 7)] @ b_sm\n",
    "ss_res = (y[2:] - z).T @ (y[2:] - z)\n",
    "ss_tot = (y[2:] - y[2:].mean()).T @ (y[2:] - y[2:].mean())\n",
    "print( 1 - ss_res/ss_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In general, overfitting occurs when the model is too complex relative to the amount (and \"noisiness\") of the data. In the example above, a simple way to avoid overfitting is to add more training data.\n",
    "\n",
    "Since this is often not an option, the solution to overfitting is to essentially make the models simpler, either by explicitly reducing the number of features used for training the model, or by implicitly doing so. The latter is done by regularization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before talking about regularization, note that when we go back to the previous split in training and test data, but still only use the two features, the prediction performance is much worse than before, for both training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.18\n",
      "Test set score: 0.09\n"
     ]
    }
   ],
   "source": [
    "X_train_sm, X_test_sm, y_train, y_test = train_test_split(X[:, (0, 7)], y, random_state=0)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_sm, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train_sm, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test_sm, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is again not surprising, since we have omitted a lot of information that was previously there. \n",
    "\n",
    "This is an example of *underfitting*: the model is too simple to learn the underlying structure of the data. As seen, a remedy against underfitting is adding more (or better) features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### ML with regularization\n",
    "\n",
    "As indicated by these examples, an essential aspect when doing machine learning is to find and train models that neither over- nor underfit the data, or in other words, that fit the training data well, while generalizing to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The usual strategy is to start with a relatively complex model, i.e. with many features, to avoid underfitting. For example, above we have included 104 features in a linear LS model for the house price data.\n",
    "\n",
    "Then, in order to reduce overfitting, we *constrain* or *regularize* the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As the prime example, consider what is called *Least Absolute Shrinkage and Selection Operator* (LASSO) regression, which is essentially a regularized version of linear LS regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In more detail, for its objective function, Lasso takes the objective function from linear LS, $L(\\mathbf{b})$, but adds a regularization term:\n",
    "\n",
    "\\begin{equation}\n",
    "   \\min_{\\mathbf{b}} J(\\mathbf{b}) = L(\\mathbf{b}) + \\alpha\\ \\left|\\left|\\ \\mathbf{b}\\ \\right| \\right|_1 = \\frac{1}{2}\\sum_{i = 1}^m \\left[r_i(\\mathbf{b})\\right]^2 + \\alpha \\sum^n_{j = 1} \\left| b_j \\right|\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note the following:\n",
    "\n",
    "- the regularization term is given by the $l_1$-norm (i.e. the sum of absolute values) of the parameter vector. Since it is part of the minimization objective, the algorithm has an incentive to choose as small values for $\\mathbf{b}$ as possible. In fact, an important characteristic of Lasso is that it sets some of the elements of $\\mathbf{b}$ to zero in order to minimize $J(\\mathbf{b})$. In other words, it \"eliminates\" the least important features from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $\\alpha$ is called a *hyperparameter* (to distinguish it from the \"parameters\" in $\\mathbf{b}$). It controls the amount of regularization that is applied. It is a parameter of the learning algorithm, not of the model, and hence must be constant while training the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With respect to $\\alpha$, there is an obvious trade-off:\n",
    "- the smaller $\\alpha$, the less regularization is applied, and hence the closer the model is to linear LS and to potentially overfitting the model;\n",
    "- in contrast, a large $\\alpha$ leads to a lot of features being eliminated by the algorithm, which prevents overfitting; however, *too much regularization* can lead to underfitting the data (e.g. in the extreme case when only one or two features are left)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finding the \"right\" hyperparameters (also called *hypertuning*) is one the most important tasks when applying ML. There are established procedures how to go about this search, in particular *(k-fold) cross-validation*.\n",
    "\n",
    "In the interest of time, we will skip this (very important) topic here and refer to more advanced courses or material on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Going back to linear LS, the main reason why it is not popular for ML is that it does not have any hyperparameters. In other words, there is no way to constrain linear LS or to choose what degree of regularization to apply to it. \n",
    "\n",
    "Put differently, Lasso is inherently more *flexible*, since the algorithm essentially decides by itself how many and what features to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a side note, there also exist version of regularized linear LS that uses the $l_2$-norm of $\\mathbf{b}$ for the regularization parameter (*Ridge regression*) or a combination of $l_1$- and $l_2$-norms (*Elastic Net*). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, below we run Lasso on our house price example, and compare its prediction performance to linear LS. As you would expect, there is a class **Lasso** in scitkit-learn that lets us do this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The syntax for training the data is the same as for linear LS. We use a hyperparameter of 0.01 (smaller than the default) and increase the maximum number of iterations (otherwise we would get a warning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=100000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=None,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = Lasso(alpha = 0.01, max_iter=100000)\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Evaluating its performance, we can see that Lasso does slightly worse on the training set, but considerably better on the test set - exactly what we would like! We can also see that out of the 104 features, only 33 are used, the rest receives parameters equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.90\n",
      "Test set score: 0.77\n",
      "Number of features used: 33\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "## check how many features have a non-zero parameter, i.e. are used\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a final remark, note that the Lasso implementation in scikit-learn uses what is called *coordinate descent*, a derivative-free iterative method - the normal equations cannot be applied for regularized linear LS regression!\n",
    "\n",
    "Again due to time constraints, we will not go into the details here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "<a id = 'summary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wrapping Up\n",
    "\n",
    "This lecture has scratched the very surface of machine learning: we have shown its connection to linear LS regression (which is itself a specific application of numerical optimization) and its relation to standard econometric analysis.\n",
    "\n",
    "Of course, there is a lot in the context of ML that we have not talked about. The following are some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, regularized linear LS is by no means the only class of ML methods. There are many more, some of them \"non-parametric\". Well-known examples include decision trees/random forests, neural nets (\"deep learning\") and various ensemble methods.\n",
    "\n",
    "Most of them are covered by textbooks on ML (see above) or in ML courses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a formalization of the general idea of all these ML methods, we can follow Mullainathan and Spiess (2017) who summarize ML in the following expression:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\min_{f \\in F} L(f(x_i), y_i)\\quad s.t.\\quad R(f) \\le c\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As before, $L$ denotes some cost that we aim to minimize. For variants of linear LS, we can write it as a function of $\\mathbf{b}$; more generally, it is a function of $f$ (and the data). \n",
    "\n",
    "The minimizer $f$ is itself a particular function (from a class $F$) that maps the feature data (the $x_i$'s) into predicted values for the $y_i$'s. Above, $F$ was essentially the class of *linear* hypothesis functions $h$, while the a particular function $f$ was represented by the parameter vector $\\mathbf{b}$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$R(f)$ captures the regularizer; in the case of Lasso, for example, $R(f)$ would capture the $l_1$-norm of $\\mathbf{b}$.\n",
    "\n",
    "As a different examples, in the case of decision trees, $F$ would represent the class of trees, and an example for $R(f)$ would be the \"depth\" of the particular tree (compare Mullainathan and Spiess for some intuition on decision trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Second, we have focused here on the case of regression, where the target variable is continuous (e.g. house prices). \n",
    "\n",
    "*Classification* (where the target values come from a discrete set, e.g. spam or no spam) is an equally important part of supervised ML.\n",
    "\n",
    "Moreover, as this indicates, there is also a subfield of *unsupervised* ML. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Third, as indicated above, we have not discussed how to proceed when tuning the hyperparameters of your model, and more generally to evaluate your model. Cross-Validation and related topics are an essentially part of every specialized course and book on ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, why are ML methods actually useful for economists? As stated above, ML is not concerned with causal estimation, and hence may at first glance seem useless for econometric analysis.\n",
    "\n",
    "It is important to note, however, that while ML is certainly not a substitute for econometrics, it can be understood as a *complement*. That is, we can use ML methods to support econometric analyis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider some examples (from Mullainathan and Spiess - this does not claim to be a complete list!):\n",
    "\n",
    "- ML can be applied to process \"unconventional\" data, e.g. to translate satellites images into meaningful measures that can be used as input in econometric analysis. It can also be used to improve more standard measures, like democracy indices (compare the work by ifo researcher Klaus Gründler: https://www.wiwi.uni-wuerzburg.de/lehrstuhl/vwl4/data/svmdi-dataset/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In estimation problems, there may be (implicit) prediction components. For example, the first stage of an IV approach is essentially a prediction problem. More generally, ML can be used to create counterfactuals to be used in estimation. Moreover, there is a large literature on the estimation of \"heterogeneous treatment effects\" using ML as \"a means to an end\" (compare, among others, the work by Susan Athey). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Finally, ML can be directly applied in public policy. An example is the famous \"bail decision\" problem - should a defendant be released on bail or not? - which could be \"outsourced\" to an ML algorithm rather than made by a judge. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
